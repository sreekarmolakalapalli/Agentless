{"model_name_or_path": "agentless", "instance_id": "scikit-learn__scikit-learn-13241", "model_patch": "diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py\nindex 650c24f..0d96457 100644\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -215,7 +215,10 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         self.lambdas_ = self.lambdas_[indices]\n         self.alphas_ = self.alphas_[:, indices]\n \n-        # remove eigenvectors with a zero eigenvalue\n+        # Fix the sign of the eigenvectors for consistency\n+        self.alphas_ = np.sign(self.alphas_[0, :]) * self.alphas_\n+\n+        # Remove eigenvectors with a zero eigenvalue\n         if self.remove_zero_eig or self.n_components is None:\n             self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n             self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n", "raw_model_patch": "diff --git a/sklearn/decomposition/kernel_pca.py b/sklearn/decomposition/kernel_pca.py\nindex 650c24f..0d96457 100644\n--- a/sklearn/decomposition/kernel_pca.py\n+++ b/sklearn/decomposition/kernel_pca.py\n@@ -215,7 +215,10 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n         self.lambdas_ = self.lambdas_[indices]\n         self.alphas_ = self.alphas_[:, indices]\n \n-        # remove eigenvectors with a zero eigenvalue\n+        # Fix the sign of the eigenvectors for consistency\n+        self.alphas_ = np.sign(self.alphas_[0, :]) * self.alphas_\n+\n+        # Remove eigenvectors with a zero eigenvalue\n         if self.remove_zero_eig or self.n_components is None:\n             self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n             self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n", "original_file_content": "\"\"\"Kernel Principal Components Analysis\"\"\"\n\n# Author: Mathieu Blondel <mathieu@mblondel.org>\n# License: BSD 3 clause\n\nimport numpy as np\nfrom scipy import linalg\nfrom scipy.sparse.linalg import eigsh\n\nfrom ..utils import check_random_state\nfrom ..utils.validation import check_is_fitted, check_array\nfrom ..exceptions import NotFittedError\nfrom ..base import BaseEstimator, TransformerMixin, _UnstableOn32BitMixin\nfrom ..preprocessing import KernelCenterer\nfrom ..metrics.pairwise import pairwise_kernels\n\n\nclass KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):\n    \"\"\"Kernel Principal component analysis (KPCA)\n\n    Non-linear dimensionality reduction through the use of kernels (see\n    :ref:`metrics`).\n\n    Read more in the :ref:`User Guide <kernel_PCA>`.\n\n    Parameters\n    ----------\n    n_components : int, default=None\n        Number of components. If None, all non-zero components are kept.\n\n    kernel : \"linear\" | \"poly\" | \"rbf\" | \"sigmoid\" | \"cosine\" | \"precomputed\"\n        Kernel. Default=\"linear\".\n\n    gamma : float, default=1/n_features\n        Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other\n        kernels.\n\n    degree : int, default=3\n        Degree for poly kernels. Ignored by other kernels.\n\n    coef0 : float, default=1\n        Independent term in poly and sigmoid kernels.\n        Ignored by other kernels.\n\n    kernel_params : mapping of string to any, default=None\n        Parameters (keyword arguments) and values for kernel passed as\n        callable object. Ignored by other kernels.\n\n    alpha : int, default=1.0\n        Hyperparameter of the ridge regression that learns the\n        inverse transform (when fit_inverse_transform=True).\n\n    fit_inverse_transform : bool, default=False\n        Learn the inverse transform for non-precomputed kernels.\n        (i.e. learn to find the pre-image of a point)\n\n    eigen_solver : string ['auto'|'dense'|'arpack'], default='auto'\n        Select eigensolver to use. If n_components is much less than\n        the number of training samples, arpack may be more efficient\n        than the dense eigensolver.\n\n    tol : float, default=0\n        Convergence tolerance for arpack.\n        If 0, optimal value will be chosen by arpack.\n\n    max_iter : int, default=None\n        Maximum number of iterations for arpack.\n        If None, optimal value will be chosen by arpack.\n\n    remove_zero_eig : boolean, default=False\n        If True, then all components with zero eigenvalues are removed, so\n        that the number of components in the output may be < n_components\n        (and sometimes even zero due to numerical instability).\n        When n_components is None, this parameter is ignored and components\n        with zero eigenvalues are removed regardless.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`. Used when ``eigen_solver`` == 'arpack'.\n\n        .. versionadded:: 0.18\n\n    copy_X : boolean, default=True\n        If True, input X is copied and stored by the model in the `X_fit_`\n        attribute. If no further changes will be done to X, setting\n        `copy_X=False` saves memory by storing a reference.\n\n        .. versionadded:: 0.18\n\n    n_jobs : int or None, optional (default=None)\n        The number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. versionadded:: 0.18\n\n    Attributes\n    ----------\n    lambdas_ : array, (n_components,)\n        Eigenvalues of the centered kernel matrix in decreasing order.\n        If `n_components` and `remove_zero_eig` are not set,\n        then all values are stored.\n\n    alphas_ : array, (n_samples, n_components)\n        Eigenvectors of the centered kernel matrix. If `n_components` and\n        `remove_zero_eig` are not set, then all components are stored.\n\n    dual_coef_ : array, (n_samples, n_features)\n        Inverse transform matrix. Only available when\n        ``fit_inverse_transform`` is True.\n\n    X_transformed_fit_ : array, (n_samples, n_components)\n        Projection of the fitted data on the kernel principal components.\n        Only available when ``fit_inverse_transform`` is True.\n\n    X_fit_ : (n_samples, n_features)\n        The data used to fit the model. If `copy_X=False`, then `X_fit_` is\n        a reference. This attribute is used for the calls to transform.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_digits\n    >>> from sklearn.decomposition import KernelPCA\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> transformer = KernelPCA(n_components=7, kernel='linear')\n    >>> X_transformed = transformer.fit_transform(X)\n    >>> X_transformed.shape\n    (1797, 7)\n\n    References\n    ----------\n    Kernel PCA was introduced in:\n        Bernhard Schoelkopf, Alexander J. Smola,\n        and Klaus-Robert Mueller. 1999. Kernel principal\n        component analysis. In Advances in kernel methods,\n        MIT Press, Cambridge, MA, USA 327-352.\n    \"\"\"\n\n    def __init__(self, n_components=None, kernel=\"linear\",\n                 gamma=None, degree=3, coef0=1, kernel_params=None,\n                 alpha=1.0, fit_inverse_transform=False, eigen_solver='auto',\n                 tol=0, max_iter=None, remove_zero_eig=False,\n                 random_state=None, copy_X=True, n_jobs=None):\n        if fit_inverse_transform and kernel == 'precomputed':\n            raise ValueError(\n                \"Cannot fit_inverse_transform with a precomputed kernel.\")\n        self.n_components = n_components\n        self.kernel = kernel\n        self.kernel_params = kernel_params\n        self.gamma = gamma\n        self.degree = degree\n        self.coef0 = coef0\n        self.alpha = alpha\n        self.fit_inverse_transform = fit_inverse_transform\n        self.eigen_solver = eigen_solver\n        self.remove_zero_eig = remove_zero_eig\n        self.tol = tol\n        self.max_iter = max_iter\n        self.random_state = random_state\n        self.n_jobs = n_jobs\n        self.copy_X = copy_X\n\n    @property\n    def _pairwise(self):\n        return self.kernel == \"precomputed\"\n\n    def _get_kernel(self, X, Y=None):\n        if callable(self.kernel):\n            params = self.kernel_params or {}\n        else:\n            params = {\"gamma\": self.gamma,\n                      \"degree\": self.degree,\n                      \"coef0\": self.coef0}\n        return pairwise_kernels(X, Y, metric=self.kernel,\n                                filter_params=True, n_jobs=self.n_jobs,\n                                **params)\n\n    def _fit_transform(self, K):\n        \"\"\" Fit's using kernel K\"\"\"\n        # center kernel\n        K = self._centerer.fit_transform(K)\n\n        if self.n_components is None:\n            n_components = K.shape[0]\n        else:\n            n_components = min(K.shape[0], self.n_components)\n\n        # compute eigenvectors\n        if self.eigen_solver == 'auto':\n            if K.shape[0] > 200 and n_components < 10:\n                eigen_solver = 'arpack'\n            else:\n                eigen_solver = 'dense'\n        else:\n            eigen_solver = self.eigen_solver\n\n        if eigen_solver == 'dense':\n            self.lambdas_, self.alphas_ = linalg.eigh(\n                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n        elif eigen_solver == 'arpack':\n            random_state = check_random_state(self.random_state)\n            # initialize with [-1,1] as in ARPACK\n            v0 = random_state.uniform(-1, 1, K.shape[0])\n            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n                                                which=\"LA\",\n                                                tol=self.tol,\n                                                maxiter=self.max_iter,\n                                                v0=v0)\n\n        # sort eigenvectors in descending order\n        indices = self.lambdas_.argsort()[::-1]\n        self.lambdas_ = self.lambdas_[indices]\n        self.alphas_ = self.alphas_[:, indices]\n\n        # remove eigenvectors with a zero eigenvalue\n        if self.remove_zero_eig or self.n_components is None:\n            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n\n        return K\n\n    def _fit_inverse_transform(self, X_transformed, X):\n        if hasattr(X, \"tocsr\"):\n            raise NotImplementedError(\"Inverse transform not implemented for \"\n                                      \"sparse matrices!\")\n\n        n_samples = X_transformed.shape[0]\n        K = self._get_kernel(X_transformed)\n        K.flat[::n_samples + 1] += self.alpha\n        self.dual_coef_ = linalg.solve(K, X, sym_pos=True, overwrite_a=True)\n        self.X_transformed_fit_ = X_transformed\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the model from data in X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training vector, where n_samples in the number of samples\n            and n_features is the number of features.\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        X = check_array(X, accept_sparse='csr', copy=self.copy_X)\n        self._centerer = KernelCenterer()\n        K = self._get_kernel(X)\n        self._fit_transform(K)\n\n        if self.fit_inverse_transform:\n            sqrt_lambdas = np.diag(np.sqrt(self.lambdas_))\n            X_transformed = np.dot(self.alphas_, sqrt_lambdas)\n            self._fit_inverse_transform(X_transformed, X)\n\n        self.X_fit_ = X\n        return self\n\n    def fit_transform(self, X, y=None, **params):\n        \"\"\"Fit the model from data in X and transform X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training vector, where n_samples in the number of samples\n            and n_features is the number of features.\n\n        Returns\n        -------\n        X_new : array-like, shape (n_samples, n_components)\n        \"\"\"\n        self.fit(X, **params)\n\n        X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n\n        if self.fit_inverse_transform:\n            self._fit_inverse_transform(X_transformed, X)\n\n        return X_transformed\n\n    def transform(self, X):\n        \"\"\"Transform X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n\n        Returns\n        -------\n        X_new : array-like, shape (n_samples, n_components)\n        \"\"\"\n        check_is_fitted(self, 'X_fit_')\n\n        K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n        return np.dot(K, self.alphas_ / np.sqrt(self.lambdas_))\n\n    def inverse_transform(self, X):\n        \"\"\"Transform X back to original space.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_components)\n\n        Returns\n        -------\n        X_new : array-like, shape (n_samples, n_features)\n\n        References\n        ----------\n        \"Learning to Find Pre-Images\", G BakIr et al, 2004.\n        \"\"\"\n        if not self.fit_inverse_transform:\n            raise NotFittedError(\"The fit_inverse_transform parameter was not\"\n                                 \" set to True when instantiating and hence \"\n                                 \"the inverse transform is not available.\")\n\n        K = self._get_kernel(X, self.X_transformed_fit_)\n\n        return np.dot(K, self.dual_coef_)"}
