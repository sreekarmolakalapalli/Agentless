{"instance_id": "scikit-learn__scikit-learn-25500", "raw_output": ["To fix the issue with `CalibratedClassifierCV` when using `IsotonicRegression` and `set_config(transform_output=\"pandas\")`, we need to ensure that the output of `IsotonicRegression.predict` is always a NumPy array, even when the configuration is set to output pandas DataFrames.\n\nHere are the necessary changes:\n\n1. Modify the `IsotonicRegression.predict` method to ensure it always returns a NumPy array.\n2. Update the `_CalibratedClassifier.predict_proba` method to handle the case where `calibrator.predict` might return a pandas DataFrame.\n\nLet's make these changes:\n\n```python\n### sklearn/isotonic.py\n<<<<<<< SEARCH\n        return self.transform(T)\n=======\n        return np.asarray(self.transform(T))\n>>>>>>> REPLACE\n```\n\n```python\n### sklearn/calibration.py\n<<<<<<< SEARCH\n            proba[:, class_idx] = calibrator.predict(this_pred)\n=======\n            proba[:, class_idx] = np.asarray(calibrator.predict(this_pred)).reshape(-1)\n>>>>>>> REPLACE\n```\n\nThese changes ensure that the `IsotonicRegression.predict` method always returns a NumPy array and that the `_CalibratedClassifier.predict_proba` method correctly handles the output, even if it is a pandas DataFrame.", "Based on the provided issue description and the provided code segments, the root cause of the issue is that the `predict_proba` method in the `_CalibratedClassifier` class does not handle the case when the `IsotonicRegression.predict` method returns a pandas DataFrame. The `IsotonicRegression.predict` method should return a NumPy array instead. We need to ensure that the output of `IsotonicRegression.predict` is converted to a NumPy array before it is used in `_CalibratedClassifier`.\n\nLet's create the necessary search and replace edits to fix this issue.\n\n```python\n### sklearn/isotonic.py\n<<<<<<< SEARCH\n        return res\n=======\n        import pandas as pd\n        if isinstance(res, pd.DataFrame):\n            res = res.to_numpy()\n        return res\n>>>>>>> REPLACE\n```\n\nThis edit ensures that the `transform` method of `IsotonicRegression` returns a NumPy array even if `transform_output` is set to \"pandas\".\n\n```python\n### sklearn/calibration.py\n<<<<<<< SEARCH\n            proba[:, class_idx] = calibrator.predict(this_pred)\n=======\n            pred = calibrator.predict(this_pred)\n            if hasattr(pred, \"to_numpy\"):\n                pred = pred.to_numpy()\n            proba[:, class_idx] = pred\n>>>>>>> REPLACE\n```\n\nThis edit ensures that in `_CalibratedClassifier.predict_proba`, the prediction from the calibrator is converted to a NumPy array if it is a pandas DataFrame.\n\nAfter making these changes, the `CalibratedClassifierCV` should work correctly even when `set_config(transform_output=\"pandas\")` is set.", "Based on the issue described, the problem arises because the `IsotonicRegression` class's `predict` method returns a DataFrame instead of a NumPy array when `set_config(transform_output=\"pandas\")` is set. The `_CalibratedClassifier` class expects a NumPy array when calling `calibrator.predict`.\n\nWe need to make sure that the `predict` method in `_CalibratedClassifier` handles this correctly. Therefore, we will convert the DataFrame returned by `calibrator.predict` back to a NumPy array before using it.\n\nHere is the `SEARCH/REPLACE` edit to fix the issue:\n\n```python\n### sklearn/calibration.py\n<<<<<<< SEARCH\n            proba[:, class_idx] = calibrator.predict(this_pred)\n=======\n            proba[:, class_idx] = np.asarray(calibrator.predict(this_pred)).reshape(-1)\n>>>>>>> REPLACE\n```\n\nThis change ensures that whatever the `calibrator.predict` method returns, it is converted to a NumPy array, which is then reshaped to a 1D array before being assigned to the `proba` array.\n\nThis edit addresses the issue described in the provided test case as well as in the issue statement.", "To address the issue, we need to ensure that `IsotonicRegression.predict` method in `sklearn/isotonic.py` correctly handles the configuration where the `transform_output` is set to `\"pandas\"`. Specifically, we need to make sure that it converts the result to a NumPy array if it returns a DataFrame.\n\nHere are the SEARCH/REPLACE edits to fix the issue:\n\n```python\n### sklearn/isotonic.py\n<<<<<<< SEARCH\n        return res.astype(T.dtype)\n=======\n        # Ensure the output is a numpy array if transform_output is set to pandas\n        res = res.astype(T.dtype)\n        if hasattr(sklearn, 'get_config') and sklearn.get_config()['transform_output'] == 'pandas':\n            res = res.to_numpy()\n        return res\n>>>>>>> REPLACE\n```\n\n```python\n### sklearn/calibration.py\n<<<<<<< SEARCH\n            proba[:, class_idx] = calibrator.predict(this_pred)\n=======\n            pred = calibrator.predict(this_pred)\n            if isinstance(pred, pd.DataFrame):\n                pred = pred.to_numpy().flatten()\n            proba[:, class_idx] = pred\n>>>>>>> REPLACE\n```", "The issue arises because the `IsotonicRegression`'s `predict` method returns a DataFrame instead of a numpy array when `set_config(transform_output=\"pandas\")` is set. This DataFrame causes issues when the `_CalibratedClassifier` tries to assign these predictions to a numpy array.\n\nTo fix this, we need to ensure that the `predict` method of `IsotonicRegression` always returns a numpy array, regardless of the output configuration.\n\nLet's make the necessary modifications:\n\n1. **In `sklearn/isotonic.py`, force `predict` to always return a numpy array.**\n\nHere's the *SEARCH/REPLACE* edit:\n\n```python\n### sklearn/isotonic.py\n<<<<<<< SEARCH\n        return res\n=======\n        return np.asarray(res)\n>>>>>>> REPLACE\n```\n\nNow, let's apply the change in the file `sklearn/isotonic.py`:\n\n```python\n### sklearn/isotonic.py\n<<<<<<< SEARCH\n        return res\n=======\n        return np.asarray(res)\n>>>>>>> REPLACE\n```\n\nThis change ensures that `IsotonicRegression.predict` always returns a numpy array, which will then be compatible with the `_CalibratedClassifier`.\n\nAfter applying this change, you should run the tests to ensure that the issue is resolved and that the fix doesn't introduce any new issues. If the tests pass, this means the problem is fixed.", "To fix the issue with `CalibratedClassifierCV` when using `set_config(transform_output=\"pandas\")`, we need to ensure that the `IsotonicRegression` predict method returns a numpy array even if the output configuration is set to pandas. We can achieve this by modifying the `predict` method of the `IsotonicRegression` class.\n\nHere are the necessary *SEARCH/REPLACE* edits:\n\n```python\n### sklearn/isotonic.py\n<<<<<<< SEARCH\n        return self.transform(T)\n=======\n        return np.asarray(self.transform(T))\n>>>>>>> REPLACE\n```\n\nThis ensures that the `predict` method of `IsotonicRegression` will always return a numpy array, which avoids the broadcasting issue in `_CalibratedClassifier`.\n\nApply this change in the `sklearn/isotonic.py` file. The `predict` method should convert the result of `transform` into a numpy array before returning it. This simple change should resolve the issue described.", "```python\n### sklearn/calibration.py\n<<<<<<< SEARCH\n        for class_idx, this_pred, calibrator in zip(\n            pos_class_indices, predictions.T, self.calibrators\n        ):\n=======\n        for class_idx, this_pred, calibrator in zip(\n            pos_class_indices, predictions.T, self.calibrators\n        ):\n            if isinstance(this_pred, pd.DataFrame):\n                this_pred = this_pred.values.flatten()\n>>>>>>> REPLACE\n```\n\nThis change ensures that if `this_pred` is a pandas DataFrame, we convert it to a numpy array before using it in the further computations. This should address the error where the Isotonic Regression returns a DataFrame which causes a shape mismatch during the assignment to the numpy array in `predict_proba`.", "Based on the provided issue description and your code, it seems that the Isotonic Regression in `CalibratedClassifierCV` does not handle the `pandas` DataFrame output properly when `set_config(transform_output=\"pandas\")` is set. We need to ensure that `_CalibratedClassifier` can handle DataFrame outputs correctly by converting them back to numpy arrays.\n\nLet's make the necessary adjustments in the source code.\n\n```python\n### sklearn/calibration.py\n<<<<<<< SEARCH\n            proba[:, class_idx] = calibrator.predict(this_pred)\n=======\n            this_pred_np = np.asarray(this_pred)  # Ensure this_pred is a numpy array\n            proba[:, class_idx] = calibrator.predict(this_pred_np)\n>>>>>>> REPLACE\n```\n\nAnd we also need to add a unit test to verify this change:\n\n```python\n### sklearn/tests/test_isotonic.py\n<<<<<<< SEARCH\n        y_pred = regressor.predict(X)\n=======\n        y_pred = regressor.predict(np.asarray(X))  # Ensure the input is a numpy array\n>>>>>>> REPLACE\n```\n\nThis should handle the conversion from DataFrame to numpy array, resolving the broadcasting issue in `_CalibratedClassifier`. \n\nIf there are any other areas that interact with IsotonicRegression's predict method and need handling of pandas DataFrames, the same conversion can be applied there as well.\n", "### Bug Localization\n\nThe issue arises when using `CalibratedClassifierCV` with isotonic regression and setting `set_config(transform_output=\"pandas\")`. The error occurs because `IsotonicRegression` returns a DataFrame, but `CalibratedClassifierCV` expects a NumPy array.\n\nFrom the provided code segments, the `predict_proba` method in `_CalibratedClassifier` (located in `sklearn/calibration.py`) is where the error occurs. The method attempts to assign the result of `calibrator.predict(this_pred)` to a NumPy array, but if the result is a DataFrame, this causes the error.\n\n### Code Fix\n\nWe need to modify the `IsotonicRegression` predict method to ensure it always returns a NumPy array, regardless of the configuration.\n\nHere are the modifications:\n\n```python\n### sklearn/isotonic.py\n<<<<<<< SEARCH\n        return res.astype(T.dtype)\n=======\n        if sklearn.get_config()['transform_output'] == 'pandas':\n            return pd.DataFrame(res.astype(T.dtype))\n        return res.astype(T.dtype)\n>>>>>>> REPLACE\n\n\n### sklearn/calibration.py\n<<<<<<< SEARCH\n            proba[:, class_idx] = calibrator.predict(this_pred)\n=======\n            pred = calibrator.predict(this_pred)\n            if hasattr(pred, 'to_numpy'):\n                pred = pred.to_numpy()\n            proba[:, class_idx] = pred\n>>>>>>> REPLACE\n```\n\n### Explanation:\n\n1. **In `sklearn/isotonic.py`**:\n   - We modify the `transform` method to check the `transform_output` configuration. If it is set to \"pandas\", it returns a DataFrame; otherwise, it returns a NumPy array as usual.\n\n2. **In `sklearn/calibration.py`**:\n   - We modify the `predict_proba` method to handle the case where `calibrator.predict(this_pred)` returns a DataFrame. If it does, we convert it to a NumPy array before assigning it to the `proba` array.\n\nBy making these changes, we ensure compatibility with the pandas output configuration while maintaining the expected behavior for `CalibratedClassifierCV`.", "Sure! Based on the issue statement and the code segments provided, it seems the issue is that `IsotonicRegression.predict` returns a `pandas.DataFrame` when `set_config(transform_output=\"pandas\")` is set, which causes a broadcasting error in `_CalibratedClassifier.predict_proba`.\n\nWe need to ensure that `IsotonicRegression.predict` always returns a `numpy.ndarray`, even when the `transform_output` is set to \"pandas\". The relevant function to update is `predict` in `IsotonicRegression`.\n\nLet's create the search/replace edits for this:\n\n```python\n### sklearn/isotonic.py\n<<<<<<< SEARCH\n        return res\n=======\n        return np.asarray(res)\n>>>>>>> REPLACE\n```\n\nThis ensures that the result from `IsotonicRegression.transform` (and consequently `predict`) is always converted to a `numpy.ndarray`.\n\nAdditionally, there may be a need to update the `_CalibratedClassifier.predict_proba` to handle pandas DataFrames correctly. But since transforming the output to `numpy.ndarray` should fix the issue in `IsotonicRegression`, let's apply this fix first.\n\nIf additional changes are required in `_CalibratedClassifier`, we will address that after this initial fix."], "all_generations": [["To fix the issue with `CalibratedClassifierCV` when using `IsotonicRegression` and `set_config(transform_output=\"pandas\")`, we need to ensure that the output of `IsotonicRegression.predict` is always a NumPy array, even when the configuration is set to output pandas DataFrames.\n\nHere are the necessary changes:\n\n1. Modify the `IsotonicRegression.predict` method to ensure it always returns a NumPy array.\n2. Update the `_CalibratedClassifier.predict_proba` method to handle the case where `calibrator.predict` might return a pandas DataFrame.\n\nLet's make these changes:\n\n```python\n### sklearn/isotonic.py\n<<<<<<< SEARCH\n        return self.transform(T)\n=======\n        return np.asarray(self.transform(T))\n>>>>>>> REPLACE\n```\n\n```python\n### sklearn/calibration.py\n<<<<<<< SEARCH\n            proba[:, class_idx] = calibrator.predict(this_pred)\n=======\n            proba[:, class_idx] = np.asarray(calibrator.predict(this_pred)).reshape(-1)\n>>>>>>> REPLACE\n```\n\nThese changes ensure that the `IsotonicRegression.predict` method always returns a NumPy array and that the `_CalibratedClassifier.predict_proba` method correctly handles the output, even if it is a pandas DataFrame.", "Based on the provided issue description and the provided code segments, the root cause of the issue is that the `predict_proba` method in the `_CalibratedClassifier` class does not handle the case when the `IsotonicRegression.predict` method returns a pandas DataFrame. The `IsotonicRegression.predict` method should return a NumPy array instead. We need to ensure that the output of `IsotonicRegression.predict` is converted to a NumPy array before it is used in `_CalibratedClassifier`.\n\nLet's create the necessary search and replace edits to fix this issue.\n\n```python\n### sklearn/isotonic.py\n<<<<<<< SEARCH\n        return res\n=======\n        import pandas as pd\n        if isinstance(res, pd.DataFrame):\n            res = res.to_numpy()\n        return res\n>>>>>>> REPLACE\n```\n\nThis edit ensures that the `transform` method of `IsotonicRegression` returns a NumPy array even if `transform_output` is set to \"pandas\".\n\n```python\n### sklearn/calibration.py\n<<<<<<< SEARCH\n            proba[:, class_idx] = calibrator.predict(this_pred)\n=======\n            pred = calibrator.predict(this_pred)\n            if hasattr(pred, \"to_numpy\"):\n                pred = pred.to_numpy()\n            proba[:, class_idx] = pred\n>>>>>>> REPLACE\n```\n\nThis edit ensures that in `_CalibratedClassifier.predict_proba`, the prediction from the calibrator is converted to a NumPy array if it is a pandas DataFrame.\n\nAfter making these changes, the `CalibratedClassifierCV` should work correctly even when `set_config(transform_output=\"pandas\")` is set.", "Based on the issue described, the problem arises because the `IsotonicRegression` class's `predict` method returns a DataFrame instead of a NumPy array when `set_config(transform_output=\"pandas\")` is set. The `_CalibratedClassifier` class expects a NumPy array when calling `calibrator.predict`.\n\nWe need to make sure that the `predict` method in `_CalibratedClassifier` handles this correctly. Therefore, we will convert the DataFrame returned by `calibrator.predict` back to a NumPy array before using it.\n\nHere is the `SEARCH/REPLACE` edit to fix the issue:\n\n```python\n### sklearn/calibration.py\n<<<<<<< SEARCH\n            proba[:, class_idx] = calibrator.predict(this_pred)\n=======\n            proba[:, class_idx] = np.asarray(calibrator.predict(this_pred)).reshape(-1)\n>>>>>>> REPLACE\n```\n\nThis change ensures that whatever the `calibrator.predict` method returns, it is converted to a NumPy array, which is then reshaped to a 1D array before being assigned to the `proba` array.\n\nThis edit addresses the issue described in the provided test case as well as in the issue statement.", "To address the issue, we need to ensure that `IsotonicRegression.predict` method in `sklearn/isotonic.py` correctly handles the configuration where the `transform_output` is set to `\"pandas\"`. Specifically, we need to make sure that it converts the result to a NumPy array if it returns a DataFrame.\n\nHere are the SEARCH/REPLACE edits to fix the issue:\n\n```python\n### sklearn/isotonic.py\n<<<<<<< SEARCH\n        return res.astype(T.dtype)\n=======\n        # Ensure the output is a numpy array if transform_output is set to pandas\n        res = res.astype(T.dtype)\n        if hasattr(sklearn, 'get_config') and sklearn.get_config()['transform_output'] == 'pandas':\n            res = res.to_numpy()\n        return res\n>>>>>>> REPLACE\n```\n\n```python\n### sklearn/calibration.py\n<<<<<<< SEARCH\n            proba[:, class_idx] = calibrator.predict(this_pred)\n=======\n            pred = calibrator.predict(this_pred)\n            if isinstance(pred, pd.DataFrame):\n                pred = pred.to_numpy().flatten()\n            proba[:, class_idx] = pred\n>>>>>>> REPLACE\n```", "The issue arises because the `IsotonicRegression`'s `predict` method returns a DataFrame instead of a numpy array when `set_config(transform_output=\"pandas\")` is set. This DataFrame causes issues when the `_CalibratedClassifier` tries to assign these predictions to a numpy array.\n\nTo fix this, we need to ensure that the `predict` method of `IsotonicRegression` always returns a numpy array, regardless of the output configuration.\n\nLet's make the necessary modifications:\n\n1. **In `sklearn/isotonic.py`, force `predict` to always return a numpy array.**\n\nHere's the *SEARCH/REPLACE* edit:\n\n```python\n### sklearn/isotonic.py\n<<<<<<< SEARCH\n        return res\n=======\n        return np.asarray(res)\n>>>>>>> REPLACE\n```\n\nNow, let's apply the change in the file `sklearn/isotonic.py`:\n\n```python\n### sklearn/isotonic.py\n<<<<<<< SEARCH\n        return res\n=======\n        return np.asarray(res)\n>>>>>>> REPLACE\n```\n\nThis change ensures that `IsotonicRegression.predict` always returns a numpy array, which will then be compatible with the `_CalibratedClassifier`.\n\nAfter applying this change, you should run the tests to ensure that the issue is resolved and that the fix doesn't introduce any new issues. If the tests pass, this means the problem is fixed.", "To fix the issue with `CalibratedClassifierCV` when using `set_config(transform_output=\"pandas\")`, we need to ensure that the `IsotonicRegression` predict method returns a numpy array even if the output configuration is set to pandas. We can achieve this by modifying the `predict` method of the `IsotonicRegression` class.\n\nHere are the necessary *SEARCH/REPLACE* edits:\n\n```python\n### sklearn/isotonic.py\n<<<<<<< SEARCH\n        return self.transform(T)\n=======\n        return np.asarray(self.transform(T))\n>>>>>>> REPLACE\n```\n\nThis ensures that the `predict` method of `IsotonicRegression` will always return a numpy array, which avoids the broadcasting issue in `_CalibratedClassifier`.\n\nApply this change in the `sklearn/isotonic.py` file. The `predict` method should convert the result of `transform` into a numpy array before returning it. This simple change should resolve the issue described.", "```python\n### sklearn/calibration.py\n<<<<<<< SEARCH\n        for class_idx, this_pred, calibrator in zip(\n            pos_class_indices, predictions.T, self.calibrators\n        ):\n=======\n        for class_idx, this_pred, calibrator in zip(\n            pos_class_indices, predictions.T, self.calibrators\n        ):\n            if isinstance(this_pred, pd.DataFrame):\n                this_pred = this_pred.values.flatten()\n>>>>>>> REPLACE\n```\n\nThis change ensures that if `this_pred` is a pandas DataFrame, we convert it to a numpy array before using it in the further computations. This should address the error where the Isotonic Regression returns a DataFrame which causes a shape mismatch during the assignment to the numpy array in `predict_proba`.", "Based on the provided issue description and your code, it seems that the Isotonic Regression in `CalibratedClassifierCV` does not handle the `pandas` DataFrame output properly when `set_config(transform_output=\"pandas\")` is set. We need to ensure that `_CalibratedClassifier` can handle DataFrame outputs correctly by converting them back to numpy arrays.\n\nLet's make the necessary adjustments in the source code.\n\n```python\n### sklearn/calibration.py\n<<<<<<< SEARCH\n            proba[:, class_idx] = calibrator.predict(this_pred)\n=======\n            this_pred_np = np.asarray(this_pred)  # Ensure this_pred is a numpy array\n            proba[:, class_idx] = calibrator.predict(this_pred_np)\n>>>>>>> REPLACE\n```\n\nAnd we also need to add a unit test to verify this change:\n\n```python\n### sklearn/tests/test_isotonic.py\n<<<<<<< SEARCH\n        y_pred = regressor.predict(X)\n=======\n        y_pred = regressor.predict(np.asarray(X))  # Ensure the input is a numpy array\n>>>>>>> REPLACE\n```\n\nThis should handle the conversion from DataFrame to numpy array, resolving the broadcasting issue in `_CalibratedClassifier`. \n\nIf there are any other areas that interact with IsotonicRegression's predict method and need handling of pandas DataFrames, the same conversion can be applied there as well.\n", "### Bug Localization\n\nThe issue arises when using `CalibratedClassifierCV` with isotonic regression and setting `set_config(transform_output=\"pandas\")`. The error occurs because `IsotonicRegression` returns a DataFrame, but `CalibratedClassifierCV` expects a NumPy array.\n\nFrom the provided code segments, the `predict_proba` method in `_CalibratedClassifier` (located in `sklearn/calibration.py`) is where the error occurs. The method attempts to assign the result of `calibrator.predict(this_pred)` to a NumPy array, but if the result is a DataFrame, this causes the error.\n\n### Code Fix\n\nWe need to modify the `IsotonicRegression` predict method to ensure it always returns a NumPy array, regardless of the configuration.\n\nHere are the modifications:\n\n```python\n### sklearn/isotonic.py\n<<<<<<< SEARCH\n        return res.astype(T.dtype)\n=======\n        if sklearn.get_config()['transform_output'] == 'pandas':\n            return pd.DataFrame(res.astype(T.dtype))\n        return res.astype(T.dtype)\n>>>>>>> REPLACE\n\n\n### sklearn/calibration.py\n<<<<<<< SEARCH\n            proba[:, class_idx] = calibrator.predict(this_pred)\n=======\n            pred = calibrator.predict(this_pred)\n            if hasattr(pred, 'to_numpy'):\n                pred = pred.to_numpy()\n            proba[:, class_idx] = pred\n>>>>>>> REPLACE\n```\n\n### Explanation:\n\n1. **In `sklearn/isotonic.py`**:\n   - We modify the `transform` method to check the `transform_output` configuration. If it is set to \"pandas\", it returns a DataFrame; otherwise, it returns a NumPy array as usual.\n\n2. **In `sklearn/calibration.py`**:\n   - We modify the `predict_proba` method to handle the case where `calibrator.predict(this_pred)` returns a DataFrame. If it does, we convert it to a NumPy array before assigning it to the `proba` array.\n\nBy making these changes, we ensure compatibility with the pandas output configuration while maintaining the expected behavior for `CalibratedClassifierCV`.", "Sure! Based on the issue statement and the code segments provided, it seems the issue is that `IsotonicRegression.predict` returns a `pandas.DataFrame` when `set_config(transform_output=\"pandas\")` is set, which causes a broadcasting error in `_CalibratedClassifier.predict_proba`.\n\nWe need to ensure that `IsotonicRegression.predict` always returns a `numpy.ndarray`, even when the `transform_output` is set to \"pandas\". The relevant function to update is `predict` in `IsotonicRegression`.\n\nLet's create the search/replace edits for this:\n\n```python\n### sklearn/isotonic.py\n<<<<<<< SEARCH\n        return res\n=======\n        return np.asarray(res)\n>>>>>>> REPLACE\n```\n\nThis ensures that the result from `IsotonicRegression.transform` (and consequently `predict`) is always converted to a `numpy.ndarray`.\n\nAdditionally, there may be a need to update the `_CalibratedClassifier.predict_proba` to handle pandas DataFrames correctly. But since transforming the output to `numpy.ndarray` should fix the issue in `IsotonicRegression`, let's apply this fix first.\n\nIf additional changes are required in `_CalibratedClassifier`, we will address that after this initial fix."]], "try_count": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], "traj": [{"response": "To fix the issue with `CalibratedClassifierCV` when using `IsotonicRegression` and `set_config(transform_output=\"pandas\")`, we need to ensure that the output of `IsotonicRegression.predict` is always a NumPy array, even when the configuration is set to output pandas DataFrames.\n\nHere are the necessary changes:\n\n1. Modify the `IsotonicRegression.predict` method to ensure it always returns a NumPy array.\n2. Update the `_CalibratedClassifier.predict_proba` method to handle the case where `calibrator.predict` might return a pandas DataFrame.\n\nLet's make these changes:\n\n```python\n### sklearn/isotonic.py\n<<<<<<< SEARCH\n        return self.transform(T)\n=======\n        return np.asarray(self.transform(T))\n>>>>>>> REPLACE\n```\n\n```python\n### sklearn/calibration.py\n<<<<<<< SEARCH\n            proba[:, class_idx] = calibrator.predict(this_pred)\n=======\n            proba[:, class_idx] = np.asarray(calibrator.predict(this_pred)).reshape(-1)\n>>>>>>> REPLACE\n```\n\nThese changes ensure that the `IsotonicRegression.predict` method always returns a NumPy array and that the `_CalibratedClassifier.predict_proba` method correctly handles the output, even if it is a pandas DataFrame.", "usage": {"completion_tokens": 262, "prompt_tokens": 6042}, "prompt": "We are currently solving the following issue within our repository. Here is the issue text:\n--- BEGIN ISSUE ---\nCalibratedClassifierCV doesn't work with `set_config(transform_output=\"pandas\")`\n### Describe the bug\r\n\r\nCalibratedClassifierCV with isotonic regression doesn't work when we previously set `set_config(transform_output=\"pandas\")`.\r\nThe IsotonicRegression seems to return a dataframe, which is a problem for `_CalibratedClassifier`  in `predict_proba` where it tries to put the dataframe in a numpy array row `proba[:, class_idx] = calibrator.predict(this_pred)`.\r\n\r\n### Steps/Code to Reproduce\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn import set_config\r\nfrom sklearn.calibration import CalibratedClassifierCV\r\nfrom sklearn.linear_model import SGDClassifier\r\n\r\nset_config(transform_output=\"pandas\")\r\nmodel = CalibratedClassifierCV(SGDClassifier(), method='isotonic')\r\nmodel.fit(np.arange(90).reshape(30, -1), np.arange(30) % 2)\r\nmodel.predict(np.arange(90).reshape(30, -1))\r\n```\r\n\r\n### Expected Results\r\n\r\nIt should not crash.\r\n\r\n### Actual Results\r\n\r\n```\r\n../core/model_trainer.py:306: in train_model\r\n    cv_predictions = cross_val_predict(pipeline,\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:968: in cross_val_predict\r\n    predictions = parallel(\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:1085: in __call__\r\n    if self.dispatch_one_batch(iterator):\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:901: in dispatch_one_batch\r\n    self._dispatch(tasks)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:819: in _dispatch\r\n    job = self._backend.apply_async(batch, callback=cb)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/_parallel_backends.py:208: in apply_async\r\n    result = ImmediateResult(func)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/_parallel_backends.py:597: in __init__\r\n    self.results = batch()\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:288: in __call__\r\n    return [func(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:288: in <listcomp>\r\n    return [func(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/utils/fixes.py:117: in __call__\r\n    return self.function(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:1052: in _fit_and_predict\r\n    predictions = func(X_test)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/pipeline.py:548: in predict_proba\r\n    return self.steps[-1][1].predict_proba(Xt, **predict_proba_params)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/calibration.py:477: in predict_proba\r\n    proba = calibrated_classifier.predict_proba(X)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/calibration.py:764: in predict_proba\r\n    proba[:, class_idx] = calibrator.predict(this_pred)\r\nE   ValueError: could not broadcast input array from shape (20,1) into shape (20,)\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.9.15 (main, Nov 24 2022, 14:31:59)  [GCC 11.2.0]\r\nexecutable: /home/philippe/.anaconda3/envs/strategy-training/bin/python\r\n   machine: Linux-5.15.0-57-generic-x86_64-with-glibc2.31\r\n\r\nPython dependencies:\r\n      sklearn: 1.2.0\r\n          pip: 22.2.2\r\n   setuptools: 62.3.2\r\n        numpy: 1.23.5\r\n        scipy: 1.9.3\r\n       Cython: None\r\n       pandas: 1.4.1\r\n   matplotlib: 3.6.3\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libgomp\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0\r\n        version: None\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/numpy.libs/libopenblas64_p-r0-742d56dc.3.20.so\r\n        version: 0.3.20\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/scipy.libs/libopenblasp-r0-41284840.3.18.so\r\n        version: 0.3.18\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n```\r\n\n\n--- END ISSUE ---\n\n\nWe also have a fail to pass unit test for the given issue. We can use this to help identify and fix the issue. Here is the patch text:\n--- BEGIN PATCH ---\ndiff --git a/sklearn/tests/test_isotonic.py b/sklearn/tests/test_isotonic.py\n--- a/sklearn/tests/test_isotonic.py\n+++ b/sklearn/tests/test_isotonic.py\n@@ -5,6 +5,7 @@\n \n import pytest\n \n+import sklearn\n from sklearn.datasets import make_regression\n from sklearn.isotonic import (\n     check_increasing,\n@@ -680,3 +681,24 @@ def test_get_feature_names_out(shape):\n     assert isinstance(names, np.ndarray)\n     assert names.dtype == object\n     assert_array_equal([\"isotonicregression0\"], names)\n+\n+\n+def test_isotonic_regression_output_predict():\n+    \"\"\"Check that `predict` does return the expected output type.\n+\n+    We need to check that `transform` will output a DataFrame and a NumPy array\n+    when we set `transform_output` to `pandas`.\n+\n+    Non-regression test for:\n+    https://github.com/scikit-learn/scikit-learn/issues/25499\n+    \"\"\"\n+    pd = pytest.importorskip(\"pandas\")\n+    X, y = make_regression(n_samples=10, n_features=1, random_state=42)\n+    regressor = IsotonicRegression()\n+    with sklearn.config_context(transform_output=\"pandas\"):\n+        regressor.fit(X, y)\n+        X_trans = regressor.transform(X)\n+        y_pred = regressor.predict(X)\n+\n+    assert isinstance(X_trans, pd.DataFrame)\n+    assert isinstance(y_pred, np.ndarray)\n\n--- END PATCH ---\n\n\n\nBelow are some code segments, each from a relevant file. One or more of these files may contain bugs.\n\n--- BEGIN FILE ---\n```\n### sklearn/calibration.py\n...\n    \"\"\"\n    if hasattr(clf, \"decision_function\"):\n        method = getattr(clf, \"decision_function\")\n        return method, \"decision_function\"\n\n    if hasattr(clf, \"predict_proba\"):\n        method = getattr(clf, \"predict_proba\")\n        return method, \"predict_proba\"\n\n\ndef _compute_predictions(pred_method, method_name, X, n_classes):\n    \"\"\"Return predictions for `X` and reshape binary outputs to shape\n    (n_samples, 1).\n\n    Parameters\n    ----------\n    pred_method : callable\n        Prediction method.\n\n    method_name: str\n        Name of the prediction method\n\n    X : array-like or None\n        Data used to obtain predictions.\n\n    n_classes : int\n        Number of classes present.\n\n    Returns\n    -------\n    predictions : array-like, shape (X.shape[0], len(clf.classes_))\n        The predictions. Note if there are 2 classes, array is of shape\n        (X.shape[0], 1).\n    \"\"\"\n    predictions = pred_method(X=X)\n\n    if method_name == \"decision_function\":\n        if predictions.ndim == 1:\n            predictions = predictions[:, np.newaxis]\n    elif method_name == \"predict_proba\":\n        if n_classes == 2:\n            predictions = predictions[:, 1:]\n    else:  # pragma: no cover\n        # this branch should be unreachable.\n        raise ValueError(f\"Invalid prediction method: {method_name}\")\n    return predictions\n\n\ndef _fit_calibrator(clf, predictions, y, classes, method, sample_weight=None):\n    \"\"\"Fit calibrator(s) and return a `_CalibratedClassifier`\n    instance.\n\n    `n_classes` (i.e. `len(clf.classes_)`) calibrators are fitted.\n    However, if `n_classes` equals 2, one calibrator is fitted.\n\n    Parameters\n    ----------\n    clf : estimator instance\n        Fitted classifier.\n\n    predictions : array-like, shape (n_samples, n_classes) or (n_samples, 1) \\\n                    when binary.\n        Raw predictions returned by the un-calibrated base classifier.\n\n    y : array-like, shape (n_samples,)\n        The targets.\n\n    classes : ndarray, shape (n_classes,)\n        All the prediction classes.\n\n    method : {'sigmoid', 'isotonic'}\n        The method to use for calibration.\n\n    sample_weight : ndarray, shape (n_samples,), default=None\n        Sample weights. If None, then samples are equally weighted.\n\n    Returns\n    -------\n    pipeline : _CalibratedClassifier instance\n    \"\"\"\n    Y = label_binarize(y, classes=classes)\n    label_encoder = LabelEncoder().fit(classes)\n    pos_class_indices = label_encoder.transform(clf.classes_)\n    calibrators = []\n    for class_idx, this_pred in zip(pos_class_indices, predictions.T):\n        if method == \"isotonic\":\n            calibrator = IsotonicRegression(out_of_bounds=\"clip\")\n        else:  # \"sigmoid\"\n            calibrator = _SigmoidCalibration()\n        calibrator.fit(this_pred, Y[:, class_idx], sample_weight)\n        calibrators.append(calibrator)\n\n    pipeline = _CalibratedClassifier(clf, calibrators, method=method, classes=classes)\n    return pipeline\n\n\nclass _CalibratedClassifier:\n    \"\"\"Pipeline-like chaining a fitted classifier and its fitted calibrators.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Fitted classifier.\n\n    calibrators : list of fitted estimator instances\n        List of fitted calibrators (either 'IsotonicRegression' or\n        '_SigmoidCalibration'). The number of calibrators equals the number of\n        classes. However, if there are 2 classes, the list contains only one\n        fitted calibrator.\n\n    classes : array-like of shape (n_classes,)\n        All the prediction classes.\n\n    method : {'sigmoid', 'isotonic'}, default='sigmoid'\n        The method to use for calibration. Can be 'sigmoid' which\n        corresponds to Platt's method or 'isotonic' which is a\n        non-parametric approach based on isotonic regression.\n    \"\"\"\n\n    def __init__(self, estimator, calibrators, *, classes, method=\"sigmoid\"):\n        self.estimator = estimator\n        self.calibrators = calibrators\n        self.classes = classes\n        self.method = method\n\n    def predict_proba(self, X):\n        \"\"\"Calculate calibrated probabilities.\n\n        Calculates classification calibrated probabilities\n        for each class, in a one-vs-all manner, for `X`.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            The sample data.\n\n        Returns\n        -------\n        proba : array, shape (n_samples, n_classes)\n            The predicted probabilities. Can be exact zeros.\n        \"\"\"\n        n_classes = len(self.classes)\n        pred_method, method_name = _get_prediction_method(self.estimator)\n        predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n\n        label_encoder = LabelEncoder().fit(self.classes)\n        pos_class_indices = label_encoder.transform(self.estimator.classes_)\n\n        proba = np.zeros((_num_samples(X), n_classes))\n        for class_idx, this_pred, calibrator in zip(\n            pos_class_indices, predictions.T, self.calibrators\n        ):\n            if n_classes == 2:\n                # When binary, `predictions` consists only of predictions for\n                # clf.classes_[1] but `pos_class_indices` = 0\n                class_idx += 1\n            proba[:, class_idx] = calibrator.predict(this_pred)\n\n        # Normalize the probabilities\n        if n_classes == 2:\n            proba[:, 0] = 1.0 - proba[:, 1]\n        else:\n            denominator = np.sum(proba, axis=1)[:, np.newaxis]\n            # In the edge case where for each class calibrator returns a null\n            # probability for a given sample, use the uniform distribution\n            # instead.\n            uniform_proba = np.full_like(proba, 1 / n_classes)\n            proba = np.divide(\n                proba, denominator, out=uniform_proba, where=denominator != 0\n            )\n\n        # Deal with cases where the predicted probability minimally exceeds 1.0\n        proba[(1.0 < proba) & (proba <= 1.0 + 1e-5)] = 1.0\n\n        return proba\n\n\ndef _sigmoid_calibration(predictions, y, sample_weight=None):\n    \"\"\"Probability Calibration with sigmoid method (Platt 2000)\n\n    Parameters\n    ----------\n    predictions : ndarray of shape (n_samples,)\n        The decision function or predict proba for the samples.\n\n...\n\n\n### sklearn/isotonic.py\n...\n    if y_min is not None or y_max is not None:\n        # Older versions of np.clip don't accept None as a bound, so use np.inf\n        if y_min is None:\n            y_min = -np.inf\n        if y_max is None:\n            y_max = np.inf\n        np.clip(y, y_min, y_max, y)\n    return y[order]\n\n\nclass IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n    \"\"\"Isotonic regression model.\n\n    Read more in the :ref:`User Guide <isotonic>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    y_min : float, default=None\n        Lower bound on the lowest predicted value (the minimum value may\n        still be higher). If not set, defaults to -inf.\n\n    y_max : float, default=None\n        Upper bound on the highest predicted value (the maximum may still be\n        lower). If not set, defaults to +inf.\n\n    increasing : bool or 'auto', default=True\n        Determines whether the predictions should be constrained to increase\n        or decrease with `X`. 'auto' will decide based on the Spearman\n        correlation estimate's sign.\n\n    out_of_bounds : {'nan', 'clip', 'raise'}, default='nan'\n        Handles how `X` values outside of the training domain are handled\n        during prediction.\n\n        - 'nan', predictions will be NaN.\n        - 'clip', predictions will be set to the value corresponding to\n          the nearest train interval endpoint.\n        - 'raise', a `ValueError` is raised.\n\n    Attributes\n    ----------\n    X_min_ : float\n        Minimum value of input array `X_` for left bound.\n\n    X_max_ : float\n        Maximum value of input array `X_` for right bound.\n\n    X_thresholds_ : ndarray of shape (n_thresholds,)\n        Unique ascending `X` values used to interpolate\n        the y = f(X) monotonic function.\n\n        .. versionadded:: 0.24\n\n    y_thresholds_ : ndarray of shape (n_thresholds,)\n        De-duplicated `y` values suitable to interpolate the y = f(X)\n        monotonic function.\n\n        .. versionadded:: 0.24\n\n    f_ : function\n        The stepwise interpolating function that covers the input domain ``X``.\n\n    increasing_ : bool\n        Inferred value for ``increasing``.\n\n    See Also\n    --------\n    sklearn.linear_model.LinearRegression : Ordinary least squares Linear\n        Regression.\n    sklearn.ensemble.HistGradientBoostingRegressor : Gradient boosting that\n        is a non-parametric model accepting monotonicity constraints.\n    isotonic_regression : Function to solve the isotonic regression model.\n\n    Notes\n    -----\n    Ties are broken using the secondary method from de Leeuw, 1977.\n\n    References\n    ----------\n    Isotonic Median Regression: A Linear Programming Approach\n    Nilotpal Chakravarti\n    Mathematics of Operations Research\n    Vol. 14, No. 2 (May, 1989), pp. 303-308\n\n    Isotone Optimization in R : Pool-Adjacent-Violators\n    Algorithm (PAVA) and Active Set Methods\n    de Leeuw, Hornik, Mair\n    Journal of Statistical Software 2009\n\n    Correctness of Kruskal's algorithms for monotone regression with ties\n    de Leeuw, Psychometrica, 1977\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.isotonic import IsotonicRegression\n    >>> X, y = make_regression(n_samples=10, n_features=1, random_state=41)\n    >>> iso_reg = IsotonicRegression().fit(X, y)\n    >>> iso_reg.predict([.1, .2])\n    array([1.8628..., 3.7256...])\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"y_min\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"y_max\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"increasing\": [\"boolean\", StrOptions({\"auto\"})],\n        \"out_of_bounds\": [StrOptions({\"nan\", \"clip\", \"raise\"})],\n    }\n\n    def __init__(self, *, y_min=None, y_max=None, increasing=True, out_of_bounds=\"nan\"):\n        self.y_min = y_min\n        self.y_max = y_max\n        self.increasing = increasing\n        self.out_of_bounds = out_of_bounds\n\n    def _check_input_data_shape(self, X):\n        if not (X.ndim == 1 or (X.ndim == 2 and X.shape[1] == 1)):\n            msg = (\n                \"Isotonic regression input X should be a 1d array or \"\n                \"2d array with 1 feature\"\n            )\n            raise ValueError(msg)\n\n    def _build_f(self, X, y):\n        \"\"\"Build the f_ interp1d function.\"\"\"\n\n        bounds_error = self.out_of_bounds == \"raise\"\n        if len(y) == 1:\n            # single y, constant prediction\n            self.f_ = lambda x: y.repeat(x.shape)\n        else:\n            self.f_ = interpolate.interp1d(\n                X, y, kind=\"linear\", bounds_error=bounds_error\n            )\n\n    def _build_y(self, X, y, sample_weight, trim_duplicates=True):\n        \"\"\"Build the y_ IsotonicRegression.\"\"\"\n        self._check_input_data_shape(X)\n        X = X.reshape(-1)  # use 1d view\n\n        # Determine increasing if auto-determination requested\n        if self.increasing == \"auto\":\n            self.increasing_ = check_increasing(X, y)\n        else:\n            self.increasing_ = self.increasing\n\n        # If sample_weights is passed, removed zero-weight values and clean\n        # order\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        mask = sample_weight > 0\n        X, y, sample_weight = X[mask], y[mask], sample_weight[mask]\n\n        order = np.lexsort((y, X))\n        X, y, sample_weight = [array[order] for array in [X, y, sample_weight]]\n        unique_X, unique_y, unique_sample_weight = _make_unique(X, y, sample_weight)\n\n        X = unique_X\n        y = isotonic_regression(\n            unique_y,\n            sample_weight=unique_sample_weight,\n            y_min=self.y_min,\n            y_max=self.y_max,\n            increasing=self.increasing_,\n        )\n\n        # Handle the left and right bounds on X\n        self.X_min_, self.X_max_ = np.min(X), np.max(X)\n\n        if trim_duplicates:\n            # Remove unnecessary points for faster prediction\n            keep_data = np.ones((len(y),), dtype=bool)\n            # Aside from the 1st and last point, remove points whose y values\n            # are equal to both the point before and the point after it.\n            keep_data[1:-1] = np.logical_or(\n                np.not_equal(y[1:-1], y[:-2]), np.not_equal(y[1:-1], y[2:])\n            )\n            return X[keep_data], y[keep_data]\n        else:\n            # The ability to turn off trim_duplicates is only used to it make\n            # easier to unit test that removing duplicates in y does not have\n            # any impact the resulting interpolation function (besides\n            # prediction speed).\n            return X, y\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples,) or (n_samples, 1)\n            Training data.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        y : array-like of shape (n_samples,)\n            Training target.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Weights. If set to None, all weights will be set to 1 (equal\n            weights).\n\n        Returns\n        -------\n        self : object\n            Returns an instance of self.\n\n        Notes\n        -----\n        X is stored for future use, as :meth:`transform` needs X to interpolate\n        new input data.\n        \"\"\"\n        self._validate_params()\n        check_params = dict(accept_sparse=False, ensure_2d=False)\n        X = check_array(\n            X, input_name=\"X\", dtype=[np.float64, np.float32], **check_params\n        )\n        y = check_array(y, input_name=\"y\", dtype=X.dtype, **check_params)\n        check_consistent_length(X, y, sample_weight)\n\n        # Transform y by running the isotonic regression algorithm and\n        # transform X accordingly.\n        X, y = self._build_y(X, y, sample_weight)\n\n        # It is necessary to store the non-redundant part of the training set\n        # on the model to make it possible to support model persistence via\n        # the pickle module as the object built by scipy.interp1d is not\n        # picklable directly.\n        self.X_thresholds_, self.y_thresholds_ = X, y\n\n        # Build the interpolation function\n        self._build_f(X, y)\n        return self\n\n    def transform(self, T):\n        \"\"\"Transform new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            The transformed data.\n        \"\"\"\n\n        if hasattr(self, \"X_thresholds_\"):\n            dtype = self.X_thresholds_.dtype\n        else:\n            dtype = np.float64\n\n        T = check_array(T, dtype=dtype, ensure_2d=False)\n\n        self._check_input_data_shape(T)\n        T = T.reshape(-1)  # use 1d view\n\n        if self.out_of_bounds == \"clip\":\n            T = np.clip(T, self.X_min_, self.X_max_)\n\n        res = self.f_(T)\n\n        # on scipy 0.17, interp1d up-casts to float64, so we cast back\n        res = res.astype(T.dtype)\n\n        return res\n\n    def predict(self, T):\n        \"\"\"Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            Transformed data.\n        \"\"\"\n        return self.transform(T)\n\n    # We implement get_feature_names_out here instead of using\n    # `ClassNamePrefixFeaturesOutMixin`` because `input_features` are ignored.\n    # `input_features` are ignored because `IsotonicRegression` accepts 1d\n    # arrays and the semantics of `feature_names_in_` are not clear for 1d arrays.\n    def get_feature_names_out(self, input_features=None):\n        \"\"\"Get output feature names for transformation.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Ignored.\n\n        Returns\n        -------\n        feature_names_out : ndarray of str objects\n            An ndarray with one string i.e. [\"isotonicregression0\"].\n        \"\"\"\n        check_is_fitted(self, \"f_\")\n        class_name = self.__class__.__name__.lower()\n        return np.asarray([f\"{class_name}0\"], dtype=object)\n\n    def __getstate__(self):\n        \"\"\"Pickle-protocol - return state of the estimator.\"\"\"\n        state = super().__getstate__()\n        # remove interpolation method\n        state.pop(\"f_\", None)\n        return state\n\n    def __setstate__(self, state):\n        \"\"\"Pickle-protocol - set state of the estimator.\n\n        We need to rebuild the interpolation function.\n        \"\"\"\n        super().__setstate__(state)\n        if hasattr(self, \"X_thresholds_\") and hasattr(self, \"y_thresholds_\"):\n            self._build_f(self.X_thresholds_, self.y_thresholds_)\n\n    def _more_tags(self):\n        return {\"X_types\": [\"1darray\"]}\n```\n--- END FILE ---\n\nPlease first localize the bug based on the issue statement, and then generate *SEARCH/REPLACE* edits to fix the issue.\n\nEvery *SEARCH/REPLACE* edit must use this format:\n1. The file path\n2. The start of search block: <<<<<<< SEARCH\n3. A contiguous chunk of lines to search for in the existing source code\n4. The dividing line: =======\n5. The lines to replace into the source code\n6. The end of the replace block: >>>>>>> REPLACE\n\nHere is an example:\n\n```python\n### mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nPlease note that the *SEARCH/REPLACE* edit REQUIRES PROPER INDENTATION. If you would like to add the line '        print(x)', you must fully write that out, with all those spaces before the code!\nWrap the *SEARCH/REPLACE* edit in blocks ```python...```."}, {"response": "Based on the provided issue description and the provided code segments, the root cause of the issue is that the `predict_proba` method in the `_CalibratedClassifier` class does not handle the case when the `IsotonicRegression.predict` method returns a pandas DataFrame. The `IsotonicRegression.predict` method should return a NumPy array instead. We need to ensure that the output of `IsotonicRegression.predict` is converted to a NumPy array before it is used in `_CalibratedClassifier`.\n\nLet's create the necessary search and replace edits to fix this issue.\n\n```python\n### sklearn/isotonic.py\n<<<<<<< SEARCH\n        return res\n=======\n        import pandas as pd\n        if isinstance(res, pd.DataFrame):\n            res = res.to_numpy()\n        return res\n>>>>>>> REPLACE\n```\n\nThis edit ensures that the `transform` method of `IsotonicRegression` returns a NumPy array even if `transform_output` is set to \"pandas\".\n\n```python\n### sklearn/calibration.py\n<<<<<<< SEARCH\n            proba[:, class_idx] = calibrator.predict(this_pred)\n=======\n            pred = calibrator.predict(this_pred)\n            if hasattr(pred, \"to_numpy\"):\n                pred = pred.to_numpy()\n            proba[:, class_idx] = pred\n>>>>>>> REPLACE\n```\n\nThis edit ensures that in `_CalibratedClassifier.predict_proba`, the prediction from the calibrator is converted to a NumPy array if it is a pandas DataFrame.\n\nAfter making these changes, the `CalibratedClassifierCV` should work correctly even when `set_config(transform_output=\"pandas\")` is set.", "usage": {"completion_tokens": 2434, "prompt_tokens": 6042}, "prompt": "We are currently solving the following issue within our repository. Here is the issue text:\n--- BEGIN ISSUE ---\nCalibratedClassifierCV doesn't work with `set_config(transform_output=\"pandas\")`\n### Describe the bug\r\n\r\nCalibratedClassifierCV with isotonic regression doesn't work when we previously set `set_config(transform_output=\"pandas\")`.\r\nThe IsotonicRegression seems to return a dataframe, which is a problem for `_CalibratedClassifier`  in `predict_proba` where it tries to put the dataframe in a numpy array row `proba[:, class_idx] = calibrator.predict(this_pred)`.\r\n\r\n### Steps/Code to Reproduce\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn import set_config\r\nfrom sklearn.calibration import CalibratedClassifierCV\r\nfrom sklearn.linear_model import SGDClassifier\r\n\r\nset_config(transform_output=\"pandas\")\r\nmodel = CalibratedClassifierCV(SGDClassifier(), method='isotonic')\r\nmodel.fit(np.arange(90).reshape(30, -1), np.arange(30) % 2)\r\nmodel.predict(np.arange(90).reshape(30, -1))\r\n```\r\n\r\n### Expected Results\r\n\r\nIt should not crash.\r\n\r\n### Actual Results\r\n\r\n```\r\n../core/model_trainer.py:306: in train_model\r\n    cv_predictions = cross_val_predict(pipeline,\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:968: in cross_val_predict\r\n    predictions = parallel(\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:1085: in __call__\r\n    if self.dispatch_one_batch(iterator):\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:901: in dispatch_one_batch\r\n    self._dispatch(tasks)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:819: in _dispatch\r\n    job = self._backend.apply_async(batch, callback=cb)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/_parallel_backends.py:208: in apply_async\r\n    result = ImmediateResult(func)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/_parallel_backends.py:597: in __init__\r\n    self.results = batch()\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:288: in __call__\r\n    return [func(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:288: in <listcomp>\r\n    return [func(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/utils/fixes.py:117: in __call__\r\n    return self.function(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:1052: in _fit_and_predict\r\n    predictions = func(X_test)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/pipeline.py:548: in predict_proba\r\n    return self.steps[-1][1].predict_proba(Xt, **predict_proba_params)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/calibration.py:477: in predict_proba\r\n    proba = calibrated_classifier.predict_proba(X)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/calibration.py:764: in predict_proba\r\n    proba[:, class_idx] = calibrator.predict(this_pred)\r\nE   ValueError: could not broadcast input array from shape (20,1) into shape (20,)\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.9.15 (main, Nov 24 2022, 14:31:59)  [GCC 11.2.0]\r\nexecutable: /home/philippe/.anaconda3/envs/strategy-training/bin/python\r\n   machine: Linux-5.15.0-57-generic-x86_64-with-glibc2.31\r\n\r\nPython dependencies:\r\n      sklearn: 1.2.0\r\n          pip: 22.2.2\r\n   setuptools: 62.3.2\r\n        numpy: 1.23.5\r\n        scipy: 1.9.3\r\n       Cython: None\r\n       pandas: 1.4.1\r\n   matplotlib: 3.6.3\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libgomp\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0\r\n        version: None\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/numpy.libs/libopenblas64_p-r0-742d56dc.3.20.so\r\n        version: 0.3.20\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/scipy.libs/libopenblasp-r0-41284840.3.18.so\r\n        version: 0.3.18\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n```\r\n\n\n--- END ISSUE ---\n\n\nWe also have a fail to pass unit test for the given issue. We can use this to help identify and fix the issue. Here is the patch text:\n--- BEGIN PATCH ---\ndiff --git a/sklearn/tests/test_isotonic.py b/sklearn/tests/test_isotonic.py\n--- a/sklearn/tests/test_isotonic.py\n+++ b/sklearn/tests/test_isotonic.py\n@@ -5,6 +5,7 @@\n \n import pytest\n \n+import sklearn\n from sklearn.datasets import make_regression\n from sklearn.isotonic import (\n     check_increasing,\n@@ -680,3 +681,24 @@ def test_get_feature_names_out(shape):\n     assert isinstance(names, np.ndarray)\n     assert names.dtype == object\n     assert_array_equal([\"isotonicregression0\"], names)\n+\n+\n+def test_isotonic_regression_output_predict():\n+    \"\"\"Check that `predict` does return the expected output type.\n+\n+    We need to check that `transform` will output a DataFrame and a NumPy array\n+    when we set `transform_output` to `pandas`.\n+\n+    Non-regression test for:\n+    https://github.com/scikit-learn/scikit-learn/issues/25499\n+    \"\"\"\n+    pd = pytest.importorskip(\"pandas\")\n+    X, y = make_regression(n_samples=10, n_features=1, random_state=42)\n+    regressor = IsotonicRegression()\n+    with sklearn.config_context(transform_output=\"pandas\"):\n+        regressor.fit(X, y)\n+        X_trans = regressor.transform(X)\n+        y_pred = regressor.predict(X)\n+\n+    assert isinstance(X_trans, pd.DataFrame)\n+    assert isinstance(y_pred, np.ndarray)\n\n--- END PATCH ---\n\n\n\nBelow are some code segments, each from a relevant file. One or more of these files may contain bugs.\n\n--- BEGIN FILE ---\n```\n### sklearn/calibration.py\n...\n    \"\"\"\n    if hasattr(clf, \"decision_function\"):\n        method = getattr(clf, \"decision_function\")\n        return method, \"decision_function\"\n\n    if hasattr(clf, \"predict_proba\"):\n        method = getattr(clf, \"predict_proba\")\n        return method, \"predict_proba\"\n\n\ndef _compute_predictions(pred_method, method_name, X, n_classes):\n    \"\"\"Return predictions for `X` and reshape binary outputs to shape\n    (n_samples, 1).\n\n    Parameters\n    ----------\n    pred_method : callable\n        Prediction method.\n\n    method_name: str\n        Name of the prediction method\n\n    X : array-like or None\n        Data used to obtain predictions.\n\n    n_classes : int\n        Number of classes present.\n\n    Returns\n    -------\n    predictions : array-like, shape (X.shape[0], len(clf.classes_))\n        The predictions. Note if there are 2 classes, array is of shape\n        (X.shape[0], 1).\n    \"\"\"\n    predictions = pred_method(X=X)\n\n    if method_name == \"decision_function\":\n        if predictions.ndim == 1:\n            predictions = predictions[:, np.newaxis]\n    elif method_name == \"predict_proba\":\n        if n_classes == 2:\n            predictions = predictions[:, 1:]\n    else:  # pragma: no cover\n        # this branch should be unreachable.\n        raise ValueError(f\"Invalid prediction method: {method_name}\")\n    return predictions\n\n\ndef _fit_calibrator(clf, predictions, y, classes, method, sample_weight=None):\n    \"\"\"Fit calibrator(s) and return a `_CalibratedClassifier`\n    instance.\n\n    `n_classes` (i.e. `len(clf.classes_)`) calibrators are fitted.\n    However, if `n_classes` equals 2, one calibrator is fitted.\n\n    Parameters\n    ----------\n    clf : estimator instance\n        Fitted classifier.\n\n    predictions : array-like, shape (n_samples, n_classes) or (n_samples, 1) \\\n                    when binary.\n        Raw predictions returned by the un-calibrated base classifier.\n\n    y : array-like, shape (n_samples,)\n        The targets.\n\n    classes : ndarray, shape (n_classes,)\n        All the prediction classes.\n\n    method : {'sigmoid', 'isotonic'}\n        The method to use for calibration.\n\n    sample_weight : ndarray, shape (n_samples,), default=None\n        Sample weights. If None, then samples are equally weighted.\n\n    Returns\n    -------\n    pipeline : _CalibratedClassifier instance\n    \"\"\"\n    Y = label_binarize(y, classes=classes)\n    label_encoder = LabelEncoder().fit(classes)\n    pos_class_indices = label_encoder.transform(clf.classes_)\n    calibrators = []\n    for class_idx, this_pred in zip(pos_class_indices, predictions.T):\n        if method == \"isotonic\":\n            calibrator = IsotonicRegression(out_of_bounds=\"clip\")\n        else:  # \"sigmoid\"\n            calibrator = _SigmoidCalibration()\n        calibrator.fit(this_pred, Y[:, class_idx], sample_weight)\n        calibrators.append(calibrator)\n\n    pipeline = _CalibratedClassifier(clf, calibrators, method=method, classes=classes)\n    return pipeline\n\n\nclass _CalibratedClassifier:\n    \"\"\"Pipeline-like chaining a fitted classifier and its fitted calibrators.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Fitted classifier.\n\n    calibrators : list of fitted estimator instances\n        List of fitted calibrators (either 'IsotonicRegression' or\n        '_SigmoidCalibration'). The number of calibrators equals the number of\n        classes. However, if there are 2 classes, the list contains only one\n        fitted calibrator.\n\n    classes : array-like of shape (n_classes,)\n        All the prediction classes.\n\n    method : {'sigmoid', 'isotonic'}, default='sigmoid'\n        The method to use for calibration. Can be 'sigmoid' which\n        corresponds to Platt's method or 'isotonic' which is a\n        non-parametric approach based on isotonic regression.\n    \"\"\"\n\n    def __init__(self, estimator, calibrators, *, classes, method=\"sigmoid\"):\n        self.estimator = estimator\n        self.calibrators = calibrators\n        self.classes = classes\n        self.method = method\n\n    def predict_proba(self, X):\n        \"\"\"Calculate calibrated probabilities.\n\n        Calculates classification calibrated probabilities\n        for each class, in a one-vs-all manner, for `X`.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            The sample data.\n\n        Returns\n        -------\n        proba : array, shape (n_samples, n_classes)\n            The predicted probabilities. Can be exact zeros.\n        \"\"\"\n        n_classes = len(self.classes)\n        pred_method, method_name = _get_prediction_method(self.estimator)\n        predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n\n        label_encoder = LabelEncoder().fit(self.classes)\n        pos_class_indices = label_encoder.transform(self.estimator.classes_)\n\n        proba = np.zeros((_num_samples(X), n_classes))\n        for class_idx, this_pred, calibrator in zip(\n            pos_class_indices, predictions.T, self.calibrators\n        ):\n            if n_classes == 2:\n                # When binary, `predictions` consists only of predictions for\n                # clf.classes_[1] but `pos_class_indices` = 0\n                class_idx += 1\n            proba[:, class_idx] = calibrator.predict(this_pred)\n\n        # Normalize the probabilities\n        if n_classes == 2:\n            proba[:, 0] = 1.0 - proba[:, 1]\n        else:\n            denominator = np.sum(proba, axis=1)[:, np.newaxis]\n            # In the edge case where for each class calibrator returns a null\n            # probability for a given sample, use the uniform distribution\n            # instead.\n            uniform_proba = np.full_like(proba, 1 / n_classes)\n            proba = np.divide(\n                proba, denominator, out=uniform_proba, where=denominator != 0\n            )\n\n        # Deal with cases where the predicted probability minimally exceeds 1.0\n        proba[(1.0 < proba) & (proba <= 1.0 + 1e-5)] = 1.0\n\n        return proba\n\n\ndef _sigmoid_calibration(predictions, y, sample_weight=None):\n    \"\"\"Probability Calibration with sigmoid method (Platt 2000)\n\n    Parameters\n    ----------\n    predictions : ndarray of shape (n_samples,)\n        The decision function or predict proba for the samples.\n\n...\n\n\n### sklearn/isotonic.py\n...\n    if y_min is not None or y_max is not None:\n        # Older versions of np.clip don't accept None as a bound, so use np.inf\n        if y_min is None:\n            y_min = -np.inf\n        if y_max is None:\n            y_max = np.inf\n        np.clip(y, y_min, y_max, y)\n    return y[order]\n\n\nclass IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n    \"\"\"Isotonic regression model.\n\n    Read more in the :ref:`User Guide <isotonic>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    y_min : float, default=None\n        Lower bound on the lowest predicted value (the minimum value may\n        still be higher). If not set, defaults to -inf.\n\n    y_max : float, default=None\n        Upper bound on the highest predicted value (the maximum may still be\n        lower). If not set, defaults to +inf.\n\n    increasing : bool or 'auto', default=True\n        Determines whether the predictions should be constrained to increase\n        or decrease with `X`. 'auto' will decide based on the Spearman\n        correlation estimate's sign.\n\n    out_of_bounds : {'nan', 'clip', 'raise'}, default='nan'\n        Handles how `X` values outside of the training domain are handled\n        during prediction.\n\n        - 'nan', predictions will be NaN.\n        - 'clip', predictions will be set to the value corresponding to\n          the nearest train interval endpoint.\n        - 'raise', a `ValueError` is raised.\n\n    Attributes\n    ----------\n    X_min_ : float\n        Minimum value of input array `X_` for left bound.\n\n    X_max_ : float\n        Maximum value of input array `X_` for right bound.\n\n    X_thresholds_ : ndarray of shape (n_thresholds,)\n        Unique ascending `X` values used to interpolate\n        the y = f(X) monotonic function.\n\n        .. versionadded:: 0.24\n\n    y_thresholds_ : ndarray of shape (n_thresholds,)\n        De-duplicated `y` values suitable to interpolate the y = f(X)\n        monotonic function.\n\n        .. versionadded:: 0.24\n\n    f_ : function\n        The stepwise interpolating function that covers the input domain ``X``.\n\n    increasing_ : bool\n        Inferred value for ``increasing``.\n\n    See Also\n    --------\n    sklearn.linear_model.LinearRegression : Ordinary least squares Linear\n        Regression.\n    sklearn.ensemble.HistGradientBoostingRegressor : Gradient boosting that\n        is a non-parametric model accepting monotonicity constraints.\n    isotonic_regression : Function to solve the isotonic regression model.\n\n    Notes\n    -----\n    Ties are broken using the secondary method from de Leeuw, 1977.\n\n    References\n    ----------\n    Isotonic Median Regression: A Linear Programming Approach\n    Nilotpal Chakravarti\n    Mathematics of Operations Research\n    Vol. 14, No. 2 (May, 1989), pp. 303-308\n\n    Isotone Optimization in R : Pool-Adjacent-Violators\n    Algorithm (PAVA) and Active Set Methods\n    de Leeuw, Hornik, Mair\n    Journal of Statistical Software 2009\n\n    Correctness of Kruskal's algorithms for monotone regression with ties\n    de Leeuw, Psychometrica, 1977\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.isotonic import IsotonicRegression\n    >>> X, y = make_regression(n_samples=10, n_features=1, random_state=41)\n    >>> iso_reg = IsotonicRegression().fit(X, y)\n    >>> iso_reg.predict([.1, .2])\n    array([1.8628..., 3.7256...])\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"y_min\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"y_max\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"increasing\": [\"boolean\", StrOptions({\"auto\"})],\n        \"out_of_bounds\": [StrOptions({\"nan\", \"clip\", \"raise\"})],\n    }\n\n    def __init__(self, *, y_min=None, y_max=None, increasing=True, out_of_bounds=\"nan\"):\n        self.y_min = y_min\n        self.y_max = y_max\n        self.increasing = increasing\n        self.out_of_bounds = out_of_bounds\n\n    def _check_input_data_shape(self, X):\n        if not (X.ndim == 1 or (X.ndim == 2 and X.shape[1] == 1)):\n            msg = (\n                \"Isotonic regression input X should be a 1d array or \"\n                \"2d array with 1 feature\"\n            )\n            raise ValueError(msg)\n\n    def _build_f(self, X, y):\n        \"\"\"Build the f_ interp1d function.\"\"\"\n\n        bounds_error = self.out_of_bounds == \"raise\"\n        if len(y) == 1:\n            # single y, constant prediction\n            self.f_ = lambda x: y.repeat(x.shape)\n        else:\n            self.f_ = interpolate.interp1d(\n                X, y, kind=\"linear\", bounds_error=bounds_error\n            )\n\n    def _build_y(self, X, y, sample_weight, trim_duplicates=True):\n        \"\"\"Build the y_ IsotonicRegression.\"\"\"\n        self._check_input_data_shape(X)\n        X = X.reshape(-1)  # use 1d view\n\n        # Determine increasing if auto-determination requested\n        if self.increasing == \"auto\":\n            self.increasing_ = check_increasing(X, y)\n        else:\n            self.increasing_ = self.increasing\n\n        # If sample_weights is passed, removed zero-weight values and clean\n        # order\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        mask = sample_weight > 0\n        X, y, sample_weight = X[mask], y[mask], sample_weight[mask]\n\n        order = np.lexsort((y, X))\n        X, y, sample_weight = [array[order] for array in [X, y, sample_weight]]\n        unique_X, unique_y, unique_sample_weight = _make_unique(X, y, sample_weight)\n\n        X = unique_X\n        y = isotonic_regression(\n            unique_y,\n            sample_weight=unique_sample_weight,\n            y_min=self.y_min,\n            y_max=self.y_max,\n            increasing=self.increasing_,\n        )\n\n        # Handle the left and right bounds on X\n        self.X_min_, self.X_max_ = np.min(X), np.max(X)\n\n        if trim_duplicates:\n            # Remove unnecessary points for faster prediction\n            keep_data = np.ones((len(y),), dtype=bool)\n            # Aside from the 1st and last point, remove points whose y values\n            # are equal to both the point before and the point after it.\n            keep_data[1:-1] = np.logical_or(\n                np.not_equal(y[1:-1], y[:-2]), np.not_equal(y[1:-1], y[2:])\n            )\n            return X[keep_data], y[keep_data]\n        else:\n            # The ability to turn off trim_duplicates is only used to it make\n            # easier to unit test that removing duplicates in y does not have\n            # any impact the resulting interpolation function (besides\n            # prediction speed).\n            return X, y\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples,) or (n_samples, 1)\n            Training data.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        y : array-like of shape (n_samples,)\n            Training target.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Weights. If set to None, all weights will be set to 1 (equal\n            weights).\n\n        Returns\n        -------\n        self : object\n            Returns an instance of self.\n\n        Notes\n        -----\n        X is stored for future use, as :meth:`transform` needs X to interpolate\n        new input data.\n        \"\"\"\n        self._validate_params()\n        check_params = dict(accept_sparse=False, ensure_2d=False)\n        X = check_array(\n            X, input_name=\"X\", dtype=[np.float64, np.float32], **check_params\n        )\n        y = check_array(y, input_name=\"y\", dtype=X.dtype, **check_params)\n        check_consistent_length(X, y, sample_weight)\n\n        # Transform y by running the isotonic regression algorithm and\n        # transform X accordingly.\n        X, y = self._build_y(X, y, sample_weight)\n\n        # It is necessary to store the non-redundant part of the training set\n        # on the model to make it possible to support model persistence via\n        # the pickle module as the object built by scipy.interp1d is not\n        # picklable directly.\n        self.X_thresholds_, self.y_thresholds_ = X, y\n\n        # Build the interpolation function\n        self._build_f(X, y)\n        return self\n\n    def transform(self, T):\n        \"\"\"Transform new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            The transformed data.\n        \"\"\"\n\n        if hasattr(self, \"X_thresholds_\"):\n            dtype = self.X_thresholds_.dtype\n        else:\n            dtype = np.float64\n\n        T = check_array(T, dtype=dtype, ensure_2d=False)\n\n        self._check_input_data_shape(T)\n        T = T.reshape(-1)  # use 1d view\n\n        if self.out_of_bounds == \"clip\":\n            T = np.clip(T, self.X_min_, self.X_max_)\n\n        res = self.f_(T)\n\n        # on scipy 0.17, interp1d up-casts to float64, so we cast back\n        res = res.astype(T.dtype)\n\n        return res\n\n    def predict(self, T):\n        \"\"\"Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            Transformed data.\n        \"\"\"\n        return self.transform(T)\n\n    # We implement get_feature_names_out here instead of using\n    # `ClassNamePrefixFeaturesOutMixin`` because `input_features` are ignored.\n    # `input_features` are ignored because `IsotonicRegression` accepts 1d\n    # arrays and the semantics of `feature_names_in_` are not clear for 1d arrays.\n    def get_feature_names_out(self, input_features=None):\n        \"\"\"Get output feature names for transformation.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Ignored.\n\n        Returns\n        -------\n        feature_names_out : ndarray of str objects\n            An ndarray with one string i.e. [\"isotonicregression0\"].\n        \"\"\"\n        check_is_fitted(self, \"f_\")\n        class_name = self.__class__.__name__.lower()\n        return np.asarray([f\"{class_name}0\"], dtype=object)\n\n    def __getstate__(self):\n        \"\"\"Pickle-protocol - return state of the estimator.\"\"\"\n        state = super().__getstate__()\n        # remove interpolation method\n        state.pop(\"f_\", None)\n        return state\n\n    def __setstate__(self, state):\n        \"\"\"Pickle-protocol - set state of the estimator.\n\n        We need to rebuild the interpolation function.\n        \"\"\"\n        super().__setstate__(state)\n        if hasattr(self, \"X_thresholds_\") and hasattr(self, \"y_thresholds_\"):\n            self._build_f(self.X_thresholds_, self.y_thresholds_)\n\n    def _more_tags(self):\n        return {\"X_types\": [\"1darray\"]}\n```\n--- END FILE ---\n\nPlease first localize the bug based on the issue statement, and then generate *SEARCH/REPLACE* edits to fix the issue.\n\nEvery *SEARCH/REPLACE* edit must use this format:\n1. The file path\n2. The start of search block: <<<<<<< SEARCH\n3. A contiguous chunk of lines to search for in the existing source code\n4. The dividing line: =======\n5. The lines to replace into the source code\n6. The end of the replace block: >>>>>>> REPLACE\n\nHere is an example:\n\n```python\n### mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nPlease note that the *SEARCH/REPLACE* edit REQUIRES PROPER INDENTATION. If you would like to add the line '        print(x)', you must fully write that out, with all those spaces before the code!\nWrap the *SEARCH/REPLACE* edit in blocks ```python...```."}, {"response": "Based on the issue described, the problem arises because the `IsotonicRegression` class's `predict` method returns a DataFrame instead of a NumPy array when `set_config(transform_output=\"pandas\")` is set. The `_CalibratedClassifier` class expects a NumPy array when calling `calibrator.predict`.\n\nWe need to make sure that the `predict` method in `_CalibratedClassifier` handles this correctly. Therefore, we will convert the DataFrame returned by `calibrator.predict` back to a NumPy array before using it.\n\nHere is the `SEARCH/REPLACE` edit to fix the issue:\n\n```python\n### sklearn/calibration.py\n<<<<<<< SEARCH\n            proba[:, class_idx] = calibrator.predict(this_pred)\n=======\n            proba[:, class_idx] = np.asarray(calibrator.predict(this_pred)).reshape(-1)\n>>>>>>> REPLACE\n```\n\nThis change ensures that whatever the `calibrator.predict` method returns, it is converted to a NumPy array, which is then reshaped to a 1D array before being assigned to the `proba` array.\n\nThis edit addresses the issue described in the provided test case as well as in the issue statement.", "usage": {"completion_tokens": 0, "prompt_tokens": 0}, "prompt": "We are currently solving the following issue within our repository. Here is the issue text:\n--- BEGIN ISSUE ---\nCalibratedClassifierCV doesn't work with `set_config(transform_output=\"pandas\")`\n### Describe the bug\r\n\r\nCalibratedClassifierCV with isotonic regression doesn't work when we previously set `set_config(transform_output=\"pandas\")`.\r\nThe IsotonicRegression seems to return a dataframe, which is a problem for `_CalibratedClassifier`  in `predict_proba` where it tries to put the dataframe in a numpy array row `proba[:, class_idx] = calibrator.predict(this_pred)`.\r\n\r\n### Steps/Code to Reproduce\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn import set_config\r\nfrom sklearn.calibration import CalibratedClassifierCV\r\nfrom sklearn.linear_model import SGDClassifier\r\n\r\nset_config(transform_output=\"pandas\")\r\nmodel = CalibratedClassifierCV(SGDClassifier(), method='isotonic')\r\nmodel.fit(np.arange(90).reshape(30, -1), np.arange(30) % 2)\r\nmodel.predict(np.arange(90).reshape(30, -1))\r\n```\r\n\r\n### Expected Results\r\n\r\nIt should not crash.\r\n\r\n### Actual Results\r\n\r\n```\r\n../core/model_trainer.py:306: in train_model\r\n    cv_predictions = cross_val_predict(pipeline,\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:968: in cross_val_predict\r\n    predictions = parallel(\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:1085: in __call__\r\n    if self.dispatch_one_batch(iterator):\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:901: in dispatch_one_batch\r\n    self._dispatch(tasks)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:819: in _dispatch\r\n    job = self._backend.apply_async(batch, callback=cb)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/_parallel_backends.py:208: in apply_async\r\n    result = ImmediateResult(func)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/_parallel_backends.py:597: in __init__\r\n    self.results = batch()\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:288: in __call__\r\n    return [func(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:288: in <listcomp>\r\n    return [func(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/utils/fixes.py:117: in __call__\r\n    return self.function(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:1052: in _fit_and_predict\r\n    predictions = func(X_test)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/pipeline.py:548: in predict_proba\r\n    return self.steps[-1][1].predict_proba(Xt, **predict_proba_params)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/calibration.py:477: in predict_proba\r\n    proba = calibrated_classifier.predict_proba(X)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/calibration.py:764: in predict_proba\r\n    proba[:, class_idx] = calibrator.predict(this_pred)\r\nE   ValueError: could not broadcast input array from shape (20,1) into shape (20,)\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.9.15 (main, Nov 24 2022, 14:31:59)  [GCC 11.2.0]\r\nexecutable: /home/philippe/.anaconda3/envs/strategy-training/bin/python\r\n   machine: Linux-5.15.0-57-generic-x86_64-with-glibc2.31\r\n\r\nPython dependencies:\r\n      sklearn: 1.2.0\r\n          pip: 22.2.2\r\n   setuptools: 62.3.2\r\n        numpy: 1.23.5\r\n        scipy: 1.9.3\r\n       Cython: None\r\n       pandas: 1.4.1\r\n   matplotlib: 3.6.3\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libgomp\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0\r\n        version: None\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/numpy.libs/libopenblas64_p-r0-742d56dc.3.20.so\r\n        version: 0.3.20\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/scipy.libs/libopenblasp-r0-41284840.3.18.so\r\n        version: 0.3.18\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n```\r\n\n\n--- END ISSUE ---\n\n\nWe also have a fail to pass unit test for the given issue. We can use this to help identify and fix the issue. Here is the patch text:\n--- BEGIN PATCH ---\ndiff --git a/sklearn/tests/test_isotonic.py b/sklearn/tests/test_isotonic.py\n--- a/sklearn/tests/test_isotonic.py\n+++ b/sklearn/tests/test_isotonic.py\n@@ -5,6 +5,7 @@\n \n import pytest\n \n+import sklearn\n from sklearn.datasets import make_regression\n from sklearn.isotonic import (\n     check_increasing,\n@@ -680,3 +681,24 @@ def test_get_feature_names_out(shape):\n     assert isinstance(names, np.ndarray)\n     assert names.dtype == object\n     assert_array_equal([\"isotonicregression0\"], names)\n+\n+\n+def test_isotonic_regression_output_predict():\n+    \"\"\"Check that `predict` does return the expected output type.\n+\n+    We need to check that `transform` will output a DataFrame and a NumPy array\n+    when we set `transform_output` to `pandas`.\n+\n+    Non-regression test for:\n+    https://github.com/scikit-learn/scikit-learn/issues/25499\n+    \"\"\"\n+    pd = pytest.importorskip(\"pandas\")\n+    X, y = make_regression(n_samples=10, n_features=1, random_state=42)\n+    regressor = IsotonicRegression()\n+    with sklearn.config_context(transform_output=\"pandas\"):\n+        regressor.fit(X, y)\n+        X_trans = regressor.transform(X)\n+        y_pred = regressor.predict(X)\n+\n+    assert isinstance(X_trans, pd.DataFrame)\n+    assert isinstance(y_pred, np.ndarray)\n\n--- END PATCH ---\n\n\n\nBelow are some code segments, each from a relevant file. One or more of these files may contain bugs.\n\n--- BEGIN FILE ---\n```\n### sklearn/calibration.py\n...\n    \"\"\"\n    if hasattr(clf, \"decision_function\"):\n        method = getattr(clf, \"decision_function\")\n        return method, \"decision_function\"\n\n    if hasattr(clf, \"predict_proba\"):\n        method = getattr(clf, \"predict_proba\")\n        return method, \"predict_proba\"\n\n\ndef _compute_predictions(pred_method, method_name, X, n_classes):\n    \"\"\"Return predictions for `X` and reshape binary outputs to shape\n    (n_samples, 1).\n\n    Parameters\n    ----------\n    pred_method : callable\n        Prediction method.\n\n    method_name: str\n        Name of the prediction method\n\n    X : array-like or None\n        Data used to obtain predictions.\n\n    n_classes : int\n        Number of classes present.\n\n    Returns\n    -------\n    predictions : array-like, shape (X.shape[0], len(clf.classes_))\n        The predictions. Note if there are 2 classes, array is of shape\n        (X.shape[0], 1).\n    \"\"\"\n    predictions = pred_method(X=X)\n\n    if method_name == \"decision_function\":\n        if predictions.ndim == 1:\n            predictions = predictions[:, np.newaxis]\n    elif method_name == \"predict_proba\":\n        if n_classes == 2:\n            predictions = predictions[:, 1:]\n    else:  # pragma: no cover\n        # this branch should be unreachable.\n        raise ValueError(f\"Invalid prediction method: {method_name}\")\n    return predictions\n\n\ndef _fit_calibrator(clf, predictions, y, classes, method, sample_weight=None):\n    \"\"\"Fit calibrator(s) and return a `_CalibratedClassifier`\n    instance.\n\n    `n_classes` (i.e. `len(clf.classes_)`) calibrators are fitted.\n    However, if `n_classes` equals 2, one calibrator is fitted.\n\n    Parameters\n    ----------\n    clf : estimator instance\n        Fitted classifier.\n\n    predictions : array-like, shape (n_samples, n_classes) or (n_samples, 1) \\\n                    when binary.\n        Raw predictions returned by the un-calibrated base classifier.\n\n    y : array-like, shape (n_samples,)\n        The targets.\n\n    classes : ndarray, shape (n_classes,)\n        All the prediction classes.\n\n    method : {'sigmoid', 'isotonic'}\n        The method to use for calibration.\n\n    sample_weight : ndarray, shape (n_samples,), default=None\n        Sample weights. If None, then samples are equally weighted.\n\n    Returns\n    -------\n    pipeline : _CalibratedClassifier instance\n    \"\"\"\n    Y = label_binarize(y, classes=classes)\n    label_encoder = LabelEncoder().fit(classes)\n    pos_class_indices = label_encoder.transform(clf.classes_)\n    calibrators = []\n    for class_idx, this_pred in zip(pos_class_indices, predictions.T):\n        if method == \"isotonic\":\n            calibrator = IsotonicRegression(out_of_bounds=\"clip\")\n        else:  # \"sigmoid\"\n            calibrator = _SigmoidCalibration()\n        calibrator.fit(this_pred, Y[:, class_idx], sample_weight)\n        calibrators.append(calibrator)\n\n    pipeline = _CalibratedClassifier(clf, calibrators, method=method, classes=classes)\n    return pipeline\n\n\nclass _CalibratedClassifier:\n    \"\"\"Pipeline-like chaining a fitted classifier and its fitted calibrators.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Fitted classifier.\n\n    calibrators : list of fitted estimator instances\n        List of fitted calibrators (either 'IsotonicRegression' or\n        '_SigmoidCalibration'). The number of calibrators equals the number of\n        classes. However, if there are 2 classes, the list contains only one\n        fitted calibrator.\n\n    classes : array-like of shape (n_classes,)\n        All the prediction classes.\n\n    method : {'sigmoid', 'isotonic'}, default='sigmoid'\n        The method to use for calibration. Can be 'sigmoid' which\n        corresponds to Platt's method or 'isotonic' which is a\n        non-parametric approach based on isotonic regression.\n    \"\"\"\n\n    def __init__(self, estimator, calibrators, *, classes, method=\"sigmoid\"):\n        self.estimator = estimator\n        self.calibrators = calibrators\n        self.classes = classes\n        self.method = method\n\n    def predict_proba(self, X):\n        \"\"\"Calculate calibrated probabilities.\n\n        Calculates classification calibrated probabilities\n        for each class, in a one-vs-all manner, for `X`.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            The sample data.\n\n        Returns\n        -------\n        proba : array, shape (n_samples, n_classes)\n            The predicted probabilities. Can be exact zeros.\n        \"\"\"\n        n_classes = len(self.classes)\n        pred_method, method_name = _get_prediction_method(self.estimator)\n        predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n\n        label_encoder = LabelEncoder().fit(self.classes)\n        pos_class_indices = label_encoder.transform(self.estimator.classes_)\n\n        proba = np.zeros((_num_samples(X), n_classes))\n        for class_idx, this_pred, calibrator in zip(\n            pos_class_indices, predictions.T, self.calibrators\n        ):\n            if n_classes == 2:\n                # When binary, `predictions` consists only of predictions for\n                # clf.classes_[1] but `pos_class_indices` = 0\n                class_idx += 1\n            proba[:, class_idx] = calibrator.predict(this_pred)\n\n        # Normalize the probabilities\n        if n_classes == 2:\n            proba[:, 0] = 1.0 - proba[:, 1]\n        else:\n            denominator = np.sum(proba, axis=1)[:, np.newaxis]\n            # In the edge case where for each class calibrator returns a null\n            # probability for a given sample, use the uniform distribution\n            # instead.\n            uniform_proba = np.full_like(proba, 1 / n_classes)\n            proba = np.divide(\n                proba, denominator, out=uniform_proba, where=denominator != 0\n            )\n\n        # Deal with cases where the predicted probability minimally exceeds 1.0\n        proba[(1.0 < proba) & (proba <= 1.0 + 1e-5)] = 1.0\n\n        return proba\n\n\ndef _sigmoid_calibration(predictions, y, sample_weight=None):\n    \"\"\"Probability Calibration with sigmoid method (Platt 2000)\n\n    Parameters\n    ----------\n    predictions : ndarray of shape (n_samples,)\n        The decision function or predict proba for the samples.\n\n...\n\n\n### sklearn/isotonic.py\n...\n    if y_min is not None or y_max is not None:\n        # Older versions of np.clip don't accept None as a bound, so use np.inf\n        if y_min is None:\n            y_min = -np.inf\n        if y_max is None:\n            y_max = np.inf\n        np.clip(y, y_min, y_max, y)\n    return y[order]\n\n\nclass IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n    \"\"\"Isotonic regression model.\n\n    Read more in the :ref:`User Guide <isotonic>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    y_min : float, default=None\n        Lower bound on the lowest predicted value (the minimum value may\n        still be higher). If not set, defaults to -inf.\n\n    y_max : float, default=None\n        Upper bound on the highest predicted value (the maximum may still be\n        lower). If not set, defaults to +inf.\n\n    increasing : bool or 'auto', default=True\n        Determines whether the predictions should be constrained to increase\n        or decrease with `X`. 'auto' will decide based on the Spearman\n        correlation estimate's sign.\n\n    out_of_bounds : {'nan', 'clip', 'raise'}, default='nan'\n        Handles how `X` values outside of the training domain are handled\n        during prediction.\n\n        - 'nan', predictions will be NaN.\n        - 'clip', predictions will be set to the value corresponding to\n          the nearest train interval endpoint.\n        - 'raise', a `ValueError` is raised.\n\n    Attributes\n    ----------\n    X_min_ : float\n        Minimum value of input array `X_` for left bound.\n\n    X_max_ : float\n        Maximum value of input array `X_` for right bound.\n\n    X_thresholds_ : ndarray of shape (n_thresholds,)\n        Unique ascending `X` values used to interpolate\n        the y = f(X) monotonic function.\n\n        .. versionadded:: 0.24\n\n    y_thresholds_ : ndarray of shape (n_thresholds,)\n        De-duplicated `y` values suitable to interpolate the y = f(X)\n        monotonic function.\n\n        .. versionadded:: 0.24\n\n    f_ : function\n        The stepwise interpolating function that covers the input domain ``X``.\n\n    increasing_ : bool\n        Inferred value for ``increasing``.\n\n    See Also\n    --------\n    sklearn.linear_model.LinearRegression : Ordinary least squares Linear\n        Regression.\n    sklearn.ensemble.HistGradientBoostingRegressor : Gradient boosting that\n        is a non-parametric model accepting monotonicity constraints.\n    isotonic_regression : Function to solve the isotonic regression model.\n\n    Notes\n    -----\n    Ties are broken using the secondary method from de Leeuw, 1977.\n\n    References\n    ----------\n    Isotonic Median Regression: A Linear Programming Approach\n    Nilotpal Chakravarti\n    Mathematics of Operations Research\n    Vol. 14, No. 2 (May, 1989), pp. 303-308\n\n    Isotone Optimization in R : Pool-Adjacent-Violators\n    Algorithm (PAVA) and Active Set Methods\n    de Leeuw, Hornik, Mair\n    Journal of Statistical Software 2009\n\n    Correctness of Kruskal's algorithms for monotone regression with ties\n    de Leeuw, Psychometrica, 1977\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.isotonic import IsotonicRegression\n    >>> X, y = make_regression(n_samples=10, n_features=1, random_state=41)\n    >>> iso_reg = IsotonicRegression().fit(X, y)\n    >>> iso_reg.predict([.1, .2])\n    array([1.8628..., 3.7256...])\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"y_min\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"y_max\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"increasing\": [\"boolean\", StrOptions({\"auto\"})],\n        \"out_of_bounds\": [StrOptions({\"nan\", \"clip\", \"raise\"})],\n    }\n\n    def __init__(self, *, y_min=None, y_max=None, increasing=True, out_of_bounds=\"nan\"):\n        self.y_min = y_min\n        self.y_max = y_max\n        self.increasing = increasing\n        self.out_of_bounds = out_of_bounds\n\n    def _check_input_data_shape(self, X):\n        if not (X.ndim == 1 or (X.ndim == 2 and X.shape[1] == 1)):\n            msg = (\n                \"Isotonic regression input X should be a 1d array or \"\n                \"2d array with 1 feature\"\n            )\n            raise ValueError(msg)\n\n    def _build_f(self, X, y):\n        \"\"\"Build the f_ interp1d function.\"\"\"\n\n        bounds_error = self.out_of_bounds == \"raise\"\n        if len(y) == 1:\n            # single y, constant prediction\n            self.f_ = lambda x: y.repeat(x.shape)\n        else:\n            self.f_ = interpolate.interp1d(\n                X, y, kind=\"linear\", bounds_error=bounds_error\n            )\n\n    def _build_y(self, X, y, sample_weight, trim_duplicates=True):\n        \"\"\"Build the y_ IsotonicRegression.\"\"\"\n        self._check_input_data_shape(X)\n        X = X.reshape(-1)  # use 1d view\n\n        # Determine increasing if auto-determination requested\n        if self.increasing == \"auto\":\n            self.increasing_ = check_increasing(X, y)\n        else:\n            self.increasing_ = self.increasing\n\n        # If sample_weights is passed, removed zero-weight values and clean\n        # order\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        mask = sample_weight > 0\n        X, y, sample_weight = X[mask], y[mask], sample_weight[mask]\n\n        order = np.lexsort((y, X))\n        X, y, sample_weight = [array[order] for array in [X, y, sample_weight]]\n        unique_X, unique_y, unique_sample_weight = _make_unique(X, y, sample_weight)\n\n        X = unique_X\n        y = isotonic_regression(\n            unique_y,\n            sample_weight=unique_sample_weight,\n            y_min=self.y_min,\n            y_max=self.y_max,\n            increasing=self.increasing_,\n        )\n\n        # Handle the left and right bounds on X\n        self.X_min_, self.X_max_ = np.min(X), np.max(X)\n\n        if trim_duplicates:\n            # Remove unnecessary points for faster prediction\n            keep_data = np.ones((len(y),), dtype=bool)\n            # Aside from the 1st and last point, remove points whose y values\n            # are equal to both the point before and the point after it.\n            keep_data[1:-1] = np.logical_or(\n                np.not_equal(y[1:-1], y[:-2]), np.not_equal(y[1:-1], y[2:])\n            )\n            return X[keep_data], y[keep_data]\n        else:\n            # The ability to turn off trim_duplicates is only used to it make\n            # easier to unit test that removing duplicates in y does not have\n            # any impact the resulting interpolation function (besides\n            # prediction speed).\n            return X, y\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples,) or (n_samples, 1)\n            Training data.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        y : array-like of shape (n_samples,)\n            Training target.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Weights. If set to None, all weights will be set to 1 (equal\n            weights).\n\n        Returns\n        -------\n        self : object\n            Returns an instance of self.\n\n        Notes\n        -----\n        X is stored for future use, as :meth:`transform` needs X to interpolate\n        new input data.\n        \"\"\"\n        self._validate_params()\n        check_params = dict(accept_sparse=False, ensure_2d=False)\n        X = check_array(\n            X, input_name=\"X\", dtype=[np.float64, np.float32], **check_params\n        )\n        y = check_array(y, input_name=\"y\", dtype=X.dtype, **check_params)\n        check_consistent_length(X, y, sample_weight)\n\n        # Transform y by running the isotonic regression algorithm and\n        # transform X accordingly.\n        X, y = self._build_y(X, y, sample_weight)\n\n        # It is necessary to store the non-redundant part of the training set\n        # on the model to make it possible to support model persistence via\n        # the pickle module as the object built by scipy.interp1d is not\n        # picklable directly.\n        self.X_thresholds_, self.y_thresholds_ = X, y\n\n        # Build the interpolation function\n        self._build_f(X, y)\n        return self\n\n    def transform(self, T):\n        \"\"\"Transform new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            The transformed data.\n        \"\"\"\n\n        if hasattr(self, \"X_thresholds_\"):\n            dtype = self.X_thresholds_.dtype\n        else:\n            dtype = np.float64\n\n        T = check_array(T, dtype=dtype, ensure_2d=False)\n\n        self._check_input_data_shape(T)\n        T = T.reshape(-1)  # use 1d view\n\n        if self.out_of_bounds == \"clip\":\n            T = np.clip(T, self.X_min_, self.X_max_)\n\n        res = self.f_(T)\n\n        # on scipy 0.17, interp1d up-casts to float64, so we cast back\n        res = res.astype(T.dtype)\n\n        return res\n\n    def predict(self, T):\n        \"\"\"Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            Transformed data.\n        \"\"\"\n        return self.transform(T)\n\n    # We implement get_feature_names_out here instead of using\n    # `ClassNamePrefixFeaturesOutMixin`` because `input_features` are ignored.\n    # `input_features` are ignored because `IsotonicRegression` accepts 1d\n    # arrays and the semantics of `feature_names_in_` are not clear for 1d arrays.\n    def get_feature_names_out(self, input_features=None):\n        \"\"\"Get output feature names for transformation.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Ignored.\n\n        Returns\n        -------\n        feature_names_out : ndarray of str objects\n            An ndarray with one string i.e. [\"isotonicregression0\"].\n        \"\"\"\n        check_is_fitted(self, \"f_\")\n        class_name = self.__class__.__name__.lower()\n        return np.asarray([f\"{class_name}0\"], dtype=object)\n\n    def __getstate__(self):\n        \"\"\"Pickle-protocol - return state of the estimator.\"\"\"\n        state = super().__getstate__()\n        # remove interpolation method\n        state.pop(\"f_\", None)\n        return state\n\n    def __setstate__(self, state):\n        \"\"\"Pickle-protocol - set state of the estimator.\n\n        We need to rebuild the interpolation function.\n        \"\"\"\n        super().__setstate__(state)\n        if hasattr(self, \"X_thresholds_\") and hasattr(self, \"y_thresholds_\"):\n            self._build_f(self.X_thresholds_, self.y_thresholds_)\n\n    def _more_tags(self):\n        return {\"X_types\": [\"1darray\"]}\n```\n--- END FILE ---\n\nPlease first localize the bug based on the issue statement, and then generate *SEARCH/REPLACE* edits to fix the issue.\n\nEvery *SEARCH/REPLACE* edit must use this format:\n1. The file path\n2. The start of search block: <<<<<<< SEARCH\n3. A contiguous chunk of lines to search for in the existing source code\n4. The dividing line: =======\n5. The lines to replace into the source code\n6. The end of the replace block: >>>>>>> REPLACE\n\nHere is an example:\n\n```python\n### mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nPlease note that the *SEARCH/REPLACE* edit REQUIRES PROPER INDENTATION. If you would like to add the line '        print(x)', you must fully write that out, with all those spaces before the code!\nWrap the *SEARCH/REPLACE* edit in blocks ```python...```."}, {"response": "To address the issue, we need to ensure that `IsotonicRegression.predict` method in `sklearn/isotonic.py` correctly handles the configuration where the `transform_output` is set to `\"pandas\"`. Specifically, we need to make sure that it converts the result to a NumPy array if it returns a DataFrame.\n\nHere are the SEARCH/REPLACE edits to fix the issue:\n\n```python\n### sklearn/isotonic.py\n<<<<<<< SEARCH\n        return res.astype(T.dtype)\n=======\n        # Ensure the output is a numpy array if transform_output is set to pandas\n        res = res.astype(T.dtype)\n        if hasattr(sklearn, 'get_config') and sklearn.get_config()['transform_output'] == 'pandas':\n            res = res.to_numpy()\n        return res\n>>>>>>> REPLACE\n```\n\n```python\n### sklearn/calibration.py\n<<<<<<< SEARCH\n            proba[:, class_idx] = calibrator.predict(this_pred)\n=======\n            pred = calibrator.predict(this_pred)\n            if isinstance(pred, pd.DataFrame):\n                pred = pred.to_numpy().flatten()\n            proba[:, class_idx] = pred\n>>>>>>> REPLACE\n```", "usage": {"completion_tokens": 0, "prompt_tokens": 0}, "prompt": "We are currently solving the following issue within our repository. Here is the issue text:\n--- BEGIN ISSUE ---\nCalibratedClassifierCV doesn't work with `set_config(transform_output=\"pandas\")`\n### Describe the bug\r\n\r\nCalibratedClassifierCV with isotonic regression doesn't work when we previously set `set_config(transform_output=\"pandas\")`.\r\nThe IsotonicRegression seems to return a dataframe, which is a problem for `_CalibratedClassifier`  in `predict_proba` where it tries to put the dataframe in a numpy array row `proba[:, class_idx] = calibrator.predict(this_pred)`.\r\n\r\n### Steps/Code to Reproduce\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn import set_config\r\nfrom sklearn.calibration import CalibratedClassifierCV\r\nfrom sklearn.linear_model import SGDClassifier\r\n\r\nset_config(transform_output=\"pandas\")\r\nmodel = CalibratedClassifierCV(SGDClassifier(), method='isotonic')\r\nmodel.fit(np.arange(90).reshape(30, -1), np.arange(30) % 2)\r\nmodel.predict(np.arange(90).reshape(30, -1))\r\n```\r\n\r\n### Expected Results\r\n\r\nIt should not crash.\r\n\r\n### Actual Results\r\n\r\n```\r\n../core/model_trainer.py:306: in train_model\r\n    cv_predictions = cross_val_predict(pipeline,\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:968: in cross_val_predict\r\n    predictions = parallel(\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:1085: in __call__\r\n    if self.dispatch_one_batch(iterator):\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:901: in dispatch_one_batch\r\n    self._dispatch(tasks)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:819: in _dispatch\r\n    job = self._backend.apply_async(batch, callback=cb)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/_parallel_backends.py:208: in apply_async\r\n    result = ImmediateResult(func)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/_parallel_backends.py:597: in __init__\r\n    self.results = batch()\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:288: in __call__\r\n    return [func(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:288: in <listcomp>\r\n    return [func(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/utils/fixes.py:117: in __call__\r\n    return self.function(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:1052: in _fit_and_predict\r\n    predictions = func(X_test)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/pipeline.py:548: in predict_proba\r\n    return self.steps[-1][1].predict_proba(Xt, **predict_proba_params)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/calibration.py:477: in predict_proba\r\n    proba = calibrated_classifier.predict_proba(X)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/calibration.py:764: in predict_proba\r\n    proba[:, class_idx] = calibrator.predict(this_pred)\r\nE   ValueError: could not broadcast input array from shape (20,1) into shape (20,)\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.9.15 (main, Nov 24 2022, 14:31:59)  [GCC 11.2.0]\r\nexecutable: /home/philippe/.anaconda3/envs/strategy-training/bin/python\r\n   machine: Linux-5.15.0-57-generic-x86_64-with-glibc2.31\r\n\r\nPython dependencies:\r\n      sklearn: 1.2.0\r\n          pip: 22.2.2\r\n   setuptools: 62.3.2\r\n        numpy: 1.23.5\r\n        scipy: 1.9.3\r\n       Cython: None\r\n       pandas: 1.4.1\r\n   matplotlib: 3.6.3\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libgomp\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0\r\n        version: None\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/numpy.libs/libopenblas64_p-r0-742d56dc.3.20.so\r\n        version: 0.3.20\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/scipy.libs/libopenblasp-r0-41284840.3.18.so\r\n        version: 0.3.18\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n```\r\n\n\n--- END ISSUE ---\n\n\nWe also have a fail to pass unit test for the given issue. We can use this to help identify and fix the issue. Here is the patch text:\n--- BEGIN PATCH ---\ndiff --git a/sklearn/tests/test_isotonic.py b/sklearn/tests/test_isotonic.py\n--- a/sklearn/tests/test_isotonic.py\n+++ b/sklearn/tests/test_isotonic.py\n@@ -5,6 +5,7 @@\n \n import pytest\n \n+import sklearn\n from sklearn.datasets import make_regression\n from sklearn.isotonic import (\n     check_increasing,\n@@ -680,3 +681,24 @@ def test_get_feature_names_out(shape):\n     assert isinstance(names, np.ndarray)\n     assert names.dtype == object\n     assert_array_equal([\"isotonicregression0\"], names)\n+\n+\n+def test_isotonic_regression_output_predict():\n+    \"\"\"Check that `predict` does return the expected output type.\n+\n+    We need to check that `transform` will output a DataFrame and a NumPy array\n+    when we set `transform_output` to `pandas`.\n+\n+    Non-regression test for:\n+    https://github.com/scikit-learn/scikit-learn/issues/25499\n+    \"\"\"\n+    pd = pytest.importorskip(\"pandas\")\n+    X, y = make_regression(n_samples=10, n_features=1, random_state=42)\n+    regressor = IsotonicRegression()\n+    with sklearn.config_context(transform_output=\"pandas\"):\n+        regressor.fit(X, y)\n+        X_trans = regressor.transform(X)\n+        y_pred = regressor.predict(X)\n+\n+    assert isinstance(X_trans, pd.DataFrame)\n+    assert isinstance(y_pred, np.ndarray)\n\n--- END PATCH ---\n\n\n\nBelow are some code segments, each from a relevant file. One or more of these files may contain bugs.\n\n--- BEGIN FILE ---\n```\n### sklearn/calibration.py\n...\n    \"\"\"\n    if hasattr(clf, \"decision_function\"):\n        method = getattr(clf, \"decision_function\")\n        return method, \"decision_function\"\n\n    if hasattr(clf, \"predict_proba\"):\n        method = getattr(clf, \"predict_proba\")\n        return method, \"predict_proba\"\n\n\ndef _compute_predictions(pred_method, method_name, X, n_classes):\n    \"\"\"Return predictions for `X` and reshape binary outputs to shape\n    (n_samples, 1).\n\n    Parameters\n    ----------\n    pred_method : callable\n        Prediction method.\n\n    method_name: str\n        Name of the prediction method\n\n    X : array-like or None\n        Data used to obtain predictions.\n\n    n_classes : int\n        Number of classes present.\n\n    Returns\n    -------\n    predictions : array-like, shape (X.shape[0], len(clf.classes_))\n        The predictions. Note if there are 2 classes, array is of shape\n        (X.shape[0], 1).\n    \"\"\"\n    predictions = pred_method(X=X)\n\n    if method_name == \"decision_function\":\n        if predictions.ndim == 1:\n            predictions = predictions[:, np.newaxis]\n    elif method_name == \"predict_proba\":\n        if n_classes == 2:\n            predictions = predictions[:, 1:]\n    else:  # pragma: no cover\n        # this branch should be unreachable.\n        raise ValueError(f\"Invalid prediction method: {method_name}\")\n    return predictions\n\n\ndef _fit_calibrator(clf, predictions, y, classes, method, sample_weight=None):\n    \"\"\"Fit calibrator(s) and return a `_CalibratedClassifier`\n    instance.\n\n    `n_classes` (i.e. `len(clf.classes_)`) calibrators are fitted.\n    However, if `n_classes` equals 2, one calibrator is fitted.\n\n    Parameters\n    ----------\n    clf : estimator instance\n        Fitted classifier.\n\n    predictions : array-like, shape (n_samples, n_classes) or (n_samples, 1) \\\n                    when binary.\n        Raw predictions returned by the un-calibrated base classifier.\n\n    y : array-like, shape (n_samples,)\n        The targets.\n\n    classes : ndarray, shape (n_classes,)\n        All the prediction classes.\n\n    method : {'sigmoid', 'isotonic'}\n        The method to use for calibration.\n\n    sample_weight : ndarray, shape (n_samples,), default=None\n        Sample weights. If None, then samples are equally weighted.\n\n    Returns\n    -------\n    pipeline : _CalibratedClassifier instance\n    \"\"\"\n    Y = label_binarize(y, classes=classes)\n    label_encoder = LabelEncoder().fit(classes)\n    pos_class_indices = label_encoder.transform(clf.classes_)\n    calibrators = []\n    for class_idx, this_pred in zip(pos_class_indices, predictions.T):\n        if method == \"isotonic\":\n            calibrator = IsotonicRegression(out_of_bounds=\"clip\")\n        else:  # \"sigmoid\"\n            calibrator = _SigmoidCalibration()\n        calibrator.fit(this_pred, Y[:, class_idx], sample_weight)\n        calibrators.append(calibrator)\n\n    pipeline = _CalibratedClassifier(clf, calibrators, method=method, classes=classes)\n    return pipeline\n\n\nclass _CalibratedClassifier:\n    \"\"\"Pipeline-like chaining a fitted classifier and its fitted calibrators.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Fitted classifier.\n\n    calibrators : list of fitted estimator instances\n        List of fitted calibrators (either 'IsotonicRegression' or\n        '_SigmoidCalibration'). The number of calibrators equals the number of\n        classes. However, if there are 2 classes, the list contains only one\n        fitted calibrator.\n\n    classes : array-like of shape (n_classes,)\n        All the prediction classes.\n\n    method : {'sigmoid', 'isotonic'}, default='sigmoid'\n        The method to use for calibration. Can be 'sigmoid' which\n        corresponds to Platt's method or 'isotonic' which is a\n        non-parametric approach based on isotonic regression.\n    \"\"\"\n\n    def __init__(self, estimator, calibrators, *, classes, method=\"sigmoid\"):\n        self.estimator = estimator\n        self.calibrators = calibrators\n        self.classes = classes\n        self.method = method\n\n    def predict_proba(self, X):\n        \"\"\"Calculate calibrated probabilities.\n\n        Calculates classification calibrated probabilities\n        for each class, in a one-vs-all manner, for `X`.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            The sample data.\n\n        Returns\n        -------\n        proba : array, shape (n_samples, n_classes)\n            The predicted probabilities. Can be exact zeros.\n        \"\"\"\n        n_classes = len(self.classes)\n        pred_method, method_name = _get_prediction_method(self.estimator)\n        predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n\n        label_encoder = LabelEncoder().fit(self.classes)\n        pos_class_indices = label_encoder.transform(self.estimator.classes_)\n\n        proba = np.zeros((_num_samples(X), n_classes))\n        for class_idx, this_pred, calibrator in zip(\n            pos_class_indices, predictions.T, self.calibrators\n        ):\n            if n_classes == 2:\n                # When binary, `predictions` consists only of predictions for\n                # clf.classes_[1] but `pos_class_indices` = 0\n                class_idx += 1\n            proba[:, class_idx] = calibrator.predict(this_pred)\n\n        # Normalize the probabilities\n        if n_classes == 2:\n            proba[:, 0] = 1.0 - proba[:, 1]\n        else:\n            denominator = np.sum(proba, axis=1)[:, np.newaxis]\n            # In the edge case where for each class calibrator returns a null\n            # probability for a given sample, use the uniform distribution\n            # instead.\n            uniform_proba = np.full_like(proba, 1 / n_classes)\n            proba = np.divide(\n                proba, denominator, out=uniform_proba, where=denominator != 0\n            )\n\n        # Deal with cases where the predicted probability minimally exceeds 1.0\n        proba[(1.0 < proba) & (proba <= 1.0 + 1e-5)] = 1.0\n\n        return proba\n\n\ndef _sigmoid_calibration(predictions, y, sample_weight=None):\n    \"\"\"Probability Calibration with sigmoid method (Platt 2000)\n\n    Parameters\n    ----------\n    predictions : ndarray of shape (n_samples,)\n        The decision function or predict proba for the samples.\n\n...\n\n\n### sklearn/isotonic.py\n...\n    if y_min is not None or y_max is not None:\n        # Older versions of np.clip don't accept None as a bound, so use np.inf\n        if y_min is None:\n            y_min = -np.inf\n        if y_max is None:\n            y_max = np.inf\n        np.clip(y, y_min, y_max, y)\n    return y[order]\n\n\nclass IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n    \"\"\"Isotonic regression model.\n\n    Read more in the :ref:`User Guide <isotonic>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    y_min : float, default=None\n        Lower bound on the lowest predicted value (the minimum value may\n        still be higher). If not set, defaults to -inf.\n\n    y_max : float, default=None\n        Upper bound on the highest predicted value (the maximum may still be\n        lower). If not set, defaults to +inf.\n\n    increasing : bool or 'auto', default=True\n        Determines whether the predictions should be constrained to increase\n        or decrease with `X`. 'auto' will decide based on the Spearman\n        correlation estimate's sign.\n\n    out_of_bounds : {'nan', 'clip', 'raise'}, default='nan'\n        Handles how `X` values outside of the training domain are handled\n        during prediction.\n\n        - 'nan', predictions will be NaN.\n        - 'clip', predictions will be set to the value corresponding to\n          the nearest train interval endpoint.\n        - 'raise', a `ValueError` is raised.\n\n    Attributes\n    ----------\n    X_min_ : float\n        Minimum value of input array `X_` for left bound.\n\n    X_max_ : float\n        Maximum value of input array `X_` for right bound.\n\n    X_thresholds_ : ndarray of shape (n_thresholds,)\n        Unique ascending `X` values used to interpolate\n        the y = f(X) monotonic function.\n\n        .. versionadded:: 0.24\n\n    y_thresholds_ : ndarray of shape (n_thresholds,)\n        De-duplicated `y` values suitable to interpolate the y = f(X)\n        monotonic function.\n\n        .. versionadded:: 0.24\n\n    f_ : function\n        The stepwise interpolating function that covers the input domain ``X``.\n\n    increasing_ : bool\n        Inferred value for ``increasing``.\n\n    See Also\n    --------\n    sklearn.linear_model.LinearRegression : Ordinary least squares Linear\n        Regression.\n    sklearn.ensemble.HistGradientBoostingRegressor : Gradient boosting that\n        is a non-parametric model accepting monotonicity constraints.\n    isotonic_regression : Function to solve the isotonic regression model.\n\n    Notes\n    -----\n    Ties are broken using the secondary method from de Leeuw, 1977.\n\n    References\n    ----------\n    Isotonic Median Regression: A Linear Programming Approach\n    Nilotpal Chakravarti\n    Mathematics of Operations Research\n    Vol. 14, No. 2 (May, 1989), pp. 303-308\n\n    Isotone Optimization in R : Pool-Adjacent-Violators\n    Algorithm (PAVA) and Active Set Methods\n    de Leeuw, Hornik, Mair\n    Journal of Statistical Software 2009\n\n    Correctness of Kruskal's algorithms for monotone regression with ties\n    de Leeuw, Psychometrica, 1977\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.isotonic import IsotonicRegression\n    >>> X, y = make_regression(n_samples=10, n_features=1, random_state=41)\n    >>> iso_reg = IsotonicRegression().fit(X, y)\n    >>> iso_reg.predict([.1, .2])\n    array([1.8628..., 3.7256...])\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"y_min\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"y_max\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"increasing\": [\"boolean\", StrOptions({\"auto\"})],\n        \"out_of_bounds\": [StrOptions({\"nan\", \"clip\", \"raise\"})],\n    }\n\n    def __init__(self, *, y_min=None, y_max=None, increasing=True, out_of_bounds=\"nan\"):\n        self.y_min = y_min\n        self.y_max = y_max\n        self.increasing = increasing\n        self.out_of_bounds = out_of_bounds\n\n    def _check_input_data_shape(self, X):\n        if not (X.ndim == 1 or (X.ndim == 2 and X.shape[1] == 1)):\n            msg = (\n                \"Isotonic regression input X should be a 1d array or \"\n                \"2d array with 1 feature\"\n            )\n            raise ValueError(msg)\n\n    def _build_f(self, X, y):\n        \"\"\"Build the f_ interp1d function.\"\"\"\n\n        bounds_error = self.out_of_bounds == \"raise\"\n        if len(y) == 1:\n            # single y, constant prediction\n            self.f_ = lambda x: y.repeat(x.shape)\n        else:\n            self.f_ = interpolate.interp1d(\n                X, y, kind=\"linear\", bounds_error=bounds_error\n            )\n\n    def _build_y(self, X, y, sample_weight, trim_duplicates=True):\n        \"\"\"Build the y_ IsotonicRegression.\"\"\"\n        self._check_input_data_shape(X)\n        X = X.reshape(-1)  # use 1d view\n\n        # Determine increasing if auto-determination requested\n        if self.increasing == \"auto\":\n            self.increasing_ = check_increasing(X, y)\n        else:\n            self.increasing_ = self.increasing\n\n        # If sample_weights is passed, removed zero-weight values and clean\n        # order\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        mask = sample_weight > 0\n        X, y, sample_weight = X[mask], y[mask], sample_weight[mask]\n\n        order = np.lexsort((y, X))\n        X, y, sample_weight = [array[order] for array in [X, y, sample_weight]]\n        unique_X, unique_y, unique_sample_weight = _make_unique(X, y, sample_weight)\n\n        X = unique_X\n        y = isotonic_regression(\n            unique_y,\n            sample_weight=unique_sample_weight,\n            y_min=self.y_min,\n            y_max=self.y_max,\n            increasing=self.increasing_,\n        )\n\n        # Handle the left and right bounds on X\n        self.X_min_, self.X_max_ = np.min(X), np.max(X)\n\n        if trim_duplicates:\n            # Remove unnecessary points for faster prediction\n            keep_data = np.ones((len(y),), dtype=bool)\n            # Aside from the 1st and last point, remove points whose y values\n            # are equal to both the point before and the point after it.\n            keep_data[1:-1] = np.logical_or(\n                np.not_equal(y[1:-1], y[:-2]), np.not_equal(y[1:-1], y[2:])\n            )\n            return X[keep_data], y[keep_data]\n        else:\n            # The ability to turn off trim_duplicates is only used to it make\n            # easier to unit test that removing duplicates in y does not have\n            # any impact the resulting interpolation function (besides\n            # prediction speed).\n            return X, y\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples,) or (n_samples, 1)\n            Training data.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        y : array-like of shape (n_samples,)\n            Training target.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Weights. If set to None, all weights will be set to 1 (equal\n            weights).\n\n        Returns\n        -------\n        self : object\n            Returns an instance of self.\n\n        Notes\n        -----\n        X is stored for future use, as :meth:`transform` needs X to interpolate\n        new input data.\n        \"\"\"\n        self._validate_params()\n        check_params = dict(accept_sparse=False, ensure_2d=False)\n        X = check_array(\n            X, input_name=\"X\", dtype=[np.float64, np.float32], **check_params\n        )\n        y = check_array(y, input_name=\"y\", dtype=X.dtype, **check_params)\n        check_consistent_length(X, y, sample_weight)\n\n        # Transform y by running the isotonic regression algorithm and\n        # transform X accordingly.\n        X, y = self._build_y(X, y, sample_weight)\n\n        # It is necessary to store the non-redundant part of the training set\n        # on the model to make it possible to support model persistence via\n        # the pickle module as the object built by scipy.interp1d is not\n        # picklable directly.\n        self.X_thresholds_, self.y_thresholds_ = X, y\n\n        # Build the interpolation function\n        self._build_f(X, y)\n        return self\n\n    def transform(self, T):\n        \"\"\"Transform new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            The transformed data.\n        \"\"\"\n\n        if hasattr(self, \"X_thresholds_\"):\n            dtype = self.X_thresholds_.dtype\n        else:\n            dtype = np.float64\n\n        T = check_array(T, dtype=dtype, ensure_2d=False)\n\n        self._check_input_data_shape(T)\n        T = T.reshape(-1)  # use 1d view\n\n        if self.out_of_bounds == \"clip\":\n            T = np.clip(T, self.X_min_, self.X_max_)\n\n        res = self.f_(T)\n\n        # on scipy 0.17, interp1d up-casts to float64, so we cast back\n        res = res.astype(T.dtype)\n\n        return res\n\n    def predict(self, T):\n        \"\"\"Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            Transformed data.\n        \"\"\"\n        return self.transform(T)\n\n    # We implement get_feature_names_out here instead of using\n    # `ClassNamePrefixFeaturesOutMixin`` because `input_features` are ignored.\n    # `input_features` are ignored because `IsotonicRegression` accepts 1d\n    # arrays and the semantics of `feature_names_in_` are not clear for 1d arrays.\n    def get_feature_names_out(self, input_features=None):\n        \"\"\"Get output feature names for transformation.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Ignored.\n\n        Returns\n        -------\n        feature_names_out : ndarray of str objects\n            An ndarray with one string i.e. [\"isotonicregression0\"].\n        \"\"\"\n        check_is_fitted(self, \"f_\")\n        class_name = self.__class__.__name__.lower()\n        return np.asarray([f\"{class_name}0\"], dtype=object)\n\n    def __getstate__(self):\n        \"\"\"Pickle-protocol - return state of the estimator.\"\"\"\n        state = super().__getstate__()\n        # remove interpolation method\n        state.pop(\"f_\", None)\n        return state\n\n    def __setstate__(self, state):\n        \"\"\"Pickle-protocol - set state of the estimator.\n\n        We need to rebuild the interpolation function.\n        \"\"\"\n        super().__setstate__(state)\n        if hasattr(self, \"X_thresholds_\") and hasattr(self, \"y_thresholds_\"):\n            self._build_f(self.X_thresholds_, self.y_thresholds_)\n\n    def _more_tags(self):\n        return {\"X_types\": [\"1darray\"]}\n```\n--- END FILE ---\n\nPlease first localize the bug based on the issue statement, and then generate *SEARCH/REPLACE* edits to fix the issue.\n\nEvery *SEARCH/REPLACE* edit must use this format:\n1. The file path\n2. The start of search block: <<<<<<< SEARCH\n3. A contiguous chunk of lines to search for in the existing source code\n4. The dividing line: =======\n5. The lines to replace into the source code\n6. The end of the replace block: >>>>>>> REPLACE\n\nHere is an example:\n\n```python\n### mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nPlease note that the *SEARCH/REPLACE* edit REQUIRES PROPER INDENTATION. If you would like to add the line '        print(x)', you must fully write that out, with all those spaces before the code!\nWrap the *SEARCH/REPLACE* edit in blocks ```python...```."}, {"response": "The issue arises because the `IsotonicRegression`'s `predict` method returns a DataFrame instead of a numpy array when `set_config(transform_output=\"pandas\")` is set. This DataFrame causes issues when the `_CalibratedClassifier` tries to assign these predictions to a numpy array.\n\nTo fix this, we need to ensure that the `predict` method of `IsotonicRegression` always returns a numpy array, regardless of the output configuration.\n\nLet's make the necessary modifications:\n\n1. **In `sklearn/isotonic.py`, force `predict` to always return a numpy array.**\n\nHere's the *SEARCH/REPLACE* edit:\n\n```python\n### sklearn/isotonic.py\n<<<<<<< SEARCH\n        return res\n=======\n        return np.asarray(res)\n>>>>>>> REPLACE\n```\n\nNow, let's apply the change in the file `sklearn/isotonic.py`:\n\n```python\n### sklearn/isotonic.py\n<<<<<<< SEARCH\n        return res\n=======\n        return np.asarray(res)\n>>>>>>> REPLACE\n```\n\nThis change ensures that `IsotonicRegression.predict` always returns a numpy array, which will then be compatible with the `_CalibratedClassifier`.\n\nAfter applying this change, you should run the tests to ensure that the issue is resolved and that the fix doesn't introduce any new issues. If the tests pass, this means the problem is fixed.", "usage": {"completion_tokens": 0, "prompt_tokens": 0}, "prompt": "We are currently solving the following issue within our repository. Here is the issue text:\n--- BEGIN ISSUE ---\nCalibratedClassifierCV doesn't work with `set_config(transform_output=\"pandas\")`\n### Describe the bug\r\n\r\nCalibratedClassifierCV with isotonic regression doesn't work when we previously set `set_config(transform_output=\"pandas\")`.\r\nThe IsotonicRegression seems to return a dataframe, which is a problem for `_CalibratedClassifier`  in `predict_proba` where it tries to put the dataframe in a numpy array row `proba[:, class_idx] = calibrator.predict(this_pred)`.\r\n\r\n### Steps/Code to Reproduce\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn import set_config\r\nfrom sklearn.calibration import CalibratedClassifierCV\r\nfrom sklearn.linear_model import SGDClassifier\r\n\r\nset_config(transform_output=\"pandas\")\r\nmodel = CalibratedClassifierCV(SGDClassifier(), method='isotonic')\r\nmodel.fit(np.arange(90).reshape(30, -1), np.arange(30) % 2)\r\nmodel.predict(np.arange(90).reshape(30, -1))\r\n```\r\n\r\n### Expected Results\r\n\r\nIt should not crash.\r\n\r\n### Actual Results\r\n\r\n```\r\n../core/model_trainer.py:306: in train_model\r\n    cv_predictions = cross_val_predict(pipeline,\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:968: in cross_val_predict\r\n    predictions = parallel(\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:1085: in __call__\r\n    if self.dispatch_one_batch(iterator):\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:901: in dispatch_one_batch\r\n    self._dispatch(tasks)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:819: in _dispatch\r\n    job = self._backend.apply_async(batch, callback=cb)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/_parallel_backends.py:208: in apply_async\r\n    result = ImmediateResult(func)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/_parallel_backends.py:597: in __init__\r\n    self.results = batch()\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:288: in __call__\r\n    return [func(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:288: in <listcomp>\r\n    return [func(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/utils/fixes.py:117: in __call__\r\n    return self.function(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:1052: in _fit_and_predict\r\n    predictions = func(X_test)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/pipeline.py:548: in predict_proba\r\n    return self.steps[-1][1].predict_proba(Xt, **predict_proba_params)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/calibration.py:477: in predict_proba\r\n    proba = calibrated_classifier.predict_proba(X)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/calibration.py:764: in predict_proba\r\n    proba[:, class_idx] = calibrator.predict(this_pred)\r\nE   ValueError: could not broadcast input array from shape (20,1) into shape (20,)\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.9.15 (main, Nov 24 2022, 14:31:59)  [GCC 11.2.0]\r\nexecutable: /home/philippe/.anaconda3/envs/strategy-training/bin/python\r\n   machine: Linux-5.15.0-57-generic-x86_64-with-glibc2.31\r\n\r\nPython dependencies:\r\n      sklearn: 1.2.0\r\n          pip: 22.2.2\r\n   setuptools: 62.3.2\r\n        numpy: 1.23.5\r\n        scipy: 1.9.3\r\n       Cython: None\r\n       pandas: 1.4.1\r\n   matplotlib: 3.6.3\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libgomp\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0\r\n        version: None\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/numpy.libs/libopenblas64_p-r0-742d56dc.3.20.so\r\n        version: 0.3.20\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/scipy.libs/libopenblasp-r0-41284840.3.18.so\r\n        version: 0.3.18\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n```\r\n\n\n--- END ISSUE ---\n\n\nWe also have a fail to pass unit test for the given issue. We can use this to help identify and fix the issue. Here is the patch text:\n--- BEGIN PATCH ---\ndiff --git a/sklearn/tests/test_isotonic.py b/sklearn/tests/test_isotonic.py\n--- a/sklearn/tests/test_isotonic.py\n+++ b/sklearn/tests/test_isotonic.py\n@@ -5,6 +5,7 @@\n \n import pytest\n \n+import sklearn\n from sklearn.datasets import make_regression\n from sklearn.isotonic import (\n     check_increasing,\n@@ -680,3 +681,24 @@ def test_get_feature_names_out(shape):\n     assert isinstance(names, np.ndarray)\n     assert names.dtype == object\n     assert_array_equal([\"isotonicregression0\"], names)\n+\n+\n+def test_isotonic_regression_output_predict():\n+    \"\"\"Check that `predict` does return the expected output type.\n+\n+    We need to check that `transform` will output a DataFrame and a NumPy array\n+    when we set `transform_output` to `pandas`.\n+\n+    Non-regression test for:\n+    https://github.com/scikit-learn/scikit-learn/issues/25499\n+    \"\"\"\n+    pd = pytest.importorskip(\"pandas\")\n+    X, y = make_regression(n_samples=10, n_features=1, random_state=42)\n+    regressor = IsotonicRegression()\n+    with sklearn.config_context(transform_output=\"pandas\"):\n+        regressor.fit(X, y)\n+        X_trans = regressor.transform(X)\n+        y_pred = regressor.predict(X)\n+\n+    assert isinstance(X_trans, pd.DataFrame)\n+    assert isinstance(y_pred, np.ndarray)\n\n--- END PATCH ---\n\n\n\nBelow are some code segments, each from a relevant file. One or more of these files may contain bugs.\n\n--- BEGIN FILE ---\n```\n### sklearn/calibration.py\n...\n    \"\"\"\n    if hasattr(clf, \"decision_function\"):\n        method = getattr(clf, \"decision_function\")\n        return method, \"decision_function\"\n\n    if hasattr(clf, \"predict_proba\"):\n        method = getattr(clf, \"predict_proba\")\n        return method, \"predict_proba\"\n\n\ndef _compute_predictions(pred_method, method_name, X, n_classes):\n    \"\"\"Return predictions for `X` and reshape binary outputs to shape\n    (n_samples, 1).\n\n    Parameters\n    ----------\n    pred_method : callable\n        Prediction method.\n\n    method_name: str\n        Name of the prediction method\n\n    X : array-like or None\n        Data used to obtain predictions.\n\n    n_classes : int\n        Number of classes present.\n\n    Returns\n    -------\n    predictions : array-like, shape (X.shape[0], len(clf.classes_))\n        The predictions. Note if there are 2 classes, array is of shape\n        (X.shape[0], 1).\n    \"\"\"\n    predictions = pred_method(X=X)\n\n    if method_name == \"decision_function\":\n        if predictions.ndim == 1:\n            predictions = predictions[:, np.newaxis]\n    elif method_name == \"predict_proba\":\n        if n_classes == 2:\n            predictions = predictions[:, 1:]\n    else:  # pragma: no cover\n        # this branch should be unreachable.\n        raise ValueError(f\"Invalid prediction method: {method_name}\")\n    return predictions\n\n\ndef _fit_calibrator(clf, predictions, y, classes, method, sample_weight=None):\n    \"\"\"Fit calibrator(s) and return a `_CalibratedClassifier`\n    instance.\n\n    `n_classes` (i.e. `len(clf.classes_)`) calibrators are fitted.\n    However, if `n_classes` equals 2, one calibrator is fitted.\n\n    Parameters\n    ----------\n    clf : estimator instance\n        Fitted classifier.\n\n    predictions : array-like, shape (n_samples, n_classes) or (n_samples, 1) \\\n                    when binary.\n        Raw predictions returned by the un-calibrated base classifier.\n\n    y : array-like, shape (n_samples,)\n        The targets.\n\n    classes : ndarray, shape (n_classes,)\n        All the prediction classes.\n\n    method : {'sigmoid', 'isotonic'}\n        The method to use for calibration.\n\n    sample_weight : ndarray, shape (n_samples,), default=None\n        Sample weights. If None, then samples are equally weighted.\n\n    Returns\n    -------\n    pipeline : _CalibratedClassifier instance\n    \"\"\"\n    Y = label_binarize(y, classes=classes)\n    label_encoder = LabelEncoder().fit(classes)\n    pos_class_indices = label_encoder.transform(clf.classes_)\n    calibrators = []\n    for class_idx, this_pred in zip(pos_class_indices, predictions.T):\n        if method == \"isotonic\":\n            calibrator = IsotonicRegression(out_of_bounds=\"clip\")\n        else:  # \"sigmoid\"\n            calibrator = _SigmoidCalibration()\n        calibrator.fit(this_pred, Y[:, class_idx], sample_weight)\n        calibrators.append(calibrator)\n\n    pipeline = _CalibratedClassifier(clf, calibrators, method=method, classes=classes)\n    return pipeline\n\n\nclass _CalibratedClassifier:\n    \"\"\"Pipeline-like chaining a fitted classifier and its fitted calibrators.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Fitted classifier.\n\n    calibrators : list of fitted estimator instances\n        List of fitted calibrators (either 'IsotonicRegression' or\n        '_SigmoidCalibration'). The number of calibrators equals the number of\n        classes. However, if there are 2 classes, the list contains only one\n        fitted calibrator.\n\n    classes : array-like of shape (n_classes,)\n        All the prediction classes.\n\n    method : {'sigmoid', 'isotonic'}, default='sigmoid'\n        The method to use for calibration. Can be 'sigmoid' which\n        corresponds to Platt's method or 'isotonic' which is a\n        non-parametric approach based on isotonic regression.\n    \"\"\"\n\n    def __init__(self, estimator, calibrators, *, classes, method=\"sigmoid\"):\n        self.estimator = estimator\n        self.calibrators = calibrators\n        self.classes = classes\n        self.method = method\n\n    def predict_proba(self, X):\n        \"\"\"Calculate calibrated probabilities.\n\n        Calculates classification calibrated probabilities\n        for each class, in a one-vs-all manner, for `X`.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            The sample data.\n\n        Returns\n        -------\n        proba : array, shape (n_samples, n_classes)\n            The predicted probabilities. Can be exact zeros.\n        \"\"\"\n        n_classes = len(self.classes)\n        pred_method, method_name = _get_prediction_method(self.estimator)\n        predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n\n        label_encoder = LabelEncoder().fit(self.classes)\n        pos_class_indices = label_encoder.transform(self.estimator.classes_)\n\n        proba = np.zeros((_num_samples(X), n_classes))\n        for class_idx, this_pred, calibrator in zip(\n            pos_class_indices, predictions.T, self.calibrators\n        ):\n            if n_classes == 2:\n                # When binary, `predictions` consists only of predictions for\n                # clf.classes_[1] but `pos_class_indices` = 0\n                class_idx += 1\n            proba[:, class_idx] = calibrator.predict(this_pred)\n\n        # Normalize the probabilities\n        if n_classes == 2:\n            proba[:, 0] = 1.0 - proba[:, 1]\n        else:\n            denominator = np.sum(proba, axis=1)[:, np.newaxis]\n            # In the edge case where for each class calibrator returns a null\n            # probability for a given sample, use the uniform distribution\n            # instead.\n            uniform_proba = np.full_like(proba, 1 / n_classes)\n            proba = np.divide(\n                proba, denominator, out=uniform_proba, where=denominator != 0\n            )\n\n        # Deal with cases where the predicted probability minimally exceeds 1.0\n        proba[(1.0 < proba) & (proba <= 1.0 + 1e-5)] = 1.0\n\n        return proba\n\n\ndef _sigmoid_calibration(predictions, y, sample_weight=None):\n    \"\"\"Probability Calibration with sigmoid method (Platt 2000)\n\n    Parameters\n    ----------\n    predictions : ndarray of shape (n_samples,)\n        The decision function or predict proba for the samples.\n\n...\n\n\n### sklearn/isotonic.py\n...\n    if y_min is not None or y_max is not None:\n        # Older versions of np.clip don't accept None as a bound, so use np.inf\n        if y_min is None:\n            y_min = -np.inf\n        if y_max is None:\n            y_max = np.inf\n        np.clip(y, y_min, y_max, y)\n    return y[order]\n\n\nclass IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n    \"\"\"Isotonic regression model.\n\n    Read more in the :ref:`User Guide <isotonic>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    y_min : float, default=None\n        Lower bound on the lowest predicted value (the minimum value may\n        still be higher). If not set, defaults to -inf.\n\n    y_max : float, default=None\n        Upper bound on the highest predicted value (the maximum may still be\n        lower). If not set, defaults to +inf.\n\n    increasing : bool or 'auto', default=True\n        Determines whether the predictions should be constrained to increase\n        or decrease with `X`. 'auto' will decide based on the Spearman\n        correlation estimate's sign.\n\n    out_of_bounds : {'nan', 'clip', 'raise'}, default='nan'\n        Handles how `X` values outside of the training domain are handled\n        during prediction.\n\n        - 'nan', predictions will be NaN.\n        - 'clip', predictions will be set to the value corresponding to\n          the nearest train interval endpoint.\n        - 'raise', a `ValueError` is raised.\n\n    Attributes\n    ----------\n    X_min_ : float\n        Minimum value of input array `X_` for left bound.\n\n    X_max_ : float\n        Maximum value of input array `X_` for right bound.\n\n    X_thresholds_ : ndarray of shape (n_thresholds,)\n        Unique ascending `X` values used to interpolate\n        the y = f(X) monotonic function.\n\n        .. versionadded:: 0.24\n\n    y_thresholds_ : ndarray of shape (n_thresholds,)\n        De-duplicated `y` values suitable to interpolate the y = f(X)\n        monotonic function.\n\n        .. versionadded:: 0.24\n\n    f_ : function\n        The stepwise interpolating function that covers the input domain ``X``.\n\n    increasing_ : bool\n        Inferred value for ``increasing``.\n\n    See Also\n    --------\n    sklearn.linear_model.LinearRegression : Ordinary least squares Linear\n        Regression.\n    sklearn.ensemble.HistGradientBoostingRegressor : Gradient boosting that\n        is a non-parametric model accepting monotonicity constraints.\n    isotonic_regression : Function to solve the isotonic regression model.\n\n    Notes\n    -----\n    Ties are broken using the secondary method from de Leeuw, 1977.\n\n    References\n    ----------\n    Isotonic Median Regression: A Linear Programming Approach\n    Nilotpal Chakravarti\n    Mathematics of Operations Research\n    Vol. 14, No. 2 (May, 1989), pp. 303-308\n\n    Isotone Optimization in R : Pool-Adjacent-Violators\n    Algorithm (PAVA) and Active Set Methods\n    de Leeuw, Hornik, Mair\n    Journal of Statistical Software 2009\n\n    Correctness of Kruskal's algorithms for monotone regression with ties\n    de Leeuw, Psychometrica, 1977\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.isotonic import IsotonicRegression\n    >>> X, y = make_regression(n_samples=10, n_features=1, random_state=41)\n    >>> iso_reg = IsotonicRegression().fit(X, y)\n    >>> iso_reg.predict([.1, .2])\n    array([1.8628..., 3.7256...])\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"y_min\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"y_max\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"increasing\": [\"boolean\", StrOptions({\"auto\"})],\n        \"out_of_bounds\": [StrOptions({\"nan\", \"clip\", \"raise\"})],\n    }\n\n    def __init__(self, *, y_min=None, y_max=None, increasing=True, out_of_bounds=\"nan\"):\n        self.y_min = y_min\n        self.y_max = y_max\n        self.increasing = increasing\n        self.out_of_bounds = out_of_bounds\n\n    def _check_input_data_shape(self, X):\n        if not (X.ndim == 1 or (X.ndim == 2 and X.shape[1] == 1)):\n            msg = (\n                \"Isotonic regression input X should be a 1d array or \"\n                \"2d array with 1 feature\"\n            )\n            raise ValueError(msg)\n\n    def _build_f(self, X, y):\n        \"\"\"Build the f_ interp1d function.\"\"\"\n\n        bounds_error = self.out_of_bounds == \"raise\"\n        if len(y) == 1:\n            # single y, constant prediction\n            self.f_ = lambda x: y.repeat(x.shape)\n        else:\n            self.f_ = interpolate.interp1d(\n                X, y, kind=\"linear\", bounds_error=bounds_error\n            )\n\n    def _build_y(self, X, y, sample_weight, trim_duplicates=True):\n        \"\"\"Build the y_ IsotonicRegression.\"\"\"\n        self._check_input_data_shape(X)\n        X = X.reshape(-1)  # use 1d view\n\n        # Determine increasing if auto-determination requested\n        if self.increasing == \"auto\":\n            self.increasing_ = check_increasing(X, y)\n        else:\n            self.increasing_ = self.increasing\n\n        # If sample_weights is passed, removed zero-weight values and clean\n        # order\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        mask = sample_weight > 0\n        X, y, sample_weight = X[mask], y[mask], sample_weight[mask]\n\n        order = np.lexsort((y, X))\n        X, y, sample_weight = [array[order] for array in [X, y, sample_weight]]\n        unique_X, unique_y, unique_sample_weight = _make_unique(X, y, sample_weight)\n\n        X = unique_X\n        y = isotonic_regression(\n            unique_y,\n            sample_weight=unique_sample_weight,\n            y_min=self.y_min,\n            y_max=self.y_max,\n            increasing=self.increasing_,\n        )\n\n        # Handle the left and right bounds on X\n        self.X_min_, self.X_max_ = np.min(X), np.max(X)\n\n        if trim_duplicates:\n            # Remove unnecessary points for faster prediction\n            keep_data = np.ones((len(y),), dtype=bool)\n            # Aside from the 1st and last point, remove points whose y values\n            # are equal to both the point before and the point after it.\n            keep_data[1:-1] = np.logical_or(\n                np.not_equal(y[1:-1], y[:-2]), np.not_equal(y[1:-1], y[2:])\n            )\n            return X[keep_data], y[keep_data]\n        else:\n            # The ability to turn off trim_duplicates is only used to it make\n            # easier to unit test that removing duplicates in y does not have\n            # any impact the resulting interpolation function (besides\n            # prediction speed).\n            return X, y\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples,) or (n_samples, 1)\n            Training data.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        y : array-like of shape (n_samples,)\n            Training target.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Weights. If set to None, all weights will be set to 1 (equal\n            weights).\n\n        Returns\n        -------\n        self : object\n            Returns an instance of self.\n\n        Notes\n        -----\n        X is stored for future use, as :meth:`transform` needs X to interpolate\n        new input data.\n        \"\"\"\n        self._validate_params()\n        check_params = dict(accept_sparse=False, ensure_2d=False)\n        X = check_array(\n            X, input_name=\"X\", dtype=[np.float64, np.float32], **check_params\n        )\n        y = check_array(y, input_name=\"y\", dtype=X.dtype, **check_params)\n        check_consistent_length(X, y, sample_weight)\n\n        # Transform y by running the isotonic regression algorithm and\n        # transform X accordingly.\n        X, y = self._build_y(X, y, sample_weight)\n\n        # It is necessary to store the non-redundant part of the training set\n        # on the model to make it possible to support model persistence via\n        # the pickle module as the object built by scipy.interp1d is not\n        # picklable directly.\n        self.X_thresholds_, self.y_thresholds_ = X, y\n\n        # Build the interpolation function\n        self._build_f(X, y)\n        return self\n\n    def transform(self, T):\n        \"\"\"Transform new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            The transformed data.\n        \"\"\"\n\n        if hasattr(self, \"X_thresholds_\"):\n            dtype = self.X_thresholds_.dtype\n        else:\n            dtype = np.float64\n\n        T = check_array(T, dtype=dtype, ensure_2d=False)\n\n        self._check_input_data_shape(T)\n        T = T.reshape(-1)  # use 1d view\n\n        if self.out_of_bounds == \"clip\":\n            T = np.clip(T, self.X_min_, self.X_max_)\n\n        res = self.f_(T)\n\n        # on scipy 0.17, interp1d up-casts to float64, so we cast back\n        res = res.astype(T.dtype)\n\n        return res\n\n    def predict(self, T):\n        \"\"\"Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            Transformed data.\n        \"\"\"\n        return self.transform(T)\n\n    # We implement get_feature_names_out here instead of using\n    # `ClassNamePrefixFeaturesOutMixin`` because `input_features` are ignored.\n    # `input_features` are ignored because `IsotonicRegression` accepts 1d\n    # arrays and the semantics of `feature_names_in_` are not clear for 1d arrays.\n    def get_feature_names_out(self, input_features=None):\n        \"\"\"Get output feature names for transformation.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Ignored.\n\n        Returns\n        -------\n        feature_names_out : ndarray of str objects\n            An ndarray with one string i.e. [\"isotonicregression0\"].\n        \"\"\"\n        check_is_fitted(self, \"f_\")\n        class_name = self.__class__.__name__.lower()\n        return np.asarray([f\"{class_name}0\"], dtype=object)\n\n    def __getstate__(self):\n        \"\"\"Pickle-protocol - return state of the estimator.\"\"\"\n        state = super().__getstate__()\n        # remove interpolation method\n        state.pop(\"f_\", None)\n        return state\n\n    def __setstate__(self, state):\n        \"\"\"Pickle-protocol - set state of the estimator.\n\n        We need to rebuild the interpolation function.\n        \"\"\"\n        super().__setstate__(state)\n        if hasattr(self, \"X_thresholds_\") and hasattr(self, \"y_thresholds_\"):\n            self._build_f(self.X_thresholds_, self.y_thresholds_)\n\n    def _more_tags(self):\n        return {\"X_types\": [\"1darray\"]}\n```\n--- END FILE ---\n\nPlease first localize the bug based on the issue statement, and then generate *SEARCH/REPLACE* edits to fix the issue.\n\nEvery *SEARCH/REPLACE* edit must use this format:\n1. The file path\n2. The start of search block: <<<<<<< SEARCH\n3. A contiguous chunk of lines to search for in the existing source code\n4. The dividing line: =======\n5. The lines to replace into the source code\n6. The end of the replace block: >>>>>>> REPLACE\n\nHere is an example:\n\n```python\n### mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nPlease note that the *SEARCH/REPLACE* edit REQUIRES PROPER INDENTATION. If you would like to add the line '        print(x)', you must fully write that out, with all those spaces before the code!\nWrap the *SEARCH/REPLACE* edit in blocks ```python...```."}, {"response": "To fix the issue with `CalibratedClassifierCV` when using `set_config(transform_output=\"pandas\")`, we need to ensure that the `IsotonicRegression` predict method returns a numpy array even if the output configuration is set to pandas. We can achieve this by modifying the `predict` method of the `IsotonicRegression` class.\n\nHere are the necessary *SEARCH/REPLACE* edits:\n\n```python\n### sklearn/isotonic.py\n<<<<<<< SEARCH\n        return self.transform(T)\n=======\n        return np.asarray(self.transform(T))\n>>>>>>> REPLACE\n```\n\nThis ensures that the `predict` method of `IsotonicRegression` will always return a numpy array, which avoids the broadcasting issue in `_CalibratedClassifier`.\n\nApply this change in the `sklearn/isotonic.py` file. The `predict` method should convert the result of `transform` into a numpy array before returning it. This simple change should resolve the issue described.", "usage": {"completion_tokens": 0, "prompt_tokens": 0}, "prompt": "We are currently solving the following issue within our repository. Here is the issue text:\n--- BEGIN ISSUE ---\nCalibratedClassifierCV doesn't work with `set_config(transform_output=\"pandas\")`\n### Describe the bug\r\n\r\nCalibratedClassifierCV with isotonic regression doesn't work when we previously set `set_config(transform_output=\"pandas\")`.\r\nThe IsotonicRegression seems to return a dataframe, which is a problem for `_CalibratedClassifier`  in `predict_proba` where it tries to put the dataframe in a numpy array row `proba[:, class_idx] = calibrator.predict(this_pred)`.\r\n\r\n### Steps/Code to Reproduce\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn import set_config\r\nfrom sklearn.calibration import CalibratedClassifierCV\r\nfrom sklearn.linear_model import SGDClassifier\r\n\r\nset_config(transform_output=\"pandas\")\r\nmodel = CalibratedClassifierCV(SGDClassifier(), method='isotonic')\r\nmodel.fit(np.arange(90).reshape(30, -1), np.arange(30) % 2)\r\nmodel.predict(np.arange(90).reshape(30, -1))\r\n```\r\n\r\n### Expected Results\r\n\r\nIt should not crash.\r\n\r\n### Actual Results\r\n\r\n```\r\n../core/model_trainer.py:306: in train_model\r\n    cv_predictions = cross_val_predict(pipeline,\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:968: in cross_val_predict\r\n    predictions = parallel(\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:1085: in __call__\r\n    if self.dispatch_one_batch(iterator):\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:901: in dispatch_one_batch\r\n    self._dispatch(tasks)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:819: in _dispatch\r\n    job = self._backend.apply_async(batch, callback=cb)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/_parallel_backends.py:208: in apply_async\r\n    result = ImmediateResult(func)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/_parallel_backends.py:597: in __init__\r\n    self.results = batch()\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:288: in __call__\r\n    return [func(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:288: in <listcomp>\r\n    return [func(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/utils/fixes.py:117: in __call__\r\n    return self.function(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:1052: in _fit_and_predict\r\n    predictions = func(X_test)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/pipeline.py:548: in predict_proba\r\n    return self.steps[-1][1].predict_proba(Xt, **predict_proba_params)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/calibration.py:477: in predict_proba\r\n    proba = calibrated_classifier.predict_proba(X)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/calibration.py:764: in predict_proba\r\n    proba[:, class_idx] = calibrator.predict(this_pred)\r\nE   ValueError: could not broadcast input array from shape (20,1) into shape (20,)\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.9.15 (main, Nov 24 2022, 14:31:59)  [GCC 11.2.0]\r\nexecutable: /home/philippe/.anaconda3/envs/strategy-training/bin/python\r\n   machine: Linux-5.15.0-57-generic-x86_64-with-glibc2.31\r\n\r\nPython dependencies:\r\n      sklearn: 1.2.0\r\n          pip: 22.2.2\r\n   setuptools: 62.3.2\r\n        numpy: 1.23.5\r\n        scipy: 1.9.3\r\n       Cython: None\r\n       pandas: 1.4.1\r\n   matplotlib: 3.6.3\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libgomp\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0\r\n        version: None\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/numpy.libs/libopenblas64_p-r0-742d56dc.3.20.so\r\n        version: 0.3.20\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/scipy.libs/libopenblasp-r0-41284840.3.18.so\r\n        version: 0.3.18\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n```\r\n\n\n--- END ISSUE ---\n\n\nWe also have a fail to pass unit test for the given issue. We can use this to help identify and fix the issue. Here is the patch text:\n--- BEGIN PATCH ---\ndiff --git a/sklearn/tests/test_isotonic.py b/sklearn/tests/test_isotonic.py\n--- a/sklearn/tests/test_isotonic.py\n+++ b/sklearn/tests/test_isotonic.py\n@@ -5,6 +5,7 @@\n \n import pytest\n \n+import sklearn\n from sklearn.datasets import make_regression\n from sklearn.isotonic import (\n     check_increasing,\n@@ -680,3 +681,24 @@ def test_get_feature_names_out(shape):\n     assert isinstance(names, np.ndarray)\n     assert names.dtype == object\n     assert_array_equal([\"isotonicregression0\"], names)\n+\n+\n+def test_isotonic_regression_output_predict():\n+    \"\"\"Check that `predict` does return the expected output type.\n+\n+    We need to check that `transform` will output a DataFrame and a NumPy array\n+    when we set `transform_output` to `pandas`.\n+\n+    Non-regression test for:\n+    https://github.com/scikit-learn/scikit-learn/issues/25499\n+    \"\"\"\n+    pd = pytest.importorskip(\"pandas\")\n+    X, y = make_regression(n_samples=10, n_features=1, random_state=42)\n+    regressor = IsotonicRegression()\n+    with sklearn.config_context(transform_output=\"pandas\"):\n+        regressor.fit(X, y)\n+        X_trans = regressor.transform(X)\n+        y_pred = regressor.predict(X)\n+\n+    assert isinstance(X_trans, pd.DataFrame)\n+    assert isinstance(y_pred, np.ndarray)\n\n--- END PATCH ---\n\n\n\nBelow are some code segments, each from a relevant file. One or more of these files may contain bugs.\n\n--- BEGIN FILE ---\n```\n### sklearn/calibration.py\n...\n    \"\"\"\n    if hasattr(clf, \"decision_function\"):\n        method = getattr(clf, \"decision_function\")\n        return method, \"decision_function\"\n\n    if hasattr(clf, \"predict_proba\"):\n        method = getattr(clf, \"predict_proba\")\n        return method, \"predict_proba\"\n\n\ndef _compute_predictions(pred_method, method_name, X, n_classes):\n    \"\"\"Return predictions for `X` and reshape binary outputs to shape\n    (n_samples, 1).\n\n    Parameters\n    ----------\n    pred_method : callable\n        Prediction method.\n\n    method_name: str\n        Name of the prediction method\n\n    X : array-like or None\n        Data used to obtain predictions.\n\n    n_classes : int\n        Number of classes present.\n\n    Returns\n    -------\n    predictions : array-like, shape (X.shape[0], len(clf.classes_))\n        The predictions. Note if there are 2 classes, array is of shape\n        (X.shape[0], 1).\n    \"\"\"\n    predictions = pred_method(X=X)\n\n    if method_name == \"decision_function\":\n        if predictions.ndim == 1:\n            predictions = predictions[:, np.newaxis]\n    elif method_name == \"predict_proba\":\n        if n_classes == 2:\n            predictions = predictions[:, 1:]\n    else:  # pragma: no cover\n        # this branch should be unreachable.\n        raise ValueError(f\"Invalid prediction method: {method_name}\")\n    return predictions\n\n\ndef _fit_calibrator(clf, predictions, y, classes, method, sample_weight=None):\n    \"\"\"Fit calibrator(s) and return a `_CalibratedClassifier`\n    instance.\n\n    `n_classes` (i.e. `len(clf.classes_)`) calibrators are fitted.\n    However, if `n_classes` equals 2, one calibrator is fitted.\n\n    Parameters\n    ----------\n    clf : estimator instance\n        Fitted classifier.\n\n    predictions : array-like, shape (n_samples, n_classes) or (n_samples, 1) \\\n                    when binary.\n        Raw predictions returned by the un-calibrated base classifier.\n\n    y : array-like, shape (n_samples,)\n        The targets.\n\n    classes : ndarray, shape (n_classes,)\n        All the prediction classes.\n\n    method : {'sigmoid', 'isotonic'}\n        The method to use for calibration.\n\n    sample_weight : ndarray, shape (n_samples,), default=None\n        Sample weights. If None, then samples are equally weighted.\n\n    Returns\n    -------\n    pipeline : _CalibratedClassifier instance\n    \"\"\"\n    Y = label_binarize(y, classes=classes)\n    label_encoder = LabelEncoder().fit(classes)\n    pos_class_indices = label_encoder.transform(clf.classes_)\n    calibrators = []\n    for class_idx, this_pred in zip(pos_class_indices, predictions.T):\n        if method == \"isotonic\":\n            calibrator = IsotonicRegression(out_of_bounds=\"clip\")\n        else:  # \"sigmoid\"\n            calibrator = _SigmoidCalibration()\n        calibrator.fit(this_pred, Y[:, class_idx], sample_weight)\n        calibrators.append(calibrator)\n\n    pipeline = _CalibratedClassifier(clf, calibrators, method=method, classes=classes)\n    return pipeline\n\n\nclass _CalibratedClassifier:\n    \"\"\"Pipeline-like chaining a fitted classifier and its fitted calibrators.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Fitted classifier.\n\n    calibrators : list of fitted estimator instances\n        List of fitted calibrators (either 'IsotonicRegression' or\n        '_SigmoidCalibration'). The number of calibrators equals the number of\n        classes. However, if there are 2 classes, the list contains only one\n        fitted calibrator.\n\n    classes : array-like of shape (n_classes,)\n        All the prediction classes.\n\n    method : {'sigmoid', 'isotonic'}, default='sigmoid'\n        The method to use for calibration. Can be 'sigmoid' which\n        corresponds to Platt's method or 'isotonic' which is a\n        non-parametric approach based on isotonic regression.\n    \"\"\"\n\n    def __init__(self, estimator, calibrators, *, classes, method=\"sigmoid\"):\n        self.estimator = estimator\n        self.calibrators = calibrators\n        self.classes = classes\n        self.method = method\n\n    def predict_proba(self, X):\n        \"\"\"Calculate calibrated probabilities.\n\n        Calculates classification calibrated probabilities\n        for each class, in a one-vs-all manner, for `X`.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            The sample data.\n\n        Returns\n        -------\n        proba : array, shape (n_samples, n_classes)\n            The predicted probabilities. Can be exact zeros.\n        \"\"\"\n        n_classes = len(self.classes)\n        pred_method, method_name = _get_prediction_method(self.estimator)\n        predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n\n        label_encoder = LabelEncoder().fit(self.classes)\n        pos_class_indices = label_encoder.transform(self.estimator.classes_)\n\n        proba = np.zeros((_num_samples(X), n_classes))\n        for class_idx, this_pred, calibrator in zip(\n            pos_class_indices, predictions.T, self.calibrators\n        ):\n            if n_classes == 2:\n                # When binary, `predictions` consists only of predictions for\n                # clf.classes_[1] but `pos_class_indices` = 0\n                class_idx += 1\n            proba[:, class_idx] = calibrator.predict(this_pred)\n\n        # Normalize the probabilities\n        if n_classes == 2:\n            proba[:, 0] = 1.0 - proba[:, 1]\n        else:\n            denominator = np.sum(proba, axis=1)[:, np.newaxis]\n            # In the edge case where for each class calibrator returns a null\n            # probability for a given sample, use the uniform distribution\n            # instead.\n            uniform_proba = np.full_like(proba, 1 / n_classes)\n            proba = np.divide(\n                proba, denominator, out=uniform_proba, where=denominator != 0\n            )\n\n        # Deal with cases where the predicted probability minimally exceeds 1.0\n        proba[(1.0 < proba) & (proba <= 1.0 + 1e-5)] = 1.0\n\n        return proba\n\n\ndef _sigmoid_calibration(predictions, y, sample_weight=None):\n    \"\"\"Probability Calibration with sigmoid method (Platt 2000)\n\n    Parameters\n    ----------\n    predictions : ndarray of shape (n_samples,)\n        The decision function or predict proba for the samples.\n\n...\n\n\n### sklearn/isotonic.py\n...\n    if y_min is not None or y_max is not None:\n        # Older versions of np.clip don't accept None as a bound, so use np.inf\n        if y_min is None:\n            y_min = -np.inf\n        if y_max is None:\n            y_max = np.inf\n        np.clip(y, y_min, y_max, y)\n    return y[order]\n\n\nclass IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n    \"\"\"Isotonic regression model.\n\n    Read more in the :ref:`User Guide <isotonic>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    y_min : float, default=None\n        Lower bound on the lowest predicted value (the minimum value may\n        still be higher). If not set, defaults to -inf.\n\n    y_max : float, default=None\n        Upper bound on the highest predicted value (the maximum may still be\n        lower). If not set, defaults to +inf.\n\n    increasing : bool or 'auto', default=True\n        Determines whether the predictions should be constrained to increase\n        or decrease with `X`. 'auto' will decide based on the Spearman\n        correlation estimate's sign.\n\n    out_of_bounds : {'nan', 'clip', 'raise'}, default='nan'\n        Handles how `X` values outside of the training domain are handled\n        during prediction.\n\n        - 'nan', predictions will be NaN.\n        - 'clip', predictions will be set to the value corresponding to\n          the nearest train interval endpoint.\n        - 'raise', a `ValueError` is raised.\n\n    Attributes\n    ----------\n    X_min_ : float\n        Minimum value of input array `X_` for left bound.\n\n    X_max_ : float\n        Maximum value of input array `X_` for right bound.\n\n    X_thresholds_ : ndarray of shape (n_thresholds,)\n        Unique ascending `X` values used to interpolate\n        the y = f(X) monotonic function.\n\n        .. versionadded:: 0.24\n\n    y_thresholds_ : ndarray of shape (n_thresholds,)\n        De-duplicated `y` values suitable to interpolate the y = f(X)\n        monotonic function.\n\n        .. versionadded:: 0.24\n\n    f_ : function\n        The stepwise interpolating function that covers the input domain ``X``.\n\n    increasing_ : bool\n        Inferred value for ``increasing``.\n\n    See Also\n    --------\n    sklearn.linear_model.LinearRegression : Ordinary least squares Linear\n        Regression.\n    sklearn.ensemble.HistGradientBoostingRegressor : Gradient boosting that\n        is a non-parametric model accepting monotonicity constraints.\n    isotonic_regression : Function to solve the isotonic regression model.\n\n    Notes\n    -----\n    Ties are broken using the secondary method from de Leeuw, 1977.\n\n    References\n    ----------\n    Isotonic Median Regression: A Linear Programming Approach\n    Nilotpal Chakravarti\n    Mathematics of Operations Research\n    Vol. 14, No. 2 (May, 1989), pp. 303-308\n\n    Isotone Optimization in R : Pool-Adjacent-Violators\n    Algorithm (PAVA) and Active Set Methods\n    de Leeuw, Hornik, Mair\n    Journal of Statistical Software 2009\n\n    Correctness of Kruskal's algorithms for monotone regression with ties\n    de Leeuw, Psychometrica, 1977\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.isotonic import IsotonicRegression\n    >>> X, y = make_regression(n_samples=10, n_features=1, random_state=41)\n    >>> iso_reg = IsotonicRegression().fit(X, y)\n    >>> iso_reg.predict([.1, .2])\n    array([1.8628..., 3.7256...])\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"y_min\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"y_max\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"increasing\": [\"boolean\", StrOptions({\"auto\"})],\n        \"out_of_bounds\": [StrOptions({\"nan\", \"clip\", \"raise\"})],\n    }\n\n    def __init__(self, *, y_min=None, y_max=None, increasing=True, out_of_bounds=\"nan\"):\n        self.y_min = y_min\n        self.y_max = y_max\n        self.increasing = increasing\n        self.out_of_bounds = out_of_bounds\n\n    def _check_input_data_shape(self, X):\n        if not (X.ndim == 1 or (X.ndim == 2 and X.shape[1] == 1)):\n            msg = (\n                \"Isotonic regression input X should be a 1d array or \"\n                \"2d array with 1 feature\"\n            )\n            raise ValueError(msg)\n\n    def _build_f(self, X, y):\n        \"\"\"Build the f_ interp1d function.\"\"\"\n\n        bounds_error = self.out_of_bounds == \"raise\"\n        if len(y) == 1:\n            # single y, constant prediction\n            self.f_ = lambda x: y.repeat(x.shape)\n        else:\n            self.f_ = interpolate.interp1d(\n                X, y, kind=\"linear\", bounds_error=bounds_error\n            )\n\n    def _build_y(self, X, y, sample_weight, trim_duplicates=True):\n        \"\"\"Build the y_ IsotonicRegression.\"\"\"\n        self._check_input_data_shape(X)\n        X = X.reshape(-1)  # use 1d view\n\n        # Determine increasing if auto-determination requested\n        if self.increasing == \"auto\":\n            self.increasing_ = check_increasing(X, y)\n        else:\n            self.increasing_ = self.increasing\n\n        # If sample_weights is passed, removed zero-weight values and clean\n        # order\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        mask = sample_weight > 0\n        X, y, sample_weight = X[mask], y[mask], sample_weight[mask]\n\n        order = np.lexsort((y, X))\n        X, y, sample_weight = [array[order] for array in [X, y, sample_weight]]\n        unique_X, unique_y, unique_sample_weight = _make_unique(X, y, sample_weight)\n\n        X = unique_X\n        y = isotonic_regression(\n            unique_y,\n            sample_weight=unique_sample_weight,\n            y_min=self.y_min,\n            y_max=self.y_max,\n            increasing=self.increasing_,\n        )\n\n        # Handle the left and right bounds on X\n        self.X_min_, self.X_max_ = np.min(X), np.max(X)\n\n        if trim_duplicates:\n            # Remove unnecessary points for faster prediction\n            keep_data = np.ones((len(y),), dtype=bool)\n            # Aside from the 1st and last point, remove points whose y values\n            # are equal to both the point before and the point after it.\n            keep_data[1:-1] = np.logical_or(\n                np.not_equal(y[1:-1], y[:-2]), np.not_equal(y[1:-1], y[2:])\n            )\n            return X[keep_data], y[keep_data]\n        else:\n            # The ability to turn off trim_duplicates is only used to it make\n            # easier to unit test that removing duplicates in y does not have\n            # any impact the resulting interpolation function (besides\n            # prediction speed).\n            return X, y\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples,) or (n_samples, 1)\n            Training data.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        y : array-like of shape (n_samples,)\n            Training target.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Weights. If set to None, all weights will be set to 1 (equal\n            weights).\n\n        Returns\n        -------\n        self : object\n            Returns an instance of self.\n\n        Notes\n        -----\n        X is stored for future use, as :meth:`transform` needs X to interpolate\n        new input data.\n        \"\"\"\n        self._validate_params()\n        check_params = dict(accept_sparse=False, ensure_2d=False)\n        X = check_array(\n            X, input_name=\"X\", dtype=[np.float64, np.float32], **check_params\n        )\n        y = check_array(y, input_name=\"y\", dtype=X.dtype, **check_params)\n        check_consistent_length(X, y, sample_weight)\n\n        # Transform y by running the isotonic regression algorithm and\n        # transform X accordingly.\n        X, y = self._build_y(X, y, sample_weight)\n\n        # It is necessary to store the non-redundant part of the training set\n        # on the model to make it possible to support model persistence via\n        # the pickle module as the object built by scipy.interp1d is not\n        # picklable directly.\n        self.X_thresholds_, self.y_thresholds_ = X, y\n\n        # Build the interpolation function\n        self._build_f(X, y)\n        return self\n\n    def transform(self, T):\n        \"\"\"Transform new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            The transformed data.\n        \"\"\"\n\n        if hasattr(self, \"X_thresholds_\"):\n            dtype = self.X_thresholds_.dtype\n        else:\n            dtype = np.float64\n\n        T = check_array(T, dtype=dtype, ensure_2d=False)\n\n        self._check_input_data_shape(T)\n        T = T.reshape(-1)  # use 1d view\n\n        if self.out_of_bounds == \"clip\":\n            T = np.clip(T, self.X_min_, self.X_max_)\n\n        res = self.f_(T)\n\n        # on scipy 0.17, interp1d up-casts to float64, so we cast back\n        res = res.astype(T.dtype)\n\n        return res\n\n    def predict(self, T):\n        \"\"\"Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            Transformed data.\n        \"\"\"\n        return self.transform(T)\n\n    # We implement get_feature_names_out here instead of using\n    # `ClassNamePrefixFeaturesOutMixin`` because `input_features` are ignored.\n    # `input_features` are ignored because `IsotonicRegression` accepts 1d\n    # arrays and the semantics of `feature_names_in_` are not clear for 1d arrays.\n    def get_feature_names_out(self, input_features=None):\n        \"\"\"Get output feature names for transformation.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Ignored.\n\n        Returns\n        -------\n        feature_names_out : ndarray of str objects\n            An ndarray with one string i.e. [\"isotonicregression0\"].\n        \"\"\"\n        check_is_fitted(self, \"f_\")\n        class_name = self.__class__.__name__.lower()\n        return np.asarray([f\"{class_name}0\"], dtype=object)\n\n    def __getstate__(self):\n        \"\"\"Pickle-protocol - return state of the estimator.\"\"\"\n        state = super().__getstate__()\n        # remove interpolation method\n        state.pop(\"f_\", None)\n        return state\n\n    def __setstate__(self, state):\n        \"\"\"Pickle-protocol - set state of the estimator.\n\n        We need to rebuild the interpolation function.\n        \"\"\"\n        super().__setstate__(state)\n        if hasattr(self, \"X_thresholds_\") and hasattr(self, \"y_thresholds_\"):\n            self._build_f(self.X_thresholds_, self.y_thresholds_)\n\n    def _more_tags(self):\n        return {\"X_types\": [\"1darray\"]}\n```\n--- END FILE ---\n\nPlease first localize the bug based on the issue statement, and then generate *SEARCH/REPLACE* edits to fix the issue.\n\nEvery *SEARCH/REPLACE* edit must use this format:\n1. The file path\n2. The start of search block: <<<<<<< SEARCH\n3. A contiguous chunk of lines to search for in the existing source code\n4. The dividing line: =======\n5. The lines to replace into the source code\n6. The end of the replace block: >>>>>>> REPLACE\n\nHere is an example:\n\n```python\n### mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nPlease note that the *SEARCH/REPLACE* edit REQUIRES PROPER INDENTATION. If you would like to add the line '        print(x)', you must fully write that out, with all those spaces before the code!\nWrap the *SEARCH/REPLACE* edit in blocks ```python...```."}, {"response": "```python\n### sklearn/calibration.py\n<<<<<<< SEARCH\n        for class_idx, this_pred, calibrator in zip(\n            pos_class_indices, predictions.T, self.calibrators\n        ):\n=======\n        for class_idx, this_pred, calibrator in zip(\n            pos_class_indices, predictions.T, self.calibrators\n        ):\n            if isinstance(this_pred, pd.DataFrame):\n                this_pred = this_pred.values.flatten()\n>>>>>>> REPLACE\n```\n\nThis change ensures that if `this_pred` is a pandas DataFrame, we convert it to a numpy array before using it in the further computations. This should address the error where the Isotonic Regression returns a DataFrame which causes a shape mismatch during the assignment to the numpy array in `predict_proba`.", "usage": {"completion_tokens": 0, "prompt_tokens": 0}, "prompt": "We are currently solving the following issue within our repository. Here is the issue text:\n--- BEGIN ISSUE ---\nCalibratedClassifierCV doesn't work with `set_config(transform_output=\"pandas\")`\n### Describe the bug\r\n\r\nCalibratedClassifierCV with isotonic regression doesn't work when we previously set `set_config(transform_output=\"pandas\")`.\r\nThe IsotonicRegression seems to return a dataframe, which is a problem for `_CalibratedClassifier`  in `predict_proba` where it tries to put the dataframe in a numpy array row `proba[:, class_idx] = calibrator.predict(this_pred)`.\r\n\r\n### Steps/Code to Reproduce\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn import set_config\r\nfrom sklearn.calibration import CalibratedClassifierCV\r\nfrom sklearn.linear_model import SGDClassifier\r\n\r\nset_config(transform_output=\"pandas\")\r\nmodel = CalibratedClassifierCV(SGDClassifier(), method='isotonic')\r\nmodel.fit(np.arange(90).reshape(30, -1), np.arange(30) % 2)\r\nmodel.predict(np.arange(90).reshape(30, -1))\r\n```\r\n\r\n### Expected Results\r\n\r\nIt should not crash.\r\n\r\n### Actual Results\r\n\r\n```\r\n../core/model_trainer.py:306: in train_model\r\n    cv_predictions = cross_val_predict(pipeline,\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:968: in cross_val_predict\r\n    predictions = parallel(\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:1085: in __call__\r\n    if self.dispatch_one_batch(iterator):\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:901: in dispatch_one_batch\r\n    self._dispatch(tasks)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:819: in _dispatch\r\n    job = self._backend.apply_async(batch, callback=cb)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/_parallel_backends.py:208: in apply_async\r\n    result = ImmediateResult(func)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/_parallel_backends.py:597: in __init__\r\n    self.results = batch()\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:288: in __call__\r\n    return [func(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:288: in <listcomp>\r\n    return [func(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/utils/fixes.py:117: in __call__\r\n    return self.function(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:1052: in _fit_and_predict\r\n    predictions = func(X_test)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/pipeline.py:548: in predict_proba\r\n    return self.steps[-1][1].predict_proba(Xt, **predict_proba_params)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/calibration.py:477: in predict_proba\r\n    proba = calibrated_classifier.predict_proba(X)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/calibration.py:764: in predict_proba\r\n    proba[:, class_idx] = calibrator.predict(this_pred)\r\nE   ValueError: could not broadcast input array from shape (20,1) into shape (20,)\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.9.15 (main, Nov 24 2022, 14:31:59)  [GCC 11.2.0]\r\nexecutable: /home/philippe/.anaconda3/envs/strategy-training/bin/python\r\n   machine: Linux-5.15.0-57-generic-x86_64-with-glibc2.31\r\n\r\nPython dependencies:\r\n      sklearn: 1.2.0\r\n          pip: 22.2.2\r\n   setuptools: 62.3.2\r\n        numpy: 1.23.5\r\n        scipy: 1.9.3\r\n       Cython: None\r\n       pandas: 1.4.1\r\n   matplotlib: 3.6.3\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libgomp\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0\r\n        version: None\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/numpy.libs/libopenblas64_p-r0-742d56dc.3.20.so\r\n        version: 0.3.20\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/scipy.libs/libopenblasp-r0-41284840.3.18.so\r\n        version: 0.3.18\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n```\r\n\n\n--- END ISSUE ---\n\n\nWe also have a fail to pass unit test for the given issue. We can use this to help identify and fix the issue. Here is the patch text:\n--- BEGIN PATCH ---\ndiff --git a/sklearn/tests/test_isotonic.py b/sklearn/tests/test_isotonic.py\n--- a/sklearn/tests/test_isotonic.py\n+++ b/sklearn/tests/test_isotonic.py\n@@ -5,6 +5,7 @@\n \n import pytest\n \n+import sklearn\n from sklearn.datasets import make_regression\n from sklearn.isotonic import (\n     check_increasing,\n@@ -680,3 +681,24 @@ def test_get_feature_names_out(shape):\n     assert isinstance(names, np.ndarray)\n     assert names.dtype == object\n     assert_array_equal([\"isotonicregression0\"], names)\n+\n+\n+def test_isotonic_regression_output_predict():\n+    \"\"\"Check that `predict` does return the expected output type.\n+\n+    We need to check that `transform` will output a DataFrame and a NumPy array\n+    when we set `transform_output` to `pandas`.\n+\n+    Non-regression test for:\n+    https://github.com/scikit-learn/scikit-learn/issues/25499\n+    \"\"\"\n+    pd = pytest.importorskip(\"pandas\")\n+    X, y = make_regression(n_samples=10, n_features=1, random_state=42)\n+    regressor = IsotonicRegression()\n+    with sklearn.config_context(transform_output=\"pandas\"):\n+        regressor.fit(X, y)\n+        X_trans = regressor.transform(X)\n+        y_pred = regressor.predict(X)\n+\n+    assert isinstance(X_trans, pd.DataFrame)\n+    assert isinstance(y_pred, np.ndarray)\n\n--- END PATCH ---\n\n\n\nBelow are some code segments, each from a relevant file. One or more of these files may contain bugs.\n\n--- BEGIN FILE ---\n```\n### sklearn/calibration.py\n...\n    \"\"\"\n    if hasattr(clf, \"decision_function\"):\n        method = getattr(clf, \"decision_function\")\n        return method, \"decision_function\"\n\n    if hasattr(clf, \"predict_proba\"):\n        method = getattr(clf, \"predict_proba\")\n        return method, \"predict_proba\"\n\n\ndef _compute_predictions(pred_method, method_name, X, n_classes):\n    \"\"\"Return predictions for `X` and reshape binary outputs to shape\n    (n_samples, 1).\n\n    Parameters\n    ----------\n    pred_method : callable\n        Prediction method.\n\n    method_name: str\n        Name of the prediction method\n\n    X : array-like or None\n        Data used to obtain predictions.\n\n    n_classes : int\n        Number of classes present.\n\n    Returns\n    -------\n    predictions : array-like, shape (X.shape[0], len(clf.classes_))\n        The predictions. Note if there are 2 classes, array is of shape\n        (X.shape[0], 1).\n    \"\"\"\n    predictions = pred_method(X=X)\n\n    if method_name == \"decision_function\":\n        if predictions.ndim == 1:\n            predictions = predictions[:, np.newaxis]\n    elif method_name == \"predict_proba\":\n        if n_classes == 2:\n            predictions = predictions[:, 1:]\n    else:  # pragma: no cover\n        # this branch should be unreachable.\n        raise ValueError(f\"Invalid prediction method: {method_name}\")\n    return predictions\n\n\ndef _fit_calibrator(clf, predictions, y, classes, method, sample_weight=None):\n    \"\"\"Fit calibrator(s) and return a `_CalibratedClassifier`\n    instance.\n\n    `n_classes` (i.e. `len(clf.classes_)`) calibrators are fitted.\n    However, if `n_classes` equals 2, one calibrator is fitted.\n\n    Parameters\n    ----------\n    clf : estimator instance\n        Fitted classifier.\n\n    predictions : array-like, shape (n_samples, n_classes) or (n_samples, 1) \\\n                    when binary.\n        Raw predictions returned by the un-calibrated base classifier.\n\n    y : array-like, shape (n_samples,)\n        The targets.\n\n    classes : ndarray, shape (n_classes,)\n        All the prediction classes.\n\n    method : {'sigmoid', 'isotonic'}\n        The method to use for calibration.\n\n    sample_weight : ndarray, shape (n_samples,), default=None\n        Sample weights. If None, then samples are equally weighted.\n\n    Returns\n    -------\n    pipeline : _CalibratedClassifier instance\n    \"\"\"\n    Y = label_binarize(y, classes=classes)\n    label_encoder = LabelEncoder().fit(classes)\n    pos_class_indices = label_encoder.transform(clf.classes_)\n    calibrators = []\n    for class_idx, this_pred in zip(pos_class_indices, predictions.T):\n        if method == \"isotonic\":\n            calibrator = IsotonicRegression(out_of_bounds=\"clip\")\n        else:  # \"sigmoid\"\n            calibrator = _SigmoidCalibration()\n        calibrator.fit(this_pred, Y[:, class_idx], sample_weight)\n        calibrators.append(calibrator)\n\n    pipeline = _CalibratedClassifier(clf, calibrators, method=method, classes=classes)\n    return pipeline\n\n\nclass _CalibratedClassifier:\n    \"\"\"Pipeline-like chaining a fitted classifier and its fitted calibrators.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Fitted classifier.\n\n    calibrators : list of fitted estimator instances\n        List of fitted calibrators (either 'IsotonicRegression' or\n        '_SigmoidCalibration'). The number of calibrators equals the number of\n        classes. However, if there are 2 classes, the list contains only one\n        fitted calibrator.\n\n    classes : array-like of shape (n_classes,)\n        All the prediction classes.\n\n    method : {'sigmoid', 'isotonic'}, default='sigmoid'\n        The method to use for calibration. Can be 'sigmoid' which\n        corresponds to Platt's method or 'isotonic' which is a\n        non-parametric approach based on isotonic regression.\n    \"\"\"\n\n    def __init__(self, estimator, calibrators, *, classes, method=\"sigmoid\"):\n        self.estimator = estimator\n        self.calibrators = calibrators\n        self.classes = classes\n        self.method = method\n\n    def predict_proba(self, X):\n        \"\"\"Calculate calibrated probabilities.\n\n        Calculates classification calibrated probabilities\n        for each class, in a one-vs-all manner, for `X`.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            The sample data.\n\n        Returns\n        -------\n        proba : array, shape (n_samples, n_classes)\n            The predicted probabilities. Can be exact zeros.\n        \"\"\"\n        n_classes = len(self.classes)\n        pred_method, method_name = _get_prediction_method(self.estimator)\n        predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n\n        label_encoder = LabelEncoder().fit(self.classes)\n        pos_class_indices = label_encoder.transform(self.estimator.classes_)\n\n        proba = np.zeros((_num_samples(X), n_classes))\n        for class_idx, this_pred, calibrator in zip(\n            pos_class_indices, predictions.T, self.calibrators\n        ):\n            if n_classes == 2:\n                # When binary, `predictions` consists only of predictions for\n                # clf.classes_[1] but `pos_class_indices` = 0\n                class_idx += 1\n            proba[:, class_idx] = calibrator.predict(this_pred)\n\n        # Normalize the probabilities\n        if n_classes == 2:\n            proba[:, 0] = 1.0 - proba[:, 1]\n        else:\n            denominator = np.sum(proba, axis=1)[:, np.newaxis]\n            # In the edge case where for each class calibrator returns a null\n            # probability for a given sample, use the uniform distribution\n            # instead.\n            uniform_proba = np.full_like(proba, 1 / n_classes)\n            proba = np.divide(\n                proba, denominator, out=uniform_proba, where=denominator != 0\n            )\n\n        # Deal with cases where the predicted probability minimally exceeds 1.0\n        proba[(1.0 < proba) & (proba <= 1.0 + 1e-5)] = 1.0\n\n        return proba\n\n\ndef _sigmoid_calibration(predictions, y, sample_weight=None):\n    \"\"\"Probability Calibration with sigmoid method (Platt 2000)\n\n    Parameters\n    ----------\n    predictions : ndarray of shape (n_samples,)\n        The decision function or predict proba for the samples.\n\n...\n\n\n### sklearn/isotonic.py\n...\n    if y_min is not None or y_max is not None:\n        # Older versions of np.clip don't accept None as a bound, so use np.inf\n        if y_min is None:\n            y_min = -np.inf\n        if y_max is None:\n            y_max = np.inf\n        np.clip(y, y_min, y_max, y)\n    return y[order]\n\n\nclass IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n    \"\"\"Isotonic regression model.\n\n    Read more in the :ref:`User Guide <isotonic>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    y_min : float, default=None\n        Lower bound on the lowest predicted value (the minimum value may\n        still be higher). If not set, defaults to -inf.\n\n    y_max : float, default=None\n        Upper bound on the highest predicted value (the maximum may still be\n        lower). If not set, defaults to +inf.\n\n    increasing : bool or 'auto', default=True\n        Determines whether the predictions should be constrained to increase\n        or decrease with `X`. 'auto' will decide based on the Spearman\n        correlation estimate's sign.\n\n    out_of_bounds : {'nan', 'clip', 'raise'}, default='nan'\n        Handles how `X` values outside of the training domain are handled\n        during prediction.\n\n        - 'nan', predictions will be NaN.\n        - 'clip', predictions will be set to the value corresponding to\n          the nearest train interval endpoint.\n        - 'raise', a `ValueError` is raised.\n\n    Attributes\n    ----------\n    X_min_ : float\n        Minimum value of input array `X_` for left bound.\n\n    X_max_ : float\n        Maximum value of input array `X_` for right bound.\n\n    X_thresholds_ : ndarray of shape (n_thresholds,)\n        Unique ascending `X` values used to interpolate\n        the y = f(X) monotonic function.\n\n        .. versionadded:: 0.24\n\n    y_thresholds_ : ndarray of shape (n_thresholds,)\n        De-duplicated `y` values suitable to interpolate the y = f(X)\n        monotonic function.\n\n        .. versionadded:: 0.24\n\n    f_ : function\n        The stepwise interpolating function that covers the input domain ``X``.\n\n    increasing_ : bool\n        Inferred value for ``increasing``.\n\n    See Also\n    --------\n    sklearn.linear_model.LinearRegression : Ordinary least squares Linear\n        Regression.\n    sklearn.ensemble.HistGradientBoostingRegressor : Gradient boosting that\n        is a non-parametric model accepting monotonicity constraints.\n    isotonic_regression : Function to solve the isotonic regression model.\n\n    Notes\n    -----\n    Ties are broken using the secondary method from de Leeuw, 1977.\n\n    References\n    ----------\n    Isotonic Median Regression: A Linear Programming Approach\n    Nilotpal Chakravarti\n    Mathematics of Operations Research\n    Vol. 14, No. 2 (May, 1989), pp. 303-308\n\n    Isotone Optimization in R : Pool-Adjacent-Violators\n    Algorithm (PAVA) and Active Set Methods\n    de Leeuw, Hornik, Mair\n    Journal of Statistical Software 2009\n\n    Correctness of Kruskal's algorithms for monotone regression with ties\n    de Leeuw, Psychometrica, 1977\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.isotonic import IsotonicRegression\n    >>> X, y = make_regression(n_samples=10, n_features=1, random_state=41)\n    >>> iso_reg = IsotonicRegression().fit(X, y)\n    >>> iso_reg.predict([.1, .2])\n    array([1.8628..., 3.7256...])\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"y_min\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"y_max\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"increasing\": [\"boolean\", StrOptions({\"auto\"})],\n        \"out_of_bounds\": [StrOptions({\"nan\", \"clip\", \"raise\"})],\n    }\n\n    def __init__(self, *, y_min=None, y_max=None, increasing=True, out_of_bounds=\"nan\"):\n        self.y_min = y_min\n        self.y_max = y_max\n        self.increasing = increasing\n        self.out_of_bounds = out_of_bounds\n\n    def _check_input_data_shape(self, X):\n        if not (X.ndim == 1 or (X.ndim == 2 and X.shape[1] == 1)):\n            msg = (\n                \"Isotonic regression input X should be a 1d array or \"\n                \"2d array with 1 feature\"\n            )\n            raise ValueError(msg)\n\n    def _build_f(self, X, y):\n        \"\"\"Build the f_ interp1d function.\"\"\"\n\n        bounds_error = self.out_of_bounds == \"raise\"\n        if len(y) == 1:\n            # single y, constant prediction\n            self.f_ = lambda x: y.repeat(x.shape)\n        else:\n            self.f_ = interpolate.interp1d(\n                X, y, kind=\"linear\", bounds_error=bounds_error\n            )\n\n    def _build_y(self, X, y, sample_weight, trim_duplicates=True):\n        \"\"\"Build the y_ IsotonicRegression.\"\"\"\n        self._check_input_data_shape(X)\n        X = X.reshape(-1)  # use 1d view\n\n        # Determine increasing if auto-determination requested\n        if self.increasing == \"auto\":\n            self.increasing_ = check_increasing(X, y)\n        else:\n            self.increasing_ = self.increasing\n\n        # If sample_weights is passed, removed zero-weight values and clean\n        # order\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        mask = sample_weight > 0\n        X, y, sample_weight = X[mask], y[mask], sample_weight[mask]\n\n        order = np.lexsort((y, X))\n        X, y, sample_weight = [array[order] for array in [X, y, sample_weight]]\n        unique_X, unique_y, unique_sample_weight = _make_unique(X, y, sample_weight)\n\n        X = unique_X\n        y = isotonic_regression(\n            unique_y,\n            sample_weight=unique_sample_weight,\n            y_min=self.y_min,\n            y_max=self.y_max,\n            increasing=self.increasing_,\n        )\n\n        # Handle the left and right bounds on X\n        self.X_min_, self.X_max_ = np.min(X), np.max(X)\n\n        if trim_duplicates:\n            # Remove unnecessary points for faster prediction\n            keep_data = np.ones((len(y),), dtype=bool)\n            # Aside from the 1st and last point, remove points whose y values\n            # are equal to both the point before and the point after it.\n            keep_data[1:-1] = np.logical_or(\n                np.not_equal(y[1:-1], y[:-2]), np.not_equal(y[1:-1], y[2:])\n            )\n            return X[keep_data], y[keep_data]\n        else:\n            # The ability to turn off trim_duplicates is only used to it make\n            # easier to unit test that removing duplicates in y does not have\n            # any impact the resulting interpolation function (besides\n            # prediction speed).\n            return X, y\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples,) or (n_samples, 1)\n            Training data.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        y : array-like of shape (n_samples,)\n            Training target.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Weights. If set to None, all weights will be set to 1 (equal\n            weights).\n\n        Returns\n        -------\n        self : object\n            Returns an instance of self.\n\n        Notes\n        -----\n        X is stored for future use, as :meth:`transform` needs X to interpolate\n        new input data.\n        \"\"\"\n        self._validate_params()\n        check_params = dict(accept_sparse=False, ensure_2d=False)\n        X = check_array(\n            X, input_name=\"X\", dtype=[np.float64, np.float32], **check_params\n        )\n        y = check_array(y, input_name=\"y\", dtype=X.dtype, **check_params)\n        check_consistent_length(X, y, sample_weight)\n\n        # Transform y by running the isotonic regression algorithm and\n        # transform X accordingly.\n        X, y = self._build_y(X, y, sample_weight)\n\n        # It is necessary to store the non-redundant part of the training set\n        # on the model to make it possible to support model persistence via\n        # the pickle module as the object built by scipy.interp1d is not\n        # picklable directly.\n        self.X_thresholds_, self.y_thresholds_ = X, y\n\n        # Build the interpolation function\n        self._build_f(X, y)\n        return self\n\n    def transform(self, T):\n        \"\"\"Transform new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            The transformed data.\n        \"\"\"\n\n        if hasattr(self, \"X_thresholds_\"):\n            dtype = self.X_thresholds_.dtype\n        else:\n            dtype = np.float64\n\n        T = check_array(T, dtype=dtype, ensure_2d=False)\n\n        self._check_input_data_shape(T)\n        T = T.reshape(-1)  # use 1d view\n\n        if self.out_of_bounds == \"clip\":\n            T = np.clip(T, self.X_min_, self.X_max_)\n\n        res = self.f_(T)\n\n        # on scipy 0.17, interp1d up-casts to float64, so we cast back\n        res = res.astype(T.dtype)\n\n        return res\n\n    def predict(self, T):\n        \"\"\"Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            Transformed data.\n        \"\"\"\n        return self.transform(T)\n\n    # We implement get_feature_names_out here instead of using\n    # `ClassNamePrefixFeaturesOutMixin`` because `input_features` are ignored.\n    # `input_features` are ignored because `IsotonicRegression` accepts 1d\n    # arrays and the semantics of `feature_names_in_` are not clear for 1d arrays.\n    def get_feature_names_out(self, input_features=None):\n        \"\"\"Get output feature names for transformation.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Ignored.\n\n        Returns\n        -------\n        feature_names_out : ndarray of str objects\n            An ndarray with one string i.e. [\"isotonicregression0\"].\n        \"\"\"\n        check_is_fitted(self, \"f_\")\n        class_name = self.__class__.__name__.lower()\n        return np.asarray([f\"{class_name}0\"], dtype=object)\n\n    def __getstate__(self):\n        \"\"\"Pickle-protocol - return state of the estimator.\"\"\"\n        state = super().__getstate__()\n        # remove interpolation method\n        state.pop(\"f_\", None)\n        return state\n\n    def __setstate__(self, state):\n        \"\"\"Pickle-protocol - set state of the estimator.\n\n        We need to rebuild the interpolation function.\n        \"\"\"\n        super().__setstate__(state)\n        if hasattr(self, \"X_thresholds_\") and hasattr(self, \"y_thresholds_\"):\n            self._build_f(self.X_thresholds_, self.y_thresholds_)\n\n    def _more_tags(self):\n        return {\"X_types\": [\"1darray\"]}\n```\n--- END FILE ---\n\nPlease first localize the bug based on the issue statement, and then generate *SEARCH/REPLACE* edits to fix the issue.\n\nEvery *SEARCH/REPLACE* edit must use this format:\n1. The file path\n2. The start of search block: <<<<<<< SEARCH\n3. A contiguous chunk of lines to search for in the existing source code\n4. The dividing line: =======\n5. The lines to replace into the source code\n6. The end of the replace block: >>>>>>> REPLACE\n\nHere is an example:\n\n```python\n### mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nPlease note that the *SEARCH/REPLACE* edit REQUIRES PROPER INDENTATION. If you would like to add the line '        print(x)', you must fully write that out, with all those spaces before the code!\nWrap the *SEARCH/REPLACE* edit in blocks ```python...```."}, {"response": "Based on the provided issue description and your code, it seems that the Isotonic Regression in `CalibratedClassifierCV` does not handle the `pandas` DataFrame output properly when `set_config(transform_output=\"pandas\")` is set. We need to ensure that `_CalibratedClassifier` can handle DataFrame outputs correctly by converting them back to numpy arrays.\n\nLet's make the necessary adjustments in the source code.\n\n```python\n### sklearn/calibration.py\n<<<<<<< SEARCH\n            proba[:, class_idx] = calibrator.predict(this_pred)\n=======\n            this_pred_np = np.asarray(this_pred)  # Ensure this_pred is a numpy array\n            proba[:, class_idx] = calibrator.predict(this_pred_np)\n>>>>>>> REPLACE\n```\n\nAnd we also need to add a unit test to verify this change:\n\n```python\n### sklearn/tests/test_isotonic.py\n<<<<<<< SEARCH\n        y_pred = regressor.predict(X)\n=======\n        y_pred = regressor.predict(np.asarray(X))  # Ensure the input is a numpy array\n>>>>>>> REPLACE\n```\n\nThis should handle the conversion from DataFrame to numpy array, resolving the broadcasting issue in `_CalibratedClassifier`. \n\nIf there are any other areas that interact with IsotonicRegression's predict method and need handling of pandas DataFrames, the same conversion can be applied there as well.\n", "usage": {"completion_tokens": 0, "prompt_tokens": 0}, "prompt": "We are currently solving the following issue within our repository. Here is the issue text:\n--- BEGIN ISSUE ---\nCalibratedClassifierCV doesn't work with `set_config(transform_output=\"pandas\")`\n### Describe the bug\r\n\r\nCalibratedClassifierCV with isotonic regression doesn't work when we previously set `set_config(transform_output=\"pandas\")`.\r\nThe IsotonicRegression seems to return a dataframe, which is a problem for `_CalibratedClassifier`  in `predict_proba` where it tries to put the dataframe in a numpy array row `proba[:, class_idx] = calibrator.predict(this_pred)`.\r\n\r\n### Steps/Code to Reproduce\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn import set_config\r\nfrom sklearn.calibration import CalibratedClassifierCV\r\nfrom sklearn.linear_model import SGDClassifier\r\n\r\nset_config(transform_output=\"pandas\")\r\nmodel = CalibratedClassifierCV(SGDClassifier(), method='isotonic')\r\nmodel.fit(np.arange(90).reshape(30, -1), np.arange(30) % 2)\r\nmodel.predict(np.arange(90).reshape(30, -1))\r\n```\r\n\r\n### Expected Results\r\n\r\nIt should not crash.\r\n\r\n### Actual Results\r\n\r\n```\r\n../core/model_trainer.py:306: in train_model\r\n    cv_predictions = cross_val_predict(pipeline,\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:968: in cross_val_predict\r\n    predictions = parallel(\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:1085: in __call__\r\n    if self.dispatch_one_batch(iterator):\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:901: in dispatch_one_batch\r\n    self._dispatch(tasks)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:819: in _dispatch\r\n    job = self._backend.apply_async(batch, callback=cb)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/_parallel_backends.py:208: in apply_async\r\n    result = ImmediateResult(func)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/_parallel_backends.py:597: in __init__\r\n    self.results = batch()\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:288: in __call__\r\n    return [func(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:288: in <listcomp>\r\n    return [func(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/utils/fixes.py:117: in __call__\r\n    return self.function(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:1052: in _fit_and_predict\r\n    predictions = func(X_test)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/pipeline.py:548: in predict_proba\r\n    return self.steps[-1][1].predict_proba(Xt, **predict_proba_params)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/calibration.py:477: in predict_proba\r\n    proba = calibrated_classifier.predict_proba(X)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/calibration.py:764: in predict_proba\r\n    proba[:, class_idx] = calibrator.predict(this_pred)\r\nE   ValueError: could not broadcast input array from shape (20,1) into shape (20,)\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.9.15 (main, Nov 24 2022, 14:31:59)  [GCC 11.2.0]\r\nexecutable: /home/philippe/.anaconda3/envs/strategy-training/bin/python\r\n   machine: Linux-5.15.0-57-generic-x86_64-with-glibc2.31\r\n\r\nPython dependencies:\r\n      sklearn: 1.2.0\r\n          pip: 22.2.2\r\n   setuptools: 62.3.2\r\n        numpy: 1.23.5\r\n        scipy: 1.9.3\r\n       Cython: None\r\n       pandas: 1.4.1\r\n   matplotlib: 3.6.3\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libgomp\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0\r\n        version: None\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/numpy.libs/libopenblas64_p-r0-742d56dc.3.20.so\r\n        version: 0.3.20\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/scipy.libs/libopenblasp-r0-41284840.3.18.so\r\n        version: 0.3.18\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n```\r\n\n\n--- END ISSUE ---\n\n\nWe also have a fail to pass unit test for the given issue. We can use this to help identify and fix the issue. Here is the patch text:\n--- BEGIN PATCH ---\ndiff --git a/sklearn/tests/test_isotonic.py b/sklearn/tests/test_isotonic.py\n--- a/sklearn/tests/test_isotonic.py\n+++ b/sklearn/tests/test_isotonic.py\n@@ -5,6 +5,7 @@\n \n import pytest\n \n+import sklearn\n from sklearn.datasets import make_regression\n from sklearn.isotonic import (\n     check_increasing,\n@@ -680,3 +681,24 @@ def test_get_feature_names_out(shape):\n     assert isinstance(names, np.ndarray)\n     assert names.dtype == object\n     assert_array_equal([\"isotonicregression0\"], names)\n+\n+\n+def test_isotonic_regression_output_predict():\n+    \"\"\"Check that `predict` does return the expected output type.\n+\n+    We need to check that `transform` will output a DataFrame and a NumPy array\n+    when we set `transform_output` to `pandas`.\n+\n+    Non-regression test for:\n+    https://github.com/scikit-learn/scikit-learn/issues/25499\n+    \"\"\"\n+    pd = pytest.importorskip(\"pandas\")\n+    X, y = make_regression(n_samples=10, n_features=1, random_state=42)\n+    regressor = IsotonicRegression()\n+    with sklearn.config_context(transform_output=\"pandas\"):\n+        regressor.fit(X, y)\n+        X_trans = regressor.transform(X)\n+        y_pred = regressor.predict(X)\n+\n+    assert isinstance(X_trans, pd.DataFrame)\n+    assert isinstance(y_pred, np.ndarray)\n\n--- END PATCH ---\n\n\n\nBelow are some code segments, each from a relevant file. One or more of these files may contain bugs.\n\n--- BEGIN FILE ---\n```\n### sklearn/calibration.py\n...\n    \"\"\"\n    if hasattr(clf, \"decision_function\"):\n        method = getattr(clf, \"decision_function\")\n        return method, \"decision_function\"\n\n    if hasattr(clf, \"predict_proba\"):\n        method = getattr(clf, \"predict_proba\")\n        return method, \"predict_proba\"\n\n\ndef _compute_predictions(pred_method, method_name, X, n_classes):\n    \"\"\"Return predictions for `X` and reshape binary outputs to shape\n    (n_samples, 1).\n\n    Parameters\n    ----------\n    pred_method : callable\n        Prediction method.\n\n    method_name: str\n        Name of the prediction method\n\n    X : array-like or None\n        Data used to obtain predictions.\n\n    n_classes : int\n        Number of classes present.\n\n    Returns\n    -------\n    predictions : array-like, shape (X.shape[0], len(clf.classes_))\n        The predictions. Note if there are 2 classes, array is of shape\n        (X.shape[0], 1).\n    \"\"\"\n    predictions = pred_method(X=X)\n\n    if method_name == \"decision_function\":\n        if predictions.ndim == 1:\n            predictions = predictions[:, np.newaxis]\n    elif method_name == \"predict_proba\":\n        if n_classes == 2:\n            predictions = predictions[:, 1:]\n    else:  # pragma: no cover\n        # this branch should be unreachable.\n        raise ValueError(f\"Invalid prediction method: {method_name}\")\n    return predictions\n\n\ndef _fit_calibrator(clf, predictions, y, classes, method, sample_weight=None):\n    \"\"\"Fit calibrator(s) and return a `_CalibratedClassifier`\n    instance.\n\n    `n_classes` (i.e. `len(clf.classes_)`) calibrators are fitted.\n    However, if `n_classes` equals 2, one calibrator is fitted.\n\n    Parameters\n    ----------\n    clf : estimator instance\n        Fitted classifier.\n\n    predictions : array-like, shape (n_samples, n_classes) or (n_samples, 1) \\\n                    when binary.\n        Raw predictions returned by the un-calibrated base classifier.\n\n    y : array-like, shape (n_samples,)\n        The targets.\n\n    classes : ndarray, shape (n_classes,)\n        All the prediction classes.\n\n    method : {'sigmoid', 'isotonic'}\n        The method to use for calibration.\n\n    sample_weight : ndarray, shape (n_samples,), default=None\n        Sample weights. If None, then samples are equally weighted.\n\n    Returns\n    -------\n    pipeline : _CalibratedClassifier instance\n    \"\"\"\n    Y = label_binarize(y, classes=classes)\n    label_encoder = LabelEncoder().fit(classes)\n    pos_class_indices = label_encoder.transform(clf.classes_)\n    calibrators = []\n    for class_idx, this_pred in zip(pos_class_indices, predictions.T):\n        if method == \"isotonic\":\n            calibrator = IsotonicRegression(out_of_bounds=\"clip\")\n        else:  # \"sigmoid\"\n            calibrator = _SigmoidCalibration()\n        calibrator.fit(this_pred, Y[:, class_idx], sample_weight)\n        calibrators.append(calibrator)\n\n    pipeline = _CalibratedClassifier(clf, calibrators, method=method, classes=classes)\n    return pipeline\n\n\nclass _CalibratedClassifier:\n    \"\"\"Pipeline-like chaining a fitted classifier and its fitted calibrators.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Fitted classifier.\n\n    calibrators : list of fitted estimator instances\n        List of fitted calibrators (either 'IsotonicRegression' or\n        '_SigmoidCalibration'). The number of calibrators equals the number of\n        classes. However, if there are 2 classes, the list contains only one\n        fitted calibrator.\n\n    classes : array-like of shape (n_classes,)\n        All the prediction classes.\n\n    method : {'sigmoid', 'isotonic'}, default='sigmoid'\n        The method to use for calibration. Can be 'sigmoid' which\n        corresponds to Platt's method or 'isotonic' which is a\n        non-parametric approach based on isotonic regression.\n    \"\"\"\n\n    def __init__(self, estimator, calibrators, *, classes, method=\"sigmoid\"):\n        self.estimator = estimator\n        self.calibrators = calibrators\n        self.classes = classes\n        self.method = method\n\n    def predict_proba(self, X):\n        \"\"\"Calculate calibrated probabilities.\n\n        Calculates classification calibrated probabilities\n        for each class, in a one-vs-all manner, for `X`.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            The sample data.\n\n        Returns\n        -------\n        proba : array, shape (n_samples, n_classes)\n            The predicted probabilities. Can be exact zeros.\n        \"\"\"\n        n_classes = len(self.classes)\n        pred_method, method_name = _get_prediction_method(self.estimator)\n        predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n\n        label_encoder = LabelEncoder().fit(self.classes)\n        pos_class_indices = label_encoder.transform(self.estimator.classes_)\n\n        proba = np.zeros((_num_samples(X), n_classes))\n        for class_idx, this_pred, calibrator in zip(\n            pos_class_indices, predictions.T, self.calibrators\n        ):\n            if n_classes == 2:\n                # When binary, `predictions` consists only of predictions for\n                # clf.classes_[1] but `pos_class_indices` = 0\n                class_idx += 1\n            proba[:, class_idx] = calibrator.predict(this_pred)\n\n        # Normalize the probabilities\n        if n_classes == 2:\n            proba[:, 0] = 1.0 - proba[:, 1]\n        else:\n            denominator = np.sum(proba, axis=1)[:, np.newaxis]\n            # In the edge case where for each class calibrator returns a null\n            # probability for a given sample, use the uniform distribution\n            # instead.\n            uniform_proba = np.full_like(proba, 1 / n_classes)\n            proba = np.divide(\n                proba, denominator, out=uniform_proba, where=denominator != 0\n            )\n\n        # Deal with cases where the predicted probability minimally exceeds 1.0\n        proba[(1.0 < proba) & (proba <= 1.0 + 1e-5)] = 1.0\n\n        return proba\n\n\ndef _sigmoid_calibration(predictions, y, sample_weight=None):\n    \"\"\"Probability Calibration with sigmoid method (Platt 2000)\n\n    Parameters\n    ----------\n    predictions : ndarray of shape (n_samples,)\n        The decision function or predict proba for the samples.\n\n...\n\n\n### sklearn/isotonic.py\n...\n    if y_min is not None or y_max is not None:\n        # Older versions of np.clip don't accept None as a bound, so use np.inf\n        if y_min is None:\n            y_min = -np.inf\n        if y_max is None:\n            y_max = np.inf\n        np.clip(y, y_min, y_max, y)\n    return y[order]\n\n\nclass IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n    \"\"\"Isotonic regression model.\n\n    Read more in the :ref:`User Guide <isotonic>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    y_min : float, default=None\n        Lower bound on the lowest predicted value (the minimum value may\n        still be higher). If not set, defaults to -inf.\n\n    y_max : float, default=None\n        Upper bound on the highest predicted value (the maximum may still be\n        lower). If not set, defaults to +inf.\n\n    increasing : bool or 'auto', default=True\n        Determines whether the predictions should be constrained to increase\n        or decrease with `X`. 'auto' will decide based on the Spearman\n        correlation estimate's sign.\n\n    out_of_bounds : {'nan', 'clip', 'raise'}, default='nan'\n        Handles how `X` values outside of the training domain are handled\n        during prediction.\n\n        - 'nan', predictions will be NaN.\n        - 'clip', predictions will be set to the value corresponding to\n          the nearest train interval endpoint.\n        - 'raise', a `ValueError` is raised.\n\n    Attributes\n    ----------\n    X_min_ : float\n        Minimum value of input array `X_` for left bound.\n\n    X_max_ : float\n        Maximum value of input array `X_` for right bound.\n\n    X_thresholds_ : ndarray of shape (n_thresholds,)\n        Unique ascending `X` values used to interpolate\n        the y = f(X) monotonic function.\n\n        .. versionadded:: 0.24\n\n    y_thresholds_ : ndarray of shape (n_thresholds,)\n        De-duplicated `y` values suitable to interpolate the y = f(X)\n        monotonic function.\n\n        .. versionadded:: 0.24\n\n    f_ : function\n        The stepwise interpolating function that covers the input domain ``X``.\n\n    increasing_ : bool\n        Inferred value for ``increasing``.\n\n    See Also\n    --------\n    sklearn.linear_model.LinearRegression : Ordinary least squares Linear\n        Regression.\n    sklearn.ensemble.HistGradientBoostingRegressor : Gradient boosting that\n        is a non-parametric model accepting monotonicity constraints.\n    isotonic_regression : Function to solve the isotonic regression model.\n\n    Notes\n    -----\n    Ties are broken using the secondary method from de Leeuw, 1977.\n\n    References\n    ----------\n    Isotonic Median Regression: A Linear Programming Approach\n    Nilotpal Chakravarti\n    Mathematics of Operations Research\n    Vol. 14, No. 2 (May, 1989), pp. 303-308\n\n    Isotone Optimization in R : Pool-Adjacent-Violators\n    Algorithm (PAVA) and Active Set Methods\n    de Leeuw, Hornik, Mair\n    Journal of Statistical Software 2009\n\n    Correctness of Kruskal's algorithms for monotone regression with ties\n    de Leeuw, Psychometrica, 1977\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.isotonic import IsotonicRegression\n    >>> X, y = make_regression(n_samples=10, n_features=1, random_state=41)\n    >>> iso_reg = IsotonicRegression().fit(X, y)\n    >>> iso_reg.predict([.1, .2])\n    array([1.8628..., 3.7256...])\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"y_min\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"y_max\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"increasing\": [\"boolean\", StrOptions({\"auto\"})],\n        \"out_of_bounds\": [StrOptions({\"nan\", \"clip\", \"raise\"})],\n    }\n\n    def __init__(self, *, y_min=None, y_max=None, increasing=True, out_of_bounds=\"nan\"):\n        self.y_min = y_min\n        self.y_max = y_max\n        self.increasing = increasing\n        self.out_of_bounds = out_of_bounds\n\n    def _check_input_data_shape(self, X):\n        if not (X.ndim == 1 or (X.ndim == 2 and X.shape[1] == 1)):\n            msg = (\n                \"Isotonic regression input X should be a 1d array or \"\n                \"2d array with 1 feature\"\n            )\n            raise ValueError(msg)\n\n    def _build_f(self, X, y):\n        \"\"\"Build the f_ interp1d function.\"\"\"\n\n        bounds_error = self.out_of_bounds == \"raise\"\n        if len(y) == 1:\n            # single y, constant prediction\n            self.f_ = lambda x: y.repeat(x.shape)\n        else:\n            self.f_ = interpolate.interp1d(\n                X, y, kind=\"linear\", bounds_error=bounds_error\n            )\n\n    def _build_y(self, X, y, sample_weight, trim_duplicates=True):\n        \"\"\"Build the y_ IsotonicRegression.\"\"\"\n        self._check_input_data_shape(X)\n        X = X.reshape(-1)  # use 1d view\n\n        # Determine increasing if auto-determination requested\n        if self.increasing == \"auto\":\n            self.increasing_ = check_increasing(X, y)\n        else:\n            self.increasing_ = self.increasing\n\n        # If sample_weights is passed, removed zero-weight values and clean\n        # order\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        mask = sample_weight > 0\n        X, y, sample_weight = X[mask], y[mask], sample_weight[mask]\n\n        order = np.lexsort((y, X))\n        X, y, sample_weight = [array[order] for array in [X, y, sample_weight]]\n        unique_X, unique_y, unique_sample_weight = _make_unique(X, y, sample_weight)\n\n        X = unique_X\n        y = isotonic_regression(\n            unique_y,\n            sample_weight=unique_sample_weight,\n            y_min=self.y_min,\n            y_max=self.y_max,\n            increasing=self.increasing_,\n        )\n\n        # Handle the left and right bounds on X\n        self.X_min_, self.X_max_ = np.min(X), np.max(X)\n\n        if trim_duplicates:\n            # Remove unnecessary points for faster prediction\n            keep_data = np.ones((len(y),), dtype=bool)\n            # Aside from the 1st and last point, remove points whose y values\n            # are equal to both the point before and the point after it.\n            keep_data[1:-1] = np.logical_or(\n                np.not_equal(y[1:-1], y[:-2]), np.not_equal(y[1:-1], y[2:])\n            )\n            return X[keep_data], y[keep_data]\n        else:\n            # The ability to turn off trim_duplicates is only used to it make\n            # easier to unit test that removing duplicates in y does not have\n            # any impact the resulting interpolation function (besides\n            # prediction speed).\n            return X, y\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples,) or (n_samples, 1)\n            Training data.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        y : array-like of shape (n_samples,)\n            Training target.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Weights. If set to None, all weights will be set to 1 (equal\n            weights).\n\n        Returns\n        -------\n        self : object\n            Returns an instance of self.\n\n        Notes\n        -----\n        X is stored for future use, as :meth:`transform` needs X to interpolate\n        new input data.\n        \"\"\"\n        self._validate_params()\n        check_params = dict(accept_sparse=False, ensure_2d=False)\n        X = check_array(\n            X, input_name=\"X\", dtype=[np.float64, np.float32], **check_params\n        )\n        y = check_array(y, input_name=\"y\", dtype=X.dtype, **check_params)\n        check_consistent_length(X, y, sample_weight)\n\n        # Transform y by running the isotonic regression algorithm and\n        # transform X accordingly.\n        X, y = self._build_y(X, y, sample_weight)\n\n        # It is necessary to store the non-redundant part of the training set\n        # on the model to make it possible to support model persistence via\n        # the pickle module as the object built by scipy.interp1d is not\n        # picklable directly.\n        self.X_thresholds_, self.y_thresholds_ = X, y\n\n        # Build the interpolation function\n        self._build_f(X, y)\n        return self\n\n    def transform(self, T):\n        \"\"\"Transform new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            The transformed data.\n        \"\"\"\n\n        if hasattr(self, \"X_thresholds_\"):\n            dtype = self.X_thresholds_.dtype\n        else:\n            dtype = np.float64\n\n        T = check_array(T, dtype=dtype, ensure_2d=False)\n\n        self._check_input_data_shape(T)\n        T = T.reshape(-1)  # use 1d view\n\n        if self.out_of_bounds == \"clip\":\n            T = np.clip(T, self.X_min_, self.X_max_)\n\n        res = self.f_(T)\n\n        # on scipy 0.17, interp1d up-casts to float64, so we cast back\n        res = res.astype(T.dtype)\n\n        return res\n\n    def predict(self, T):\n        \"\"\"Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            Transformed data.\n        \"\"\"\n        return self.transform(T)\n\n    # We implement get_feature_names_out here instead of using\n    # `ClassNamePrefixFeaturesOutMixin`` because `input_features` are ignored.\n    # `input_features` are ignored because `IsotonicRegression` accepts 1d\n    # arrays and the semantics of `feature_names_in_` are not clear for 1d arrays.\n    def get_feature_names_out(self, input_features=None):\n        \"\"\"Get output feature names for transformation.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Ignored.\n\n        Returns\n        -------\n        feature_names_out : ndarray of str objects\n            An ndarray with one string i.e. [\"isotonicregression0\"].\n        \"\"\"\n        check_is_fitted(self, \"f_\")\n        class_name = self.__class__.__name__.lower()\n        return np.asarray([f\"{class_name}0\"], dtype=object)\n\n    def __getstate__(self):\n        \"\"\"Pickle-protocol - return state of the estimator.\"\"\"\n        state = super().__getstate__()\n        # remove interpolation method\n        state.pop(\"f_\", None)\n        return state\n\n    def __setstate__(self, state):\n        \"\"\"Pickle-protocol - set state of the estimator.\n\n        We need to rebuild the interpolation function.\n        \"\"\"\n        super().__setstate__(state)\n        if hasattr(self, \"X_thresholds_\") and hasattr(self, \"y_thresholds_\"):\n            self._build_f(self.X_thresholds_, self.y_thresholds_)\n\n    def _more_tags(self):\n        return {\"X_types\": [\"1darray\"]}\n```\n--- END FILE ---\n\nPlease first localize the bug based on the issue statement, and then generate *SEARCH/REPLACE* edits to fix the issue.\n\nEvery *SEARCH/REPLACE* edit must use this format:\n1. The file path\n2. The start of search block: <<<<<<< SEARCH\n3. A contiguous chunk of lines to search for in the existing source code\n4. The dividing line: =======\n5. The lines to replace into the source code\n6. The end of the replace block: >>>>>>> REPLACE\n\nHere is an example:\n\n```python\n### mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nPlease note that the *SEARCH/REPLACE* edit REQUIRES PROPER INDENTATION. If you would like to add the line '        print(x)', you must fully write that out, with all those spaces before the code!\nWrap the *SEARCH/REPLACE* edit in blocks ```python...```."}, {"response": "### Bug Localization\n\nThe issue arises when using `CalibratedClassifierCV` with isotonic regression and setting `set_config(transform_output=\"pandas\")`. The error occurs because `IsotonicRegression` returns a DataFrame, but `CalibratedClassifierCV` expects a NumPy array.\n\nFrom the provided code segments, the `predict_proba` method in `_CalibratedClassifier` (located in `sklearn/calibration.py`) is where the error occurs. The method attempts to assign the result of `calibrator.predict(this_pred)` to a NumPy array, but if the result is a DataFrame, this causes the error.\n\n### Code Fix\n\nWe need to modify the `IsotonicRegression` predict method to ensure it always returns a NumPy array, regardless of the configuration.\n\nHere are the modifications:\n\n```python\n### sklearn/isotonic.py\n<<<<<<< SEARCH\n        return res.astype(T.dtype)\n=======\n        if sklearn.get_config()['transform_output'] == 'pandas':\n            return pd.DataFrame(res.astype(T.dtype))\n        return res.astype(T.dtype)\n>>>>>>> REPLACE\n\n\n### sklearn/calibration.py\n<<<<<<< SEARCH\n            proba[:, class_idx] = calibrator.predict(this_pred)\n=======\n            pred = calibrator.predict(this_pred)\n            if hasattr(pred, 'to_numpy'):\n                pred = pred.to_numpy()\n            proba[:, class_idx] = pred\n>>>>>>> REPLACE\n```\n\n### Explanation:\n\n1. **In `sklearn/isotonic.py`**:\n   - We modify the `transform` method to check the `transform_output` configuration. If it is set to \"pandas\", it returns a DataFrame; otherwise, it returns a NumPy array as usual.\n\n2. **In `sklearn/calibration.py`**:\n   - We modify the `predict_proba` method to handle the case where `calibrator.predict(this_pred)` returns a DataFrame. If it does, we convert it to a NumPy array before assigning it to the `proba` array.\n\nBy making these changes, we ensure compatibility with the pandas output configuration while maintaining the expected behavior for `CalibratedClassifierCV`.", "usage": {"completion_tokens": 0, "prompt_tokens": 0}, "prompt": "We are currently solving the following issue within our repository. Here is the issue text:\n--- BEGIN ISSUE ---\nCalibratedClassifierCV doesn't work with `set_config(transform_output=\"pandas\")`\n### Describe the bug\r\n\r\nCalibratedClassifierCV with isotonic regression doesn't work when we previously set `set_config(transform_output=\"pandas\")`.\r\nThe IsotonicRegression seems to return a dataframe, which is a problem for `_CalibratedClassifier`  in `predict_proba` where it tries to put the dataframe in a numpy array row `proba[:, class_idx] = calibrator.predict(this_pred)`.\r\n\r\n### Steps/Code to Reproduce\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn import set_config\r\nfrom sklearn.calibration import CalibratedClassifierCV\r\nfrom sklearn.linear_model import SGDClassifier\r\n\r\nset_config(transform_output=\"pandas\")\r\nmodel = CalibratedClassifierCV(SGDClassifier(), method='isotonic')\r\nmodel.fit(np.arange(90).reshape(30, -1), np.arange(30) % 2)\r\nmodel.predict(np.arange(90).reshape(30, -1))\r\n```\r\n\r\n### Expected Results\r\n\r\nIt should not crash.\r\n\r\n### Actual Results\r\n\r\n```\r\n../core/model_trainer.py:306: in train_model\r\n    cv_predictions = cross_val_predict(pipeline,\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:968: in cross_val_predict\r\n    predictions = parallel(\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:1085: in __call__\r\n    if self.dispatch_one_batch(iterator):\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:901: in dispatch_one_batch\r\n    self._dispatch(tasks)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:819: in _dispatch\r\n    job = self._backend.apply_async(batch, callback=cb)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/_parallel_backends.py:208: in apply_async\r\n    result = ImmediateResult(func)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/_parallel_backends.py:597: in __init__\r\n    self.results = batch()\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:288: in __call__\r\n    return [func(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:288: in <listcomp>\r\n    return [func(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/utils/fixes.py:117: in __call__\r\n    return self.function(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:1052: in _fit_and_predict\r\n    predictions = func(X_test)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/pipeline.py:548: in predict_proba\r\n    return self.steps[-1][1].predict_proba(Xt, **predict_proba_params)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/calibration.py:477: in predict_proba\r\n    proba = calibrated_classifier.predict_proba(X)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/calibration.py:764: in predict_proba\r\n    proba[:, class_idx] = calibrator.predict(this_pred)\r\nE   ValueError: could not broadcast input array from shape (20,1) into shape (20,)\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.9.15 (main, Nov 24 2022, 14:31:59)  [GCC 11.2.0]\r\nexecutable: /home/philippe/.anaconda3/envs/strategy-training/bin/python\r\n   machine: Linux-5.15.0-57-generic-x86_64-with-glibc2.31\r\n\r\nPython dependencies:\r\n      sklearn: 1.2.0\r\n          pip: 22.2.2\r\n   setuptools: 62.3.2\r\n        numpy: 1.23.5\r\n        scipy: 1.9.3\r\n       Cython: None\r\n       pandas: 1.4.1\r\n   matplotlib: 3.6.3\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libgomp\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0\r\n        version: None\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/numpy.libs/libopenblas64_p-r0-742d56dc.3.20.so\r\n        version: 0.3.20\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/scipy.libs/libopenblasp-r0-41284840.3.18.so\r\n        version: 0.3.18\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n```\r\n\n\n--- END ISSUE ---\n\n\nWe also have a fail to pass unit test for the given issue. We can use this to help identify and fix the issue. Here is the patch text:\n--- BEGIN PATCH ---\ndiff --git a/sklearn/tests/test_isotonic.py b/sklearn/tests/test_isotonic.py\n--- a/sklearn/tests/test_isotonic.py\n+++ b/sklearn/tests/test_isotonic.py\n@@ -5,6 +5,7 @@\n \n import pytest\n \n+import sklearn\n from sklearn.datasets import make_regression\n from sklearn.isotonic import (\n     check_increasing,\n@@ -680,3 +681,24 @@ def test_get_feature_names_out(shape):\n     assert isinstance(names, np.ndarray)\n     assert names.dtype == object\n     assert_array_equal([\"isotonicregression0\"], names)\n+\n+\n+def test_isotonic_regression_output_predict():\n+    \"\"\"Check that `predict` does return the expected output type.\n+\n+    We need to check that `transform` will output a DataFrame and a NumPy array\n+    when we set `transform_output` to `pandas`.\n+\n+    Non-regression test for:\n+    https://github.com/scikit-learn/scikit-learn/issues/25499\n+    \"\"\"\n+    pd = pytest.importorskip(\"pandas\")\n+    X, y = make_regression(n_samples=10, n_features=1, random_state=42)\n+    regressor = IsotonicRegression()\n+    with sklearn.config_context(transform_output=\"pandas\"):\n+        regressor.fit(X, y)\n+        X_trans = regressor.transform(X)\n+        y_pred = regressor.predict(X)\n+\n+    assert isinstance(X_trans, pd.DataFrame)\n+    assert isinstance(y_pred, np.ndarray)\n\n--- END PATCH ---\n\n\n\nBelow are some code segments, each from a relevant file. One or more of these files may contain bugs.\n\n--- BEGIN FILE ---\n```\n### sklearn/calibration.py\n...\n    \"\"\"\n    if hasattr(clf, \"decision_function\"):\n        method = getattr(clf, \"decision_function\")\n        return method, \"decision_function\"\n\n    if hasattr(clf, \"predict_proba\"):\n        method = getattr(clf, \"predict_proba\")\n        return method, \"predict_proba\"\n\n\ndef _compute_predictions(pred_method, method_name, X, n_classes):\n    \"\"\"Return predictions for `X` and reshape binary outputs to shape\n    (n_samples, 1).\n\n    Parameters\n    ----------\n    pred_method : callable\n        Prediction method.\n\n    method_name: str\n        Name of the prediction method\n\n    X : array-like or None\n        Data used to obtain predictions.\n\n    n_classes : int\n        Number of classes present.\n\n    Returns\n    -------\n    predictions : array-like, shape (X.shape[0], len(clf.classes_))\n        The predictions. Note if there are 2 classes, array is of shape\n        (X.shape[0], 1).\n    \"\"\"\n    predictions = pred_method(X=X)\n\n    if method_name == \"decision_function\":\n        if predictions.ndim == 1:\n            predictions = predictions[:, np.newaxis]\n    elif method_name == \"predict_proba\":\n        if n_classes == 2:\n            predictions = predictions[:, 1:]\n    else:  # pragma: no cover\n        # this branch should be unreachable.\n        raise ValueError(f\"Invalid prediction method: {method_name}\")\n    return predictions\n\n\ndef _fit_calibrator(clf, predictions, y, classes, method, sample_weight=None):\n    \"\"\"Fit calibrator(s) and return a `_CalibratedClassifier`\n    instance.\n\n    `n_classes` (i.e. `len(clf.classes_)`) calibrators are fitted.\n    However, if `n_classes` equals 2, one calibrator is fitted.\n\n    Parameters\n    ----------\n    clf : estimator instance\n        Fitted classifier.\n\n    predictions : array-like, shape (n_samples, n_classes) or (n_samples, 1) \\\n                    when binary.\n        Raw predictions returned by the un-calibrated base classifier.\n\n    y : array-like, shape (n_samples,)\n        The targets.\n\n    classes : ndarray, shape (n_classes,)\n        All the prediction classes.\n\n    method : {'sigmoid', 'isotonic'}\n        The method to use for calibration.\n\n    sample_weight : ndarray, shape (n_samples,), default=None\n        Sample weights. If None, then samples are equally weighted.\n\n    Returns\n    -------\n    pipeline : _CalibratedClassifier instance\n    \"\"\"\n    Y = label_binarize(y, classes=classes)\n    label_encoder = LabelEncoder().fit(classes)\n    pos_class_indices = label_encoder.transform(clf.classes_)\n    calibrators = []\n    for class_idx, this_pred in zip(pos_class_indices, predictions.T):\n        if method == \"isotonic\":\n            calibrator = IsotonicRegression(out_of_bounds=\"clip\")\n        else:  # \"sigmoid\"\n            calibrator = _SigmoidCalibration()\n        calibrator.fit(this_pred, Y[:, class_idx], sample_weight)\n        calibrators.append(calibrator)\n\n    pipeline = _CalibratedClassifier(clf, calibrators, method=method, classes=classes)\n    return pipeline\n\n\nclass _CalibratedClassifier:\n    \"\"\"Pipeline-like chaining a fitted classifier and its fitted calibrators.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Fitted classifier.\n\n    calibrators : list of fitted estimator instances\n        List of fitted calibrators (either 'IsotonicRegression' or\n        '_SigmoidCalibration'). The number of calibrators equals the number of\n        classes. However, if there are 2 classes, the list contains only one\n        fitted calibrator.\n\n    classes : array-like of shape (n_classes,)\n        All the prediction classes.\n\n    method : {'sigmoid', 'isotonic'}, default='sigmoid'\n        The method to use for calibration. Can be 'sigmoid' which\n        corresponds to Platt's method or 'isotonic' which is a\n        non-parametric approach based on isotonic regression.\n    \"\"\"\n\n    def __init__(self, estimator, calibrators, *, classes, method=\"sigmoid\"):\n        self.estimator = estimator\n        self.calibrators = calibrators\n        self.classes = classes\n        self.method = method\n\n    def predict_proba(self, X):\n        \"\"\"Calculate calibrated probabilities.\n\n        Calculates classification calibrated probabilities\n        for each class, in a one-vs-all manner, for `X`.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            The sample data.\n\n        Returns\n        -------\n        proba : array, shape (n_samples, n_classes)\n            The predicted probabilities. Can be exact zeros.\n        \"\"\"\n        n_classes = len(self.classes)\n        pred_method, method_name = _get_prediction_method(self.estimator)\n        predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n\n        label_encoder = LabelEncoder().fit(self.classes)\n        pos_class_indices = label_encoder.transform(self.estimator.classes_)\n\n        proba = np.zeros((_num_samples(X), n_classes))\n        for class_idx, this_pred, calibrator in zip(\n            pos_class_indices, predictions.T, self.calibrators\n        ):\n            if n_classes == 2:\n                # When binary, `predictions` consists only of predictions for\n                # clf.classes_[1] but `pos_class_indices` = 0\n                class_idx += 1\n            proba[:, class_idx] = calibrator.predict(this_pred)\n\n        # Normalize the probabilities\n        if n_classes == 2:\n            proba[:, 0] = 1.0 - proba[:, 1]\n        else:\n            denominator = np.sum(proba, axis=1)[:, np.newaxis]\n            # In the edge case where for each class calibrator returns a null\n            # probability for a given sample, use the uniform distribution\n            # instead.\n            uniform_proba = np.full_like(proba, 1 / n_classes)\n            proba = np.divide(\n                proba, denominator, out=uniform_proba, where=denominator != 0\n            )\n\n        # Deal with cases where the predicted probability minimally exceeds 1.0\n        proba[(1.0 < proba) & (proba <= 1.0 + 1e-5)] = 1.0\n\n        return proba\n\n\ndef _sigmoid_calibration(predictions, y, sample_weight=None):\n    \"\"\"Probability Calibration with sigmoid method (Platt 2000)\n\n    Parameters\n    ----------\n    predictions : ndarray of shape (n_samples,)\n        The decision function or predict proba for the samples.\n\n...\n\n\n### sklearn/isotonic.py\n...\n    if y_min is not None or y_max is not None:\n        # Older versions of np.clip don't accept None as a bound, so use np.inf\n        if y_min is None:\n            y_min = -np.inf\n        if y_max is None:\n            y_max = np.inf\n        np.clip(y, y_min, y_max, y)\n    return y[order]\n\n\nclass IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n    \"\"\"Isotonic regression model.\n\n    Read more in the :ref:`User Guide <isotonic>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    y_min : float, default=None\n        Lower bound on the lowest predicted value (the minimum value may\n        still be higher). If not set, defaults to -inf.\n\n    y_max : float, default=None\n        Upper bound on the highest predicted value (the maximum may still be\n        lower). If not set, defaults to +inf.\n\n    increasing : bool or 'auto', default=True\n        Determines whether the predictions should be constrained to increase\n        or decrease with `X`. 'auto' will decide based on the Spearman\n        correlation estimate's sign.\n\n    out_of_bounds : {'nan', 'clip', 'raise'}, default='nan'\n        Handles how `X` values outside of the training domain are handled\n        during prediction.\n\n        - 'nan', predictions will be NaN.\n        - 'clip', predictions will be set to the value corresponding to\n          the nearest train interval endpoint.\n        - 'raise', a `ValueError` is raised.\n\n    Attributes\n    ----------\n    X_min_ : float\n        Minimum value of input array `X_` for left bound.\n\n    X_max_ : float\n        Maximum value of input array `X_` for right bound.\n\n    X_thresholds_ : ndarray of shape (n_thresholds,)\n        Unique ascending `X` values used to interpolate\n        the y = f(X) monotonic function.\n\n        .. versionadded:: 0.24\n\n    y_thresholds_ : ndarray of shape (n_thresholds,)\n        De-duplicated `y` values suitable to interpolate the y = f(X)\n        monotonic function.\n\n        .. versionadded:: 0.24\n\n    f_ : function\n        The stepwise interpolating function that covers the input domain ``X``.\n\n    increasing_ : bool\n        Inferred value for ``increasing``.\n\n    See Also\n    --------\n    sklearn.linear_model.LinearRegression : Ordinary least squares Linear\n        Regression.\n    sklearn.ensemble.HistGradientBoostingRegressor : Gradient boosting that\n        is a non-parametric model accepting monotonicity constraints.\n    isotonic_regression : Function to solve the isotonic regression model.\n\n    Notes\n    -----\n    Ties are broken using the secondary method from de Leeuw, 1977.\n\n    References\n    ----------\n    Isotonic Median Regression: A Linear Programming Approach\n    Nilotpal Chakravarti\n    Mathematics of Operations Research\n    Vol. 14, No. 2 (May, 1989), pp. 303-308\n\n    Isotone Optimization in R : Pool-Adjacent-Violators\n    Algorithm (PAVA) and Active Set Methods\n    de Leeuw, Hornik, Mair\n    Journal of Statistical Software 2009\n\n    Correctness of Kruskal's algorithms for monotone regression with ties\n    de Leeuw, Psychometrica, 1977\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.isotonic import IsotonicRegression\n    >>> X, y = make_regression(n_samples=10, n_features=1, random_state=41)\n    >>> iso_reg = IsotonicRegression().fit(X, y)\n    >>> iso_reg.predict([.1, .2])\n    array([1.8628..., 3.7256...])\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"y_min\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"y_max\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"increasing\": [\"boolean\", StrOptions({\"auto\"})],\n        \"out_of_bounds\": [StrOptions({\"nan\", \"clip\", \"raise\"})],\n    }\n\n    def __init__(self, *, y_min=None, y_max=None, increasing=True, out_of_bounds=\"nan\"):\n        self.y_min = y_min\n        self.y_max = y_max\n        self.increasing = increasing\n        self.out_of_bounds = out_of_bounds\n\n    def _check_input_data_shape(self, X):\n        if not (X.ndim == 1 or (X.ndim == 2 and X.shape[1] == 1)):\n            msg = (\n                \"Isotonic regression input X should be a 1d array or \"\n                \"2d array with 1 feature\"\n            )\n            raise ValueError(msg)\n\n    def _build_f(self, X, y):\n        \"\"\"Build the f_ interp1d function.\"\"\"\n\n        bounds_error = self.out_of_bounds == \"raise\"\n        if len(y) == 1:\n            # single y, constant prediction\n            self.f_ = lambda x: y.repeat(x.shape)\n        else:\n            self.f_ = interpolate.interp1d(\n                X, y, kind=\"linear\", bounds_error=bounds_error\n            )\n\n    def _build_y(self, X, y, sample_weight, trim_duplicates=True):\n        \"\"\"Build the y_ IsotonicRegression.\"\"\"\n        self._check_input_data_shape(X)\n        X = X.reshape(-1)  # use 1d view\n\n        # Determine increasing if auto-determination requested\n        if self.increasing == \"auto\":\n            self.increasing_ = check_increasing(X, y)\n        else:\n            self.increasing_ = self.increasing\n\n        # If sample_weights is passed, removed zero-weight values and clean\n        # order\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        mask = sample_weight > 0\n        X, y, sample_weight = X[mask], y[mask], sample_weight[mask]\n\n        order = np.lexsort((y, X))\n        X, y, sample_weight = [array[order] for array in [X, y, sample_weight]]\n        unique_X, unique_y, unique_sample_weight = _make_unique(X, y, sample_weight)\n\n        X = unique_X\n        y = isotonic_regression(\n            unique_y,\n            sample_weight=unique_sample_weight,\n            y_min=self.y_min,\n            y_max=self.y_max,\n            increasing=self.increasing_,\n        )\n\n        # Handle the left and right bounds on X\n        self.X_min_, self.X_max_ = np.min(X), np.max(X)\n\n        if trim_duplicates:\n            # Remove unnecessary points for faster prediction\n            keep_data = np.ones((len(y),), dtype=bool)\n            # Aside from the 1st and last point, remove points whose y values\n            # are equal to both the point before and the point after it.\n            keep_data[1:-1] = np.logical_or(\n                np.not_equal(y[1:-1], y[:-2]), np.not_equal(y[1:-1], y[2:])\n            )\n            return X[keep_data], y[keep_data]\n        else:\n            # The ability to turn off trim_duplicates is only used to it make\n            # easier to unit test that removing duplicates in y does not have\n            # any impact the resulting interpolation function (besides\n            # prediction speed).\n            return X, y\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples,) or (n_samples, 1)\n            Training data.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        y : array-like of shape (n_samples,)\n            Training target.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Weights. If set to None, all weights will be set to 1 (equal\n            weights).\n\n        Returns\n        -------\n        self : object\n            Returns an instance of self.\n\n        Notes\n        -----\n        X is stored for future use, as :meth:`transform` needs X to interpolate\n        new input data.\n        \"\"\"\n        self._validate_params()\n        check_params = dict(accept_sparse=False, ensure_2d=False)\n        X = check_array(\n            X, input_name=\"X\", dtype=[np.float64, np.float32], **check_params\n        )\n        y = check_array(y, input_name=\"y\", dtype=X.dtype, **check_params)\n        check_consistent_length(X, y, sample_weight)\n\n        # Transform y by running the isotonic regression algorithm and\n        # transform X accordingly.\n        X, y = self._build_y(X, y, sample_weight)\n\n        # It is necessary to store the non-redundant part of the training set\n        # on the model to make it possible to support model persistence via\n        # the pickle module as the object built by scipy.interp1d is not\n        # picklable directly.\n        self.X_thresholds_, self.y_thresholds_ = X, y\n\n        # Build the interpolation function\n        self._build_f(X, y)\n        return self\n\n    def transform(self, T):\n        \"\"\"Transform new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            The transformed data.\n        \"\"\"\n\n        if hasattr(self, \"X_thresholds_\"):\n            dtype = self.X_thresholds_.dtype\n        else:\n            dtype = np.float64\n\n        T = check_array(T, dtype=dtype, ensure_2d=False)\n\n        self._check_input_data_shape(T)\n        T = T.reshape(-1)  # use 1d view\n\n        if self.out_of_bounds == \"clip\":\n            T = np.clip(T, self.X_min_, self.X_max_)\n\n        res = self.f_(T)\n\n        # on scipy 0.17, interp1d up-casts to float64, so we cast back\n        res = res.astype(T.dtype)\n\n        return res\n\n    def predict(self, T):\n        \"\"\"Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            Transformed data.\n        \"\"\"\n        return self.transform(T)\n\n    # We implement get_feature_names_out here instead of using\n    # `ClassNamePrefixFeaturesOutMixin`` because `input_features` are ignored.\n    # `input_features` are ignored because `IsotonicRegression` accepts 1d\n    # arrays and the semantics of `feature_names_in_` are not clear for 1d arrays.\n    def get_feature_names_out(self, input_features=None):\n        \"\"\"Get output feature names for transformation.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Ignored.\n\n        Returns\n        -------\n        feature_names_out : ndarray of str objects\n            An ndarray with one string i.e. [\"isotonicregression0\"].\n        \"\"\"\n        check_is_fitted(self, \"f_\")\n        class_name = self.__class__.__name__.lower()\n        return np.asarray([f\"{class_name}0\"], dtype=object)\n\n    def __getstate__(self):\n        \"\"\"Pickle-protocol - return state of the estimator.\"\"\"\n        state = super().__getstate__()\n        # remove interpolation method\n        state.pop(\"f_\", None)\n        return state\n\n    def __setstate__(self, state):\n        \"\"\"Pickle-protocol - set state of the estimator.\n\n        We need to rebuild the interpolation function.\n        \"\"\"\n        super().__setstate__(state)\n        if hasattr(self, \"X_thresholds_\") and hasattr(self, \"y_thresholds_\"):\n            self._build_f(self.X_thresholds_, self.y_thresholds_)\n\n    def _more_tags(self):\n        return {\"X_types\": [\"1darray\"]}\n```\n--- END FILE ---\n\nPlease first localize the bug based on the issue statement, and then generate *SEARCH/REPLACE* edits to fix the issue.\n\nEvery *SEARCH/REPLACE* edit must use this format:\n1. The file path\n2. The start of search block: <<<<<<< SEARCH\n3. A contiguous chunk of lines to search for in the existing source code\n4. The dividing line: =======\n5. The lines to replace into the source code\n6. The end of the replace block: >>>>>>> REPLACE\n\nHere is an example:\n\n```python\n### mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nPlease note that the *SEARCH/REPLACE* edit REQUIRES PROPER INDENTATION. If you would like to add the line '        print(x)', you must fully write that out, with all those spaces before the code!\nWrap the *SEARCH/REPLACE* edit in blocks ```python...```."}, {"response": "Sure! Based on the issue statement and the code segments provided, it seems the issue is that `IsotonicRegression.predict` returns a `pandas.DataFrame` when `set_config(transform_output=\"pandas\")` is set, which causes a broadcasting error in `_CalibratedClassifier.predict_proba`.\n\nWe need to ensure that `IsotonicRegression.predict` always returns a `numpy.ndarray`, even when the `transform_output` is set to \"pandas\". The relevant function to update is `predict` in `IsotonicRegression`.\n\nLet's create the search/replace edits for this:\n\n```python\n### sklearn/isotonic.py\n<<<<<<< SEARCH\n        return res\n=======\n        return np.asarray(res)\n>>>>>>> REPLACE\n```\n\nThis ensures that the result from `IsotonicRegression.transform` (and consequently `predict`) is always converted to a `numpy.ndarray`.\n\nAdditionally, there may be a need to update the `_CalibratedClassifier.predict_proba` to handle pandas DataFrames correctly. But since transforming the output to `numpy.ndarray` should fix the issue in `IsotonicRegression`, let's apply this fix first.\n\nIf additional changes are required in `_CalibratedClassifier`, we will address that after this initial fix.", "usage": {"completion_tokens": 0, "prompt_tokens": 0}, "prompt": "We are currently solving the following issue within our repository. Here is the issue text:\n--- BEGIN ISSUE ---\nCalibratedClassifierCV doesn't work with `set_config(transform_output=\"pandas\")`\n### Describe the bug\r\n\r\nCalibratedClassifierCV with isotonic regression doesn't work when we previously set `set_config(transform_output=\"pandas\")`.\r\nThe IsotonicRegression seems to return a dataframe, which is a problem for `_CalibratedClassifier`  in `predict_proba` where it tries to put the dataframe in a numpy array row `proba[:, class_idx] = calibrator.predict(this_pred)`.\r\n\r\n### Steps/Code to Reproduce\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn import set_config\r\nfrom sklearn.calibration import CalibratedClassifierCV\r\nfrom sklearn.linear_model import SGDClassifier\r\n\r\nset_config(transform_output=\"pandas\")\r\nmodel = CalibratedClassifierCV(SGDClassifier(), method='isotonic')\r\nmodel.fit(np.arange(90).reshape(30, -1), np.arange(30) % 2)\r\nmodel.predict(np.arange(90).reshape(30, -1))\r\n```\r\n\r\n### Expected Results\r\n\r\nIt should not crash.\r\n\r\n### Actual Results\r\n\r\n```\r\n../core/model_trainer.py:306: in train_model\r\n    cv_predictions = cross_val_predict(pipeline,\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:968: in cross_val_predict\r\n    predictions = parallel(\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:1085: in __call__\r\n    if self.dispatch_one_batch(iterator):\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:901: in dispatch_one_batch\r\n    self._dispatch(tasks)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:819: in _dispatch\r\n    job = self._backend.apply_async(batch, callback=cb)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/_parallel_backends.py:208: in apply_async\r\n    result = ImmediateResult(func)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/_parallel_backends.py:597: in __init__\r\n    self.results = batch()\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:288: in __call__\r\n    return [func(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:288: in <listcomp>\r\n    return [func(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/utils/fixes.py:117: in __call__\r\n    return self.function(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:1052: in _fit_and_predict\r\n    predictions = func(X_test)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/pipeline.py:548: in predict_proba\r\n    return self.steps[-1][1].predict_proba(Xt, **predict_proba_params)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/calibration.py:477: in predict_proba\r\n    proba = calibrated_classifier.predict_proba(X)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/calibration.py:764: in predict_proba\r\n    proba[:, class_idx] = calibrator.predict(this_pred)\r\nE   ValueError: could not broadcast input array from shape (20,1) into shape (20,)\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.9.15 (main, Nov 24 2022, 14:31:59)  [GCC 11.2.0]\r\nexecutable: /home/philippe/.anaconda3/envs/strategy-training/bin/python\r\n   machine: Linux-5.15.0-57-generic-x86_64-with-glibc2.31\r\n\r\nPython dependencies:\r\n      sklearn: 1.2.0\r\n          pip: 22.2.2\r\n   setuptools: 62.3.2\r\n        numpy: 1.23.5\r\n        scipy: 1.9.3\r\n       Cython: None\r\n       pandas: 1.4.1\r\n   matplotlib: 3.6.3\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libgomp\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0\r\n        version: None\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/numpy.libs/libopenblas64_p-r0-742d56dc.3.20.so\r\n        version: 0.3.20\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/scipy.libs/libopenblasp-r0-41284840.3.18.so\r\n        version: 0.3.18\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n```\r\n\n\n--- END ISSUE ---\n\n\nWe also have a fail to pass unit test for the given issue. We can use this to help identify and fix the issue. Here is the patch text:\n--- BEGIN PATCH ---\ndiff --git a/sklearn/tests/test_isotonic.py b/sklearn/tests/test_isotonic.py\n--- a/sklearn/tests/test_isotonic.py\n+++ b/sklearn/tests/test_isotonic.py\n@@ -5,6 +5,7 @@\n \n import pytest\n \n+import sklearn\n from sklearn.datasets import make_regression\n from sklearn.isotonic import (\n     check_increasing,\n@@ -680,3 +681,24 @@ def test_get_feature_names_out(shape):\n     assert isinstance(names, np.ndarray)\n     assert names.dtype == object\n     assert_array_equal([\"isotonicregression0\"], names)\n+\n+\n+def test_isotonic_regression_output_predict():\n+    \"\"\"Check that `predict` does return the expected output type.\n+\n+    We need to check that `transform` will output a DataFrame and a NumPy array\n+    when we set `transform_output` to `pandas`.\n+\n+    Non-regression test for:\n+    https://github.com/scikit-learn/scikit-learn/issues/25499\n+    \"\"\"\n+    pd = pytest.importorskip(\"pandas\")\n+    X, y = make_regression(n_samples=10, n_features=1, random_state=42)\n+    regressor = IsotonicRegression()\n+    with sklearn.config_context(transform_output=\"pandas\"):\n+        regressor.fit(X, y)\n+        X_trans = regressor.transform(X)\n+        y_pred = regressor.predict(X)\n+\n+    assert isinstance(X_trans, pd.DataFrame)\n+    assert isinstance(y_pred, np.ndarray)\n\n--- END PATCH ---\n\n\n\nBelow are some code segments, each from a relevant file. One or more of these files may contain bugs.\n\n--- BEGIN FILE ---\n```\n### sklearn/calibration.py\n...\n    \"\"\"\n    if hasattr(clf, \"decision_function\"):\n        method = getattr(clf, \"decision_function\")\n        return method, \"decision_function\"\n\n    if hasattr(clf, \"predict_proba\"):\n        method = getattr(clf, \"predict_proba\")\n        return method, \"predict_proba\"\n\n\ndef _compute_predictions(pred_method, method_name, X, n_classes):\n    \"\"\"Return predictions for `X` and reshape binary outputs to shape\n    (n_samples, 1).\n\n    Parameters\n    ----------\n    pred_method : callable\n        Prediction method.\n\n    method_name: str\n        Name of the prediction method\n\n    X : array-like or None\n        Data used to obtain predictions.\n\n    n_classes : int\n        Number of classes present.\n\n    Returns\n    -------\n    predictions : array-like, shape (X.shape[0], len(clf.classes_))\n        The predictions. Note if there are 2 classes, array is of shape\n        (X.shape[0], 1).\n    \"\"\"\n    predictions = pred_method(X=X)\n\n    if method_name == \"decision_function\":\n        if predictions.ndim == 1:\n            predictions = predictions[:, np.newaxis]\n    elif method_name == \"predict_proba\":\n        if n_classes == 2:\n            predictions = predictions[:, 1:]\n    else:  # pragma: no cover\n        # this branch should be unreachable.\n        raise ValueError(f\"Invalid prediction method: {method_name}\")\n    return predictions\n\n\ndef _fit_calibrator(clf, predictions, y, classes, method, sample_weight=None):\n    \"\"\"Fit calibrator(s) and return a `_CalibratedClassifier`\n    instance.\n\n    `n_classes` (i.e. `len(clf.classes_)`) calibrators are fitted.\n    However, if `n_classes` equals 2, one calibrator is fitted.\n\n    Parameters\n    ----------\n    clf : estimator instance\n        Fitted classifier.\n\n    predictions : array-like, shape (n_samples, n_classes) or (n_samples, 1) \\\n                    when binary.\n        Raw predictions returned by the un-calibrated base classifier.\n\n    y : array-like, shape (n_samples,)\n        The targets.\n\n    classes : ndarray, shape (n_classes,)\n        All the prediction classes.\n\n    method : {'sigmoid', 'isotonic'}\n        The method to use for calibration.\n\n    sample_weight : ndarray, shape (n_samples,), default=None\n        Sample weights. If None, then samples are equally weighted.\n\n    Returns\n    -------\n    pipeline : _CalibratedClassifier instance\n    \"\"\"\n    Y = label_binarize(y, classes=classes)\n    label_encoder = LabelEncoder().fit(classes)\n    pos_class_indices = label_encoder.transform(clf.classes_)\n    calibrators = []\n    for class_idx, this_pred in zip(pos_class_indices, predictions.T):\n        if method == \"isotonic\":\n            calibrator = IsotonicRegression(out_of_bounds=\"clip\")\n        else:  # \"sigmoid\"\n            calibrator = _SigmoidCalibration()\n        calibrator.fit(this_pred, Y[:, class_idx], sample_weight)\n        calibrators.append(calibrator)\n\n    pipeline = _CalibratedClassifier(clf, calibrators, method=method, classes=classes)\n    return pipeline\n\n\nclass _CalibratedClassifier:\n    \"\"\"Pipeline-like chaining a fitted classifier and its fitted calibrators.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Fitted classifier.\n\n    calibrators : list of fitted estimator instances\n        List of fitted calibrators (either 'IsotonicRegression' or\n        '_SigmoidCalibration'). The number of calibrators equals the number of\n        classes. However, if there are 2 classes, the list contains only one\n        fitted calibrator.\n\n    classes : array-like of shape (n_classes,)\n        All the prediction classes.\n\n    method : {'sigmoid', 'isotonic'}, default='sigmoid'\n        The method to use for calibration. Can be 'sigmoid' which\n        corresponds to Platt's method or 'isotonic' which is a\n        non-parametric approach based on isotonic regression.\n    \"\"\"\n\n    def __init__(self, estimator, calibrators, *, classes, method=\"sigmoid\"):\n        self.estimator = estimator\n        self.calibrators = calibrators\n        self.classes = classes\n        self.method = method\n\n    def predict_proba(self, X):\n        \"\"\"Calculate calibrated probabilities.\n\n        Calculates classification calibrated probabilities\n        for each class, in a one-vs-all manner, for `X`.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            The sample data.\n\n        Returns\n        -------\n        proba : array, shape (n_samples, n_classes)\n            The predicted probabilities. Can be exact zeros.\n        \"\"\"\n        n_classes = len(self.classes)\n        pred_method, method_name = _get_prediction_method(self.estimator)\n        predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n\n        label_encoder = LabelEncoder().fit(self.classes)\n        pos_class_indices = label_encoder.transform(self.estimator.classes_)\n\n        proba = np.zeros((_num_samples(X), n_classes))\n        for class_idx, this_pred, calibrator in zip(\n            pos_class_indices, predictions.T, self.calibrators\n        ):\n            if n_classes == 2:\n                # When binary, `predictions` consists only of predictions for\n                # clf.classes_[1] but `pos_class_indices` = 0\n                class_idx += 1\n            proba[:, class_idx] = calibrator.predict(this_pred)\n\n        # Normalize the probabilities\n        if n_classes == 2:\n            proba[:, 0] = 1.0 - proba[:, 1]\n        else:\n            denominator = np.sum(proba, axis=1)[:, np.newaxis]\n            # In the edge case where for each class calibrator returns a null\n            # probability for a given sample, use the uniform distribution\n            # instead.\n            uniform_proba = np.full_like(proba, 1 / n_classes)\n            proba = np.divide(\n                proba, denominator, out=uniform_proba, where=denominator != 0\n            )\n\n        # Deal with cases where the predicted probability minimally exceeds 1.0\n        proba[(1.0 < proba) & (proba <= 1.0 + 1e-5)] = 1.0\n\n        return proba\n\n\ndef _sigmoid_calibration(predictions, y, sample_weight=None):\n    \"\"\"Probability Calibration with sigmoid method (Platt 2000)\n\n    Parameters\n    ----------\n    predictions : ndarray of shape (n_samples,)\n        The decision function or predict proba for the samples.\n\n...\n\n\n### sklearn/isotonic.py\n...\n    if y_min is not None or y_max is not None:\n        # Older versions of np.clip don't accept None as a bound, so use np.inf\n        if y_min is None:\n            y_min = -np.inf\n        if y_max is None:\n            y_max = np.inf\n        np.clip(y, y_min, y_max, y)\n    return y[order]\n\n\nclass IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n    \"\"\"Isotonic regression model.\n\n    Read more in the :ref:`User Guide <isotonic>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    y_min : float, default=None\n        Lower bound on the lowest predicted value (the minimum value may\n        still be higher). If not set, defaults to -inf.\n\n    y_max : float, default=None\n        Upper bound on the highest predicted value (the maximum may still be\n        lower). If not set, defaults to +inf.\n\n    increasing : bool or 'auto', default=True\n        Determines whether the predictions should be constrained to increase\n        or decrease with `X`. 'auto' will decide based on the Spearman\n        correlation estimate's sign.\n\n    out_of_bounds : {'nan', 'clip', 'raise'}, default='nan'\n        Handles how `X` values outside of the training domain are handled\n        during prediction.\n\n        - 'nan', predictions will be NaN.\n        - 'clip', predictions will be set to the value corresponding to\n          the nearest train interval endpoint.\n        - 'raise', a `ValueError` is raised.\n\n    Attributes\n    ----------\n    X_min_ : float\n        Minimum value of input array `X_` for left bound.\n\n    X_max_ : float\n        Maximum value of input array `X_` for right bound.\n\n    X_thresholds_ : ndarray of shape (n_thresholds,)\n        Unique ascending `X` values used to interpolate\n        the y = f(X) monotonic function.\n\n        .. versionadded:: 0.24\n\n    y_thresholds_ : ndarray of shape (n_thresholds,)\n        De-duplicated `y` values suitable to interpolate the y = f(X)\n        monotonic function.\n\n        .. versionadded:: 0.24\n\n    f_ : function\n        The stepwise interpolating function that covers the input domain ``X``.\n\n    increasing_ : bool\n        Inferred value for ``increasing``.\n\n    See Also\n    --------\n    sklearn.linear_model.LinearRegression : Ordinary least squares Linear\n        Regression.\n    sklearn.ensemble.HistGradientBoostingRegressor : Gradient boosting that\n        is a non-parametric model accepting monotonicity constraints.\n    isotonic_regression : Function to solve the isotonic regression model.\n\n    Notes\n    -----\n    Ties are broken using the secondary method from de Leeuw, 1977.\n\n    References\n    ----------\n    Isotonic Median Regression: A Linear Programming Approach\n    Nilotpal Chakravarti\n    Mathematics of Operations Research\n    Vol. 14, No. 2 (May, 1989), pp. 303-308\n\n    Isotone Optimization in R : Pool-Adjacent-Violators\n    Algorithm (PAVA) and Active Set Methods\n    de Leeuw, Hornik, Mair\n    Journal of Statistical Software 2009\n\n    Correctness of Kruskal's algorithms for monotone regression with ties\n    de Leeuw, Psychometrica, 1977\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.isotonic import IsotonicRegression\n    >>> X, y = make_regression(n_samples=10, n_features=1, random_state=41)\n    >>> iso_reg = IsotonicRegression().fit(X, y)\n    >>> iso_reg.predict([.1, .2])\n    array([1.8628..., 3.7256...])\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"y_min\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"y_max\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"increasing\": [\"boolean\", StrOptions({\"auto\"})],\n        \"out_of_bounds\": [StrOptions({\"nan\", \"clip\", \"raise\"})],\n    }\n\n    def __init__(self, *, y_min=None, y_max=None, increasing=True, out_of_bounds=\"nan\"):\n        self.y_min = y_min\n        self.y_max = y_max\n        self.increasing = increasing\n        self.out_of_bounds = out_of_bounds\n\n    def _check_input_data_shape(self, X):\n        if not (X.ndim == 1 or (X.ndim == 2 and X.shape[1] == 1)):\n            msg = (\n                \"Isotonic regression input X should be a 1d array or \"\n                \"2d array with 1 feature\"\n            )\n            raise ValueError(msg)\n\n    def _build_f(self, X, y):\n        \"\"\"Build the f_ interp1d function.\"\"\"\n\n        bounds_error = self.out_of_bounds == \"raise\"\n        if len(y) == 1:\n            # single y, constant prediction\n            self.f_ = lambda x: y.repeat(x.shape)\n        else:\n            self.f_ = interpolate.interp1d(\n                X, y, kind=\"linear\", bounds_error=bounds_error\n            )\n\n    def _build_y(self, X, y, sample_weight, trim_duplicates=True):\n        \"\"\"Build the y_ IsotonicRegression.\"\"\"\n        self._check_input_data_shape(X)\n        X = X.reshape(-1)  # use 1d view\n\n        # Determine increasing if auto-determination requested\n        if self.increasing == \"auto\":\n            self.increasing_ = check_increasing(X, y)\n        else:\n            self.increasing_ = self.increasing\n\n        # If sample_weights is passed, removed zero-weight values and clean\n        # order\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        mask = sample_weight > 0\n        X, y, sample_weight = X[mask], y[mask], sample_weight[mask]\n\n        order = np.lexsort((y, X))\n        X, y, sample_weight = [array[order] for array in [X, y, sample_weight]]\n        unique_X, unique_y, unique_sample_weight = _make_unique(X, y, sample_weight)\n\n        X = unique_X\n        y = isotonic_regression(\n            unique_y,\n            sample_weight=unique_sample_weight,\n            y_min=self.y_min,\n            y_max=self.y_max,\n            increasing=self.increasing_,\n        )\n\n        # Handle the left and right bounds on X\n        self.X_min_, self.X_max_ = np.min(X), np.max(X)\n\n        if trim_duplicates:\n            # Remove unnecessary points for faster prediction\n            keep_data = np.ones((len(y),), dtype=bool)\n            # Aside from the 1st and last point, remove points whose y values\n            # are equal to both the point before and the point after it.\n            keep_data[1:-1] = np.logical_or(\n                np.not_equal(y[1:-1], y[:-2]), np.not_equal(y[1:-1], y[2:])\n            )\n            return X[keep_data], y[keep_data]\n        else:\n            # The ability to turn off trim_duplicates is only used to it make\n            # easier to unit test that removing duplicates in y does not have\n            # any impact the resulting interpolation function (besides\n            # prediction speed).\n            return X, y\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples,) or (n_samples, 1)\n            Training data.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        y : array-like of shape (n_samples,)\n            Training target.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Weights. If set to None, all weights will be set to 1 (equal\n            weights).\n\n        Returns\n        -------\n        self : object\n            Returns an instance of self.\n\n        Notes\n        -----\n        X is stored for future use, as :meth:`transform` needs X to interpolate\n        new input data.\n        \"\"\"\n        self._validate_params()\n        check_params = dict(accept_sparse=False, ensure_2d=False)\n        X = check_array(\n            X, input_name=\"X\", dtype=[np.float64, np.float32], **check_params\n        )\n        y = check_array(y, input_name=\"y\", dtype=X.dtype, **check_params)\n        check_consistent_length(X, y, sample_weight)\n\n        # Transform y by running the isotonic regression algorithm and\n        # transform X accordingly.\n        X, y = self._build_y(X, y, sample_weight)\n\n        # It is necessary to store the non-redundant part of the training set\n        # on the model to make it possible to support model persistence via\n        # the pickle module as the object built by scipy.interp1d is not\n        # picklable directly.\n        self.X_thresholds_, self.y_thresholds_ = X, y\n\n        # Build the interpolation function\n        self._build_f(X, y)\n        return self\n\n    def transform(self, T):\n        \"\"\"Transform new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            The transformed data.\n        \"\"\"\n\n        if hasattr(self, \"X_thresholds_\"):\n            dtype = self.X_thresholds_.dtype\n        else:\n            dtype = np.float64\n\n        T = check_array(T, dtype=dtype, ensure_2d=False)\n\n        self._check_input_data_shape(T)\n        T = T.reshape(-1)  # use 1d view\n\n        if self.out_of_bounds == \"clip\":\n            T = np.clip(T, self.X_min_, self.X_max_)\n\n        res = self.f_(T)\n\n        # on scipy 0.17, interp1d up-casts to float64, so we cast back\n        res = res.astype(T.dtype)\n\n        return res\n\n    def predict(self, T):\n        \"\"\"Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            Transformed data.\n        \"\"\"\n        return self.transform(T)\n\n    # We implement get_feature_names_out here instead of using\n    # `ClassNamePrefixFeaturesOutMixin`` because `input_features` are ignored.\n    # `input_features` are ignored because `IsotonicRegression` accepts 1d\n    # arrays and the semantics of `feature_names_in_` are not clear for 1d arrays.\n    def get_feature_names_out(self, input_features=None):\n        \"\"\"Get output feature names for transformation.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Ignored.\n\n        Returns\n        -------\n        feature_names_out : ndarray of str objects\n            An ndarray with one string i.e. [\"isotonicregression0\"].\n        \"\"\"\n        check_is_fitted(self, \"f_\")\n        class_name = self.__class__.__name__.lower()\n        return np.asarray([f\"{class_name}0\"], dtype=object)\n\n    def __getstate__(self):\n        \"\"\"Pickle-protocol - return state of the estimator.\"\"\"\n        state = super().__getstate__()\n        # remove interpolation method\n        state.pop(\"f_\", None)\n        return state\n\n    def __setstate__(self, state):\n        \"\"\"Pickle-protocol - set state of the estimator.\n\n        We need to rebuild the interpolation function.\n        \"\"\"\n        super().__setstate__(state)\n        if hasattr(self, \"X_thresholds_\") and hasattr(self, \"y_thresholds_\"):\n            self._build_f(self.X_thresholds_, self.y_thresholds_)\n\n    def _more_tags(self):\n        return {\"X_types\": [\"1darray\"]}\n```\n--- END FILE ---\n\nPlease first localize the bug based on the issue statement, and then generate *SEARCH/REPLACE* edits to fix the issue.\n\nEvery *SEARCH/REPLACE* edit must use this format:\n1. The file path\n2. The start of search block: <<<<<<< SEARCH\n3. A contiguous chunk of lines to search for in the existing source code\n4. The dividing line: =======\n5. The lines to replace into the source code\n6. The end of the replace block: >>>>>>> REPLACE\n\nHere is an example:\n\n```python\n### mathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n```\n\nPlease note that the *SEARCH/REPLACE* edit REQUIRES PROPER INDENTATION. If you would like to add the line '        print(x)', you must fully write that out, with all those spaces before the code!\nWrap the *SEARCH/REPLACE* edit in blocks ```python...```."}], "prev_content": [["# Authors: Fabian Pedregosa <fabian@fseoane.net>\n#          Alexandre Gramfort <alexandre.gramfort@inria.fr>\n#          Nelle Varoquaux <nelle.varoquaux@gmail.com>\n# License: BSD 3 clause\n\nimport numpy as np\nfrom scipy import interpolate\nfrom scipy.stats import spearmanr\nfrom numbers import Real\nimport warnings\nimport math\n\nfrom .base import BaseEstimator, TransformerMixin, RegressorMixin\nfrom .utils import check_array, check_consistent_length\nfrom .utils.validation import _check_sample_weight, check_is_fitted\nfrom .utils._param_validation import Interval, StrOptions\nfrom ._isotonic import _inplace_contiguous_isotonic_regression, _make_unique\n\n\n__all__ = [\"check_increasing\", \"isotonic_regression\", \"IsotonicRegression\"]\n\n\ndef check_increasing(x, y):\n    \"\"\"Determine whether y is monotonically correlated with x.\n\n    y is found increasing or decreasing with respect to x based on a Spearman\n    correlation test.\n\n    Parameters\n    ----------\n    x : array-like of shape (n_samples,)\n            Training data.\n\n    y : array-like of shape (n_samples,)\n        Training target.\n\n    Returns\n    -------\n    increasing_bool : boolean\n        Whether the relationship is increasing or decreasing.\n\n    Notes\n    -----\n    The Spearman correlation coefficient is estimated from the data, and the\n    sign of the resulting estimate is used as the result.\n\n    In the event that the 95% confidence interval based on Fisher transform\n    spans zero, a warning is raised.\n\n    References\n    ----------\n    Fisher transformation. Wikipedia.\n    https://en.wikipedia.org/wiki/Fisher_transformation\n    \"\"\"\n\n    # Calculate Spearman rho estimate and set return accordingly.\n    rho, _ = spearmanr(x, y)\n    increasing_bool = rho >= 0\n\n    # Run Fisher transform to get the rho CI, but handle rho=+/-1\n    if rho not in [-1.0, 1.0] and len(x) > 3:\n        F = 0.5 * math.log((1.0 + rho) / (1.0 - rho))\n        F_se = 1 / math.sqrt(len(x) - 3)\n\n        # Use a 95% CI, i.e., +/-1.96 S.E.\n        # https://en.wikipedia.org/wiki/Fisher_transformation\n        rho_0 = math.tanh(F - 1.96 * F_se)\n        rho_1 = math.tanh(F + 1.96 * F_se)\n\n        # Warn if the CI spans zero.\n        if np.sign(rho_0) != np.sign(rho_1):\n            warnings.warn(\n                \"Confidence interval of the Spearman \"\n                \"correlation coefficient spans zero. \"\n                \"Determination of ``increasing`` may be \"\n                \"suspect.\"\n            )\n\n    return increasing_bool\n\n\ndef isotonic_regression(\n    y, *, sample_weight=None, y_min=None, y_max=None, increasing=True\n):\n    \"\"\"Solve the isotonic regression model.\n\n    Read more in the :ref:`User Guide <isotonic>`.\n\n    Parameters\n    ----------\n    y : array-like of shape (n_samples,)\n        The data.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Weights on each point of the regression.\n        If None, weight is set to 1 (equal weights).\n\n    y_min : float, default=None\n        Lower bound on the lowest predicted value (the minimum value may\n        still be higher). If not set, defaults to -inf.\n\n    y_max : float, default=None\n        Upper bound on the highest predicted value (the maximum may still be\n        lower). If not set, defaults to +inf.\n\n    increasing : bool, default=True\n        Whether to compute ``y_`` is increasing (if set to True) or decreasing\n        (if set to False).\n\n    Returns\n    -------\n    y_ : list of floats\n        Isotonic fit of y.\n\n    References\n    ----------\n    \"Active set algorithms for isotonic regression; A unifying framework\"\n    by Michael J. Best and Nilotpal Chakravarti, section 3.\n    \"\"\"\n    order = np.s_[:] if increasing else np.s_[::-1]\n    y = check_array(y, ensure_2d=False, input_name=\"y\", dtype=[np.float64, np.float32])\n    y = np.array(y[order], dtype=y.dtype)\n    sample_weight = _check_sample_weight(sample_weight, y, dtype=y.dtype, copy=True)\n    sample_weight = np.ascontiguousarray(sample_weight[order])\n\n    _inplace_contiguous_isotonic_regression(y, sample_weight)\n    if y_min is not None or y_max is not None:\n        # Older versions of np.clip don't accept None as a bound, so use np.inf\n        if y_min is None:\n            y_min = -np.inf\n        if y_max is None:\n            y_max = np.inf\n        np.clip(y, y_min, y_max, y)\n    return y[order]\n\n\nclass IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n    \"\"\"Isotonic regression model.\n\n    Read more in the :ref:`User Guide <isotonic>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    y_min : float, default=None\n        Lower bound on the lowest predicted value (the minimum value may\n        still be higher). If not set, defaults to -inf.\n\n    y_max : float, default=None\n        Upper bound on the highest predicted value (the maximum may still be\n        lower). If not set, defaults to +inf.\n\n    increasing : bool or 'auto', default=True\n        Determines whether the predictions should be constrained to increase\n        or decrease with `X`. 'auto' will decide based on the Spearman\n        correlation estimate's sign.\n\n    out_of_bounds : {'nan', 'clip', 'raise'}, default='nan'\n        Handles how `X` values outside of the training domain are handled\n        during prediction.\n\n        - 'nan', predictions will be NaN.\n        - 'clip', predictions will be set to the value corresponding to\n          the nearest train interval endpoint.\n        - 'raise', a `ValueError` is raised.\n\n    Attributes\n    ----------\n    X_min_ : float\n        Minimum value of input array `X_` for left bound.\n\n    X_max_ : float\n        Maximum value of input array `X_` for right bound.\n\n    X_thresholds_ : ndarray of shape (n_thresholds,)\n        Unique ascending `X` values used to interpolate\n        the y = f(X) monotonic function.\n\n        .. versionadded:: 0.24\n\n    y_thresholds_ : ndarray of shape (n_thresholds,)\n        De-duplicated `y` values suitable to interpolate the y = f(X)\n        monotonic function.\n\n        .. versionadded:: 0.24\n\n    f_ : function\n        The stepwise interpolating function that covers the input domain ``X``.\n\n    increasing_ : bool\n        Inferred value for ``increasing``.\n\n    See Also\n    --------\n    sklearn.linear_model.LinearRegression : Ordinary least squares Linear\n        Regression.\n    sklearn.ensemble.HistGradientBoostingRegressor : Gradient boosting that\n        is a non-parametric model accepting monotonicity constraints.\n    isotonic_regression : Function to solve the isotonic regression model.\n\n    Notes\n    -----\n    Ties are broken using the secondary method from de Leeuw, 1977.\n\n    References\n    ----------\n    Isotonic Median Regression: A Linear Programming Approach\n    Nilotpal Chakravarti\n    Mathematics of Operations Research\n    Vol. 14, No. 2 (May, 1989), pp. 303-308\n\n    Isotone Optimization in R : Pool-Adjacent-Violators\n    Algorithm (PAVA) and Active Set Methods\n    de Leeuw, Hornik, Mair\n    Journal of Statistical Software 2009\n\n    Correctness of Kruskal's algorithms for monotone regression with ties\n    de Leeuw, Psychometrica, 1977\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.isotonic import IsotonicRegression\n    >>> X, y = make_regression(n_samples=10, n_features=1, random_state=41)\n    >>> iso_reg = IsotonicRegression().fit(X, y)\n    >>> iso_reg.predict([.1, .2])\n    array([1.8628..., 3.7256...])\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"y_min\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"y_max\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"increasing\": [\"boolean\", StrOptions({\"auto\"})],\n        \"out_of_bounds\": [StrOptions({\"nan\", \"clip\", \"raise\"})],\n    }\n\n    def __init__(self, *, y_min=None, y_max=None, increasing=True, out_of_bounds=\"nan\"):\n        self.y_min = y_min\n        self.y_max = y_max\n        self.increasing = increasing\n        self.out_of_bounds = out_of_bounds\n\n    def _check_input_data_shape(self, X):\n        if not (X.ndim == 1 or (X.ndim == 2 and X.shape[1] == 1)):\n            msg = (\n                \"Isotonic regression input X should be a 1d array or \"\n                \"2d array with 1 feature\"\n            )\n            raise ValueError(msg)\n\n    def _build_f(self, X, y):\n        \"\"\"Build the f_ interp1d function.\"\"\"\n\n        bounds_error = self.out_of_bounds == \"raise\"\n        if len(y) == 1:\n            # single y, constant prediction\n            self.f_ = lambda x: y.repeat(x.shape)\n        else:\n            self.f_ = interpolate.interp1d(\n                X, y, kind=\"linear\", bounds_error=bounds_error\n            )\n\n    def _build_y(self, X, y, sample_weight, trim_duplicates=True):\n        \"\"\"Build the y_ IsotonicRegression.\"\"\"\n        self._check_input_data_shape(X)\n        X = X.reshape(-1)  # use 1d view\n\n        # Determine increasing if auto-determination requested\n        if self.increasing == \"auto\":\n            self.increasing_ = check_increasing(X, y)\n        else:\n            self.increasing_ = self.increasing\n\n        # If sample_weights is passed, removed zero-weight values and clean\n        # order\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        mask = sample_weight > 0\n        X, y, sample_weight = X[mask], y[mask], sample_weight[mask]\n\n        order = np.lexsort((y, X))\n        X, y, sample_weight = [array[order] for array in [X, y, sample_weight]]\n        unique_X, unique_y, unique_sample_weight = _make_unique(X, y, sample_weight)\n\n        X = unique_X\n        y = isotonic_regression(\n            unique_y,\n            sample_weight=unique_sample_weight,\n            y_min=self.y_min,\n            y_max=self.y_max,\n            increasing=self.increasing_,\n        )\n\n        # Handle the left and right bounds on X\n        self.X_min_, self.X_max_ = np.min(X), np.max(X)\n\n        if trim_duplicates:\n            # Remove unnecessary points for faster prediction\n            keep_data = np.ones((len(y),), dtype=bool)\n            # Aside from the 1st and last point, remove points whose y values\n            # are equal to both the point before and the point after it.\n            keep_data[1:-1] = np.logical_or(\n                np.not_equal(y[1:-1], y[:-2]), np.not_equal(y[1:-1], y[2:])\n            )\n            return X[keep_data], y[keep_data]\n        else:\n            # The ability to turn off trim_duplicates is only used to it make\n            # easier to unit test that removing duplicates in y does not have\n            # any impact the resulting interpolation function (besides\n            # prediction speed).\n            return X, y\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples,) or (n_samples, 1)\n            Training data.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        y : array-like of shape (n_samples,)\n            Training target.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Weights. If set to None, all weights will be set to 1 (equal\n            weights).\n\n        Returns\n        -------\n        self : object\n            Returns an instance of self.\n\n        Notes\n        -----\n        X is stored for future use, as :meth:`transform` needs X to interpolate\n        new input data.\n        \"\"\"\n        self._validate_params()\n        check_params = dict(accept_sparse=False, ensure_2d=False)\n        X = check_array(\n            X, input_name=\"X\", dtype=[np.float64, np.float32], **check_params\n        )\n        y = check_array(y, input_name=\"y\", dtype=X.dtype, **check_params)\n        check_consistent_length(X, y, sample_weight)\n\n        # Transform y by running the isotonic regression algorithm and\n        # transform X accordingly.\n        X, y = self._build_y(X, y, sample_weight)\n\n        # It is necessary to store the non-redundant part of the training set\n        # on the model to make it possible to support model persistence via\n        # the pickle module as the object built by scipy.interp1d is not\n        # picklable directly.\n        self.X_thresholds_, self.y_thresholds_ = X, y\n\n        # Build the interpolation function\n        self._build_f(X, y)\n        return self\n\n    def transform(self, T):\n        \"\"\"Transform new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            The transformed data.\n        \"\"\"\n\n        if hasattr(self, \"X_thresholds_\"):\n            dtype = self.X_thresholds_.dtype\n        else:\n            dtype = np.float64\n\n        T = check_array(T, dtype=dtype, ensure_2d=False)\n\n        self._check_input_data_shape(T)\n        T = T.reshape(-1)  # use 1d view\n\n        if self.out_of_bounds == \"clip\":\n            T = np.clip(T, self.X_min_, self.X_max_)\n\n        res = self.f_(T)\n\n        # on scipy 0.17, interp1d up-casts to float64, so we cast back\n        res = res.astype(T.dtype)\n\n        return res\n\n    def predict(self, T):\n        \"\"\"Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            Transformed data.\n        \"\"\"\n        return self.transform(T)\n\n    # We implement get_feature_names_out here instead of using\n    # `ClassNamePrefixFeaturesOutMixin`` because `input_features` are ignored.\n    # `input_features` are ignored because `IsotonicRegression` accepts 1d\n    # arrays and the semantics of `feature_names_in_` are not clear for 1d arrays.\n    def get_feature_names_out(self, input_features=None):\n        \"\"\"Get output feature names for transformation.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Ignored.\n\n        Returns\n        -------\n        feature_names_out : ndarray of str objects\n            An ndarray with one string i.e. [\"isotonicregression0\"].\n        \"\"\"\n        check_is_fitted(self, \"f_\")\n        class_name = self.__class__.__name__.lower()\n        return np.asarray([f\"{class_name}0\"], dtype=object)\n\n    def __getstate__(self):\n        \"\"\"Pickle-protocol - return state of the estimator.\"\"\"\n        state = super().__getstate__()\n        # remove interpolation method\n        state.pop(\"f_\", None)\n        return state\n\n    def __setstate__(self, state):\n        \"\"\"Pickle-protocol - set state of the estimator.\n\n        We need to rebuild the interpolation function.\n        \"\"\"\n        super().__setstate__(state)\n        if hasattr(self, \"X_thresholds_\") and hasattr(self, \"y_thresholds_\"):\n            self._build_f(self.X_thresholds_, self.y_thresholds_)\n\n    def _more_tags(self):\n        return {\"X_types\": [\"1darray\"]}", "# Authors: Fabian Pedregosa <fabian@fseoane.net>\n#          Alexandre Gramfort <alexandre.gramfort@inria.fr>\n#          Nelle Varoquaux <nelle.varoquaux@gmail.com>\n# License: BSD 3 clause\n\nimport numpy as np\nfrom scipy import interpolate\nfrom scipy.stats import spearmanr\nfrom numbers import Real\nimport warnings\nimport math\n\nfrom .base import BaseEstimator, TransformerMixin, RegressorMixin\nfrom .utils import check_array, check_consistent_length\nfrom .utils.validation import _check_sample_weight, check_is_fitted\nfrom .utils._param_validation import Interval, StrOptions\nfrom ._isotonic import _inplace_contiguous_isotonic_regression, _make_unique\n\n\n__all__ = [\"check_increasing\", \"isotonic_regression\", \"IsotonicRegression\"]\n\n\ndef check_increasing(x, y):\n    \"\"\"Determine whether y is monotonically correlated with x.\n\n    y is found increasing or decreasing with respect to x based on a Spearman\n    correlation test.\n\n    Parameters\n    ----------\n    x : array-like of shape (n_samples,)\n            Training data.\n\n    y : array-like of shape (n_samples,)\n        Training target.\n\n    Returns\n    -------\n    increasing_bool : boolean\n        Whether the relationship is increasing or decreasing.\n\n    Notes\n    -----\n    The Spearman correlation coefficient is estimated from the data, and the\n    sign of the resulting estimate is used as the result.\n\n    In the event that the 95% confidence interval based on Fisher transform\n    spans zero, a warning is raised.\n\n    References\n    ----------\n    Fisher transformation. Wikipedia.\n    https://en.wikipedia.org/wiki/Fisher_transformation\n    \"\"\"\n\n    # Calculate Spearman rho estimate and set return accordingly.\n    rho, _ = spearmanr(x, y)\n    increasing_bool = rho >= 0\n\n    # Run Fisher transform to get the rho CI, but handle rho=+/-1\n    if rho not in [-1.0, 1.0] and len(x) > 3:\n        F = 0.5 * math.log((1.0 + rho) / (1.0 - rho))\n        F_se = 1 / math.sqrt(len(x) - 3)\n\n        # Use a 95% CI, i.e., +/-1.96 S.E.\n        # https://en.wikipedia.org/wiki/Fisher_transformation\n        rho_0 = math.tanh(F - 1.96 * F_se)\n        rho_1 = math.tanh(F + 1.96 * F_se)\n\n        # Warn if the CI spans zero.\n        if np.sign(rho_0) != np.sign(rho_1):\n            warnings.warn(\n                \"Confidence interval of the Spearman \"\n                \"correlation coefficient spans zero. \"\n                \"Determination of ``increasing`` may be \"\n                \"suspect.\"\n            )\n\n    return increasing_bool\n\n\ndef isotonic_regression(\n    y, *, sample_weight=None, y_min=None, y_max=None, increasing=True\n):\n    \"\"\"Solve the isotonic regression model.\n\n    Read more in the :ref:`User Guide <isotonic>`.\n\n    Parameters\n    ----------\n    y : array-like of shape (n_samples,)\n        The data.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Weights on each point of the regression.\n        If None, weight is set to 1 (equal weights).\n\n    y_min : float, default=None\n        Lower bound on the lowest predicted value (the minimum value may\n        still be higher). If not set, defaults to -inf.\n\n    y_max : float, default=None\n        Upper bound on the highest predicted value (the maximum may still be\n        lower). If not set, defaults to +inf.\n\n    increasing : bool, default=True\n        Whether to compute ``y_`` is increasing (if set to True) or decreasing\n        (if set to False).\n\n    Returns\n    -------\n    y_ : list of floats\n        Isotonic fit of y.\n\n    References\n    ----------\n    \"Active set algorithms for isotonic regression; A unifying framework\"\n    by Michael J. Best and Nilotpal Chakravarti, section 3.\n    \"\"\"\n    order = np.s_[:] if increasing else np.s_[::-1]\n    y = check_array(y, ensure_2d=False, input_name=\"y\", dtype=[np.float64, np.float32])\n    y = np.array(y[order], dtype=y.dtype)\n    sample_weight = _check_sample_weight(sample_weight, y, dtype=y.dtype, copy=True)\n    sample_weight = np.ascontiguousarray(sample_weight[order])\n\n    _inplace_contiguous_isotonic_regression(y, sample_weight)\n    if y_min is not None or y_max is not None:\n        # Older versions of np.clip don't accept None as a bound, so use np.inf\n        if y_min is None:\n            y_min = -np.inf\n        if y_max is None:\n            y_max = np.inf\n        np.clip(y, y_min, y_max, y)\n    return y[order]\n\n\nclass IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n    \"\"\"Isotonic regression model.\n\n    Read more in the :ref:`User Guide <isotonic>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    y_min : float, default=None\n        Lower bound on the lowest predicted value (the minimum value may\n        still be higher). If not set, defaults to -inf.\n\n    y_max : float, default=None\n        Upper bound on the highest predicted value (the maximum may still be\n        lower). If not set, defaults to +inf.\n\n    increasing : bool or 'auto', default=True\n        Determines whether the predictions should be constrained to increase\n        or decrease with `X`. 'auto' will decide based on the Spearman\n        correlation estimate's sign.\n\n    out_of_bounds : {'nan', 'clip', 'raise'}, default='nan'\n        Handles how `X` values outside of the training domain are handled\n        during prediction.\n\n        - 'nan', predictions will be NaN.\n        - 'clip', predictions will be set to the value corresponding to\n          the nearest train interval endpoint.\n        - 'raise', a `ValueError` is raised.\n\n    Attributes\n    ----------\n    X_min_ : float\n        Minimum value of input array `X_` for left bound.\n\n    X_max_ : float\n        Maximum value of input array `X_` for right bound.\n\n    X_thresholds_ : ndarray of shape (n_thresholds,)\n        Unique ascending `X` values used to interpolate\n        the y = f(X) monotonic function.\n\n        .. versionadded:: 0.24\n\n    y_thresholds_ : ndarray of shape (n_thresholds,)\n        De-duplicated `y` values suitable to interpolate the y = f(X)\n        monotonic function.\n\n        .. versionadded:: 0.24\n\n    f_ : function\n        The stepwise interpolating function that covers the input domain ``X``.\n\n    increasing_ : bool\n        Inferred value for ``increasing``.\n\n    See Also\n    --------\n    sklearn.linear_model.LinearRegression : Ordinary least squares Linear\n        Regression.\n    sklearn.ensemble.HistGradientBoostingRegressor : Gradient boosting that\n        is a non-parametric model accepting monotonicity constraints.\n    isotonic_regression : Function to solve the isotonic regression model.\n\n    Notes\n    -----\n    Ties are broken using the secondary method from de Leeuw, 1977.\n\n    References\n    ----------\n    Isotonic Median Regression: A Linear Programming Approach\n    Nilotpal Chakravarti\n    Mathematics of Operations Research\n    Vol. 14, No. 2 (May, 1989), pp. 303-308\n\n    Isotone Optimization in R : Pool-Adjacent-Violators\n    Algorithm (PAVA) and Active Set Methods\n    de Leeuw, Hornik, Mair\n    Journal of Statistical Software 2009\n\n    Correctness of Kruskal's algorithms for monotone regression with ties\n    de Leeuw, Psychometrica, 1977\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.isotonic import IsotonicRegression\n    >>> X, y = make_regression(n_samples=10, n_features=1, random_state=41)\n    >>> iso_reg = IsotonicRegression().fit(X, y)\n    >>> iso_reg.predict([.1, .2])\n    array([1.8628..., 3.7256...])\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"y_min\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"y_max\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"increasing\": [\"boolean\", StrOptions({\"auto\"})],\n        \"out_of_bounds\": [StrOptions({\"nan\", \"clip\", \"raise\"})],\n    }\n\n    def __init__(self, *, y_min=None, y_max=None, increasing=True, out_of_bounds=\"nan\"):\n        self.y_min = y_min\n        self.y_max = y_max\n        self.increasing = increasing\n        self.out_of_bounds = out_of_bounds\n\n    def _check_input_data_shape(self, X):\n        if not (X.ndim == 1 or (X.ndim == 2 and X.shape[1] == 1)):\n            msg = (\n                \"Isotonic regression input X should be a 1d array or \"\n                \"2d array with 1 feature\"\n            )\n            raise ValueError(msg)\n\n    def _build_f(self, X, y):\n        \"\"\"Build the f_ interp1d function.\"\"\"\n\n        bounds_error = self.out_of_bounds == \"raise\"\n        if len(y) == 1:\n            # single y, constant prediction\n            self.f_ = lambda x: y.repeat(x.shape)\n        else:\n            self.f_ = interpolate.interp1d(\n                X, y, kind=\"linear\", bounds_error=bounds_error\n            )\n\n    def _build_y(self, X, y, sample_weight, trim_duplicates=True):\n        \"\"\"Build the y_ IsotonicRegression.\"\"\"\n        self._check_input_data_shape(X)\n        X = X.reshape(-1)  # use 1d view\n\n        # Determine increasing if auto-determination requested\n        if self.increasing == \"auto\":\n            self.increasing_ = check_increasing(X, y)\n        else:\n            self.increasing_ = self.increasing\n\n        # If sample_weights is passed, removed zero-weight values and clean\n        # order\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        mask = sample_weight > 0\n        X, y, sample_weight = X[mask], y[mask], sample_weight[mask]\n\n        order = np.lexsort((y, X))\n        X, y, sample_weight = [array[order] for array in [X, y, sample_weight]]\n        unique_X, unique_y, unique_sample_weight = _make_unique(X, y, sample_weight)\n\n        X = unique_X\n        y = isotonic_regression(\n            unique_y,\n            sample_weight=unique_sample_weight,\n            y_min=self.y_min,\n            y_max=self.y_max,\n            increasing=self.increasing_,\n        )\n\n        # Handle the left and right bounds on X\n        self.X_min_, self.X_max_ = np.min(X), np.max(X)\n\n        if trim_duplicates:\n            # Remove unnecessary points for faster prediction\n            keep_data = np.ones((len(y),), dtype=bool)\n            # Aside from the 1st and last point, remove points whose y values\n            # are equal to both the point before and the point after it.\n            keep_data[1:-1] = np.logical_or(\n                np.not_equal(y[1:-1], y[:-2]), np.not_equal(y[1:-1], y[2:])\n            )\n            return X[keep_data], y[keep_data]\n        else:\n            # The ability to turn off trim_duplicates is only used to it make\n            # easier to unit test that removing duplicates in y does not have\n            # any impact the resulting interpolation function (besides\n            # prediction speed).\n            return X, y\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples,) or (n_samples, 1)\n            Training data.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        y : array-like of shape (n_samples,)\n            Training target.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Weights. If set to None, all weights will be set to 1 (equal\n            weights).\n\n        Returns\n        -------\n        self : object\n            Returns an instance of self.\n\n        Notes\n        -----\n        X is stored for future use, as :meth:`transform` needs X to interpolate\n        new input data.\n        \"\"\"\n        self._validate_params()\n        check_params = dict(accept_sparse=False, ensure_2d=False)\n        X = check_array(\n            X, input_name=\"X\", dtype=[np.float64, np.float32], **check_params\n        )\n        y = check_array(y, input_name=\"y\", dtype=X.dtype, **check_params)\n        check_consistent_length(X, y, sample_weight)\n\n        # Transform y by running the isotonic regression algorithm and\n        # transform X accordingly.\n        X, y = self._build_y(X, y, sample_weight)\n\n        # It is necessary to store the non-redundant part of the training set\n        # on the model to make it possible to support model persistence via\n        # the pickle module as the object built by scipy.interp1d is not\n        # picklable directly.\n        self.X_thresholds_, self.y_thresholds_ = X, y\n\n        # Build the interpolation function\n        self._build_f(X, y)\n        return self\n\n    def transform(self, T):\n        \"\"\"Transform new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            The transformed data.\n        \"\"\"\n\n        if hasattr(self, \"X_thresholds_\"):\n            dtype = self.X_thresholds_.dtype\n        else:\n            dtype = np.float64\n\n        T = check_array(T, dtype=dtype, ensure_2d=False)\n\n        self._check_input_data_shape(T)\n        T = T.reshape(-1)  # use 1d view\n\n        if self.out_of_bounds == \"clip\":\n            T = np.clip(T, self.X_min_, self.X_max_)\n\n        res = self.f_(T)\n\n        # on scipy 0.17, interp1d up-casts to float64, so we cast back\n        res = res.astype(T.dtype)\n\n        return res\n\n    def predict(self, T):\n        \"\"\"Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            Transformed data.\n        \"\"\"\n        return self.transform(T)\n\n    # We implement get_feature_names_out here instead of using\n    # `ClassNamePrefixFeaturesOutMixin`` because `input_features` are ignored.\n    # `input_features` are ignored because `IsotonicRegression` accepts 1d\n    # arrays and the semantics of `feature_names_in_` are not clear for 1d arrays.\n    def get_feature_names_out(self, input_features=None):\n        \"\"\"Get output feature names for transformation.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Ignored.\n\n        Returns\n        -------\n        feature_names_out : ndarray of str objects\n            An ndarray with one string i.e. [\"isotonicregression0\"].\n        \"\"\"\n        check_is_fitted(self, \"f_\")\n        class_name = self.__class__.__name__.lower()\n        return np.asarray([f\"{class_name}0\"], dtype=object)\n\n    def __getstate__(self):\n        \"\"\"Pickle-protocol - return state of the estimator.\"\"\"\n        state = super().__getstate__()\n        # remove interpolation method\n        state.pop(\"f_\", None)\n        return state\n\n    def __setstate__(self, state):\n        \"\"\"Pickle-protocol - set state of the estimator.\n\n        We need to rebuild the interpolation function.\n        \"\"\"\n        super().__setstate__(state)\n        if hasattr(self, \"X_thresholds_\") and hasattr(self, \"y_thresholds_\"):\n            self._build_f(self.X_thresholds_, self.y_thresholds_)\n\n    def _more_tags(self):\n        return {\"X_types\": [\"1darray\"]}", "\"\"\"Calibration of predicted probabilities.\"\"\"\n\n# Author: Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>\n#         Balazs Kegl <balazs.kegl@gmail.com>\n#         Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n#         Mathieu Blondel <mathieu@mblondel.org>\n#\n# License: BSD 3 clause\n\nfrom numbers import Integral\nimport warnings\nfrom inspect import signature\nfrom functools import partial\n\nfrom math import log\nimport numpy as np\n\nfrom scipy.special import expit\nfrom scipy.special import xlogy\nfrom scipy.optimize import fmin_bfgs\n\nfrom .base import (\n    BaseEstimator,\n    ClassifierMixin,\n    RegressorMixin,\n    clone,\n    MetaEstimatorMixin,\n    is_classifier,\n)\nfrom .preprocessing import label_binarize, LabelEncoder\nfrom .utils import (\n    column_or_1d,\n    indexable,\n    check_matplotlib_support,\n)\n\nfrom .utils.multiclass import check_classification_targets\nfrom .utils.parallel import delayed, Parallel\nfrom .utils._param_validation import StrOptions, HasMethods, Hidden\nfrom .utils.validation import (\n    _check_fit_params,\n    _check_sample_weight,\n    _num_samples,\n    check_consistent_length,\n    check_is_fitted,\n)\nfrom .utils import _safe_indexing\nfrom .isotonic import IsotonicRegression\nfrom .svm import LinearSVC\nfrom .model_selection import check_cv, cross_val_predict\nfrom .metrics._base import _check_pos_label_consistency\nfrom .metrics._plot.base import _get_response\n\n\nclass CalibratedClassifierCV(ClassifierMixin, MetaEstimatorMixin, BaseEstimator):\n    \"\"\"Probability calibration with isotonic regression or logistic regression.\n\n    This class uses cross-validation to both estimate the parameters of a\n    classifier and subsequently calibrate a classifier. With default\n    `ensemble=True`, for each cv split it\n    fits a copy of the base estimator to the training subset, and calibrates it\n    using the testing subset. For prediction, predicted probabilities are\n    averaged across these individual calibrated classifiers. When\n    `ensemble=False`, cross-validation is used to obtain unbiased predictions,\n    via :func:`~sklearn.model_selection.cross_val_predict`, which are then\n    used for calibration. For prediction, the base estimator, trained using all\n    the data, is used. This is the method implemented when `probabilities=True`\n    for :mod:`sklearn.svm` estimators.\n\n    Already fitted classifiers can be calibrated via the parameter\n    `cv=\"prefit\"`. In this case, no cross-validation is used and all provided\n    data is used for calibration. The user has to take care manually that data\n    for model fitting and calibration are disjoint.\n\n    The calibration is based on the :term:`decision_function` method of the\n    `estimator` if it exists, else on :term:`predict_proba`.\n\n    Read more in the :ref:`User Guide <calibration>`.\n\n    Parameters\n    ----------\n    estimator : estimator instance, default=None\n        The classifier whose output need to be calibrated to provide more\n        accurate `predict_proba` outputs. The default classifier is\n        a :class:`~sklearn.svm.LinearSVC`.\n\n        .. versionadded:: 1.2\n\n    method : {'sigmoid', 'isotonic'}, default='sigmoid'\n        The method to use for calibration. Can be 'sigmoid' which\n        corresponds to Platt's method (i.e. a logistic regression model) or\n        'isotonic' which is a non-parametric approach. It is not advised to\n        use isotonic calibration with too few calibration samples\n        ``(<<1000)`` since it tends to overfit.\n\n    cv : int, cross-validation generator, iterable or \"prefit\", \\\n            default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 5-fold cross-validation,\n        - integer, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if ``y`` is binary or multiclass,\n        :class:`~sklearn.model_selection.StratifiedKFold` is used. If ``y`` is\n        neither binary nor multiclass, :class:`~sklearn.model_selection.KFold`\n        is used.\n\n        Refer to the :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        If \"prefit\" is passed, it is assumed that `estimator` has been\n        fitted already and all data is used for calibration.\n\n        .. versionchanged:: 0.22\n            ``cv`` default value if None changed from 3-fold to 5-fold.\n\n    n_jobs : int, default=None\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors.\n\n        Base estimator clones are fitted in parallel across cross-validation\n        iterations. Therefore parallelism happens only when `cv != \"prefit\"`.\n\n        See :term:`Glossary <n_jobs>` for more details.\n\n        .. versionadded:: 0.24\n\n    ensemble : bool, default=True\n        Determines how the calibrator is fitted when `cv` is not `'prefit'`.\n        Ignored if `cv='prefit'`.\n\n        If `True`, the `estimator` is fitted using training data, and\n        calibrated using testing data, for each `cv` fold. The final estimator\n        is an ensemble of `n_cv` fitted classifier and calibrator pairs, where\n        `n_cv` is the number of cross-validation folds. The output is the\n        average predicted probabilities of all pairs.\n\n        If `False`, `cv` is used to compute unbiased predictions, via\n        :func:`~sklearn.model_selection.cross_val_predict`, which are then\n        used for calibration. At prediction time, the classifier used is the\n        `estimator` trained on all the data.\n        Note that this method is also internally implemented  in\n        :mod:`sklearn.svm` estimators with the `probabilities=True` parameter.\n\n        .. versionadded:: 0.24\n\n    base_estimator : estimator instance\n        This parameter is deprecated. Use `estimator` instead.\n\n        .. deprecated:: 1.2\n           The parameter `base_estimator` is deprecated in 1.2 and will be\n           removed in 1.4. Use `estimator` instead.\n\n    Attributes\n    ----------\n    classes_ : ndarray of shape (n_classes,)\n        The class labels.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`. Only defined if the\n        underlying estimator exposes such an attribute when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Only defined if the\n        underlying estimator exposes such an attribute when fit.\n\n        .. versionadded:: 1.0\n\n    calibrated_classifiers_ : list (len() equal to cv or 1 if `cv=\"prefit\"` \\\n            or `ensemble=False`)\n        The list of classifier and calibrator pairs.\n\n        - When `cv=\"prefit\"`, the fitted `estimator` and fitted\n          calibrator.\n        - When `cv` is not \"prefit\" and `ensemble=True`, `n_cv` fitted\n          `estimator` and calibrator pairs. `n_cv` is the number of\n          cross-validation folds.\n        - When `cv` is not \"prefit\" and `ensemble=False`, the `estimator`,\n          fitted on all the data, and fitted calibrator.\n\n        .. versionchanged:: 0.24\n            Single calibrated classifier case when `ensemble=False`.\n\n    See Also\n    --------\n    calibration_curve : Compute true and predicted probabilities\n        for a calibration curve.\n\n    References\n    ----------\n    .. [1] Obtaining calibrated probability estimates from decision trees\n           and naive Bayesian classifiers, B. Zadrozny & C. Elkan, ICML 2001\n\n    .. [2] Transforming Classifier Scores into Accurate Multiclass\n           Probability Estimates, B. Zadrozny & C. Elkan, (KDD 2002)\n\n    .. [3] Probabilistic Outputs for Support Vector Machines and Comparisons to\n           Regularized Likelihood Methods, J. Platt, (1999)\n\n    .. [4] Predicting Good Probabilities with Supervised Learning,\n           A. Niculescu-Mizil & R. Caruana, ICML 2005\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_classification\n    >>> from sklearn.naive_bayes import GaussianNB\n    >>> from sklearn.calibration import CalibratedClassifierCV\n    >>> X, y = make_classification(n_samples=100, n_features=2,\n    ...                            n_redundant=0, random_state=42)\n    >>> base_clf = GaussianNB()\n    >>> calibrated_clf = CalibratedClassifierCV(base_clf, cv=3)\n    >>> calibrated_clf.fit(X, y)\n    CalibratedClassifierCV(...)\n    >>> len(calibrated_clf.calibrated_classifiers_)\n    3\n    >>> calibrated_clf.predict_proba(X)[:5, :]\n    array([[0.110..., 0.889...],\n           [0.072..., 0.927...],\n           [0.928..., 0.071...],\n           [0.928..., 0.071...],\n           [0.071..., 0.928...]])\n    >>> from sklearn.model_selection import train_test_split\n    >>> X, y = make_classification(n_samples=100, n_features=2,\n    ...                            n_redundant=0, random_state=42)\n    >>> X_train, X_calib, y_train, y_calib = train_test_split(\n    ...        X, y, random_state=42\n    ... )\n    >>> base_clf = GaussianNB()\n    >>> base_clf.fit(X_train, y_train)\n    GaussianNB()\n    >>> calibrated_clf = CalibratedClassifierCV(base_clf, cv=\"prefit\")\n    >>> calibrated_clf.fit(X_calib, y_calib)\n    CalibratedClassifierCV(...)\n    >>> len(calibrated_clf.calibrated_classifiers_)\n    1\n    >>> calibrated_clf.predict_proba([[-0.5, 0.5]])\n    array([[0.936..., 0.063...]])\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"estimator\": [\n            HasMethods([\"fit\", \"predict_proba\"]),\n            HasMethods([\"fit\", \"decision_function\"]),\n            None,\n        ],\n        \"method\": [StrOptions({\"isotonic\", \"sigmoid\"})],\n        \"cv\": [\"cv_object\", StrOptions({\"prefit\"})],\n        \"n_jobs\": [Integral, None],\n        \"ensemble\": [\"boolean\"],\n        \"base_estimator\": [\n            HasMethods([\"fit\", \"predict_proba\"]),\n            HasMethods([\"fit\", \"decision_function\"]),\n            None,\n            Hidden(StrOptions({\"deprecated\"})),\n        ],\n    }\n\n    def __init__(\n        self,\n        estimator=None,\n        *,\n        method=\"sigmoid\",\n        cv=None,\n        n_jobs=None,\n        ensemble=True,\n        base_estimator=\"deprecated\",\n    ):\n        self.estimator = estimator\n        self.method = method\n        self.cv = cv\n        self.n_jobs = n_jobs\n        self.ensemble = ensemble\n        self.base_estimator = base_estimator\n\n    def fit(self, X, y, sample_weight=None, **fit_params):\n        \"\"\"Fit the calibrated model.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted.\n\n        **fit_params : dict\n            Parameters to pass to the `fit` method of the underlying\n            classifier.\n\n        Returns\n        -------\n        self : object\n            Returns an instance of self.\n        \"\"\"\n        self._validate_params()\n\n        check_classification_targets(y)\n        X, y = indexable(X, y)\n        if sample_weight is not None:\n            sample_weight = _check_sample_weight(sample_weight, X)\n\n        for sample_aligned_params in fit_params.values():\n            check_consistent_length(y, sample_aligned_params)\n\n        # TODO(1.4): Remove when base_estimator is removed\n        if self.base_estimator != \"deprecated\":\n            if self.estimator is not None:\n                raise ValueError(\n                    \"Both `base_estimator` and `estimator` are set. Only set \"\n                    \"`estimator` since `base_estimator` is deprecated.\"\n                )\n            warnings.warn(\n                \"`base_estimator` was renamed to `estimator` in version 1.2 and \"\n                \"will be removed in 1.4.\",\n                FutureWarning,\n            )\n            estimator = self.base_estimator\n        else:\n            estimator = self.estimator\n\n        if estimator is None:\n            # we want all classifiers that don't expose a random_state\n            # to be deterministic (and we don't want to expose this one).\n            estimator = LinearSVC(random_state=0)\n\n        self.calibrated_classifiers_ = []\n        if self.cv == \"prefit\":\n            # `classes_` should be consistent with that of estimator\n            check_is_fitted(self.estimator, attributes=[\"classes_\"])\n            self.classes_ = self.estimator.classes_\n\n            pred_method, method_name = _get_prediction_method(estimator)\n            n_classes = len(self.classes_)\n            predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n\n            calibrated_classifier = _fit_calibrator(\n                estimator,\n                predictions,\n                y,\n                self.classes_,\n                self.method,\n                sample_weight,\n            )\n            self.calibrated_classifiers_.append(calibrated_classifier)\n        else:\n            # Set `classes_` using all `y`\n            label_encoder_ = LabelEncoder().fit(y)\n            self.classes_ = label_encoder_.classes_\n            n_classes = len(self.classes_)\n\n            # sample_weight checks\n            fit_parameters = signature(estimator.fit).parameters\n            supports_sw = \"sample_weight\" in fit_parameters\n            if sample_weight is not None and not supports_sw:\n                estimator_name = type(estimator).__name__\n                warnings.warn(\n                    f\"Since {estimator_name} does not appear to accept sample_weight, \"\n                    \"sample weights will only be used for the calibration itself. This \"\n                    \"can be caused by a limitation of the current scikit-learn API. \"\n                    \"See the following issue for more details: \"\n                    \"https://github.com/scikit-learn/scikit-learn/issues/21134. Be \"\n                    \"warned that the result of the calibration is likely to be \"\n                    \"incorrect.\"\n                )\n\n            # Check that each cross-validation fold can have at least one\n            # example per class\n            if isinstance(self.cv, int):\n                n_folds = self.cv\n            elif hasattr(self.cv, \"n_splits\"):\n                n_folds = self.cv.n_splits\n            else:\n                n_folds = None\n            if n_folds and np.any(\n                [np.sum(y == class_) < n_folds for class_ in self.classes_]\n            ):\n                raise ValueError(\n                    f\"Requesting {n_folds}-fold \"\n                    \"cross-validation but provided less than \"\n                    f\"{n_folds} examples for at least one class.\"\n                )\n            cv = check_cv(self.cv, y, classifier=True)\n\n            if self.ensemble:\n                parallel = Parallel(n_jobs=self.n_jobs)\n                self.calibrated_classifiers_ = parallel(\n                    delayed(_fit_classifier_calibrator_pair)(\n                        clone(estimator),\n                        X,\n                        y,\n                        train=train,\n                        test=test,\n                        method=self.method,\n                        classes=self.classes_,\n                        supports_sw=supports_sw,\n                        sample_weight=sample_weight,\n                        **fit_params,\n                    )\n                    for train, test in cv.split(X, y)\n                )\n            else:\n                this_estimator = clone(estimator)\n                _, method_name = _get_prediction_method(this_estimator)\n                fit_params = (\n                    {\"sample_weight\": sample_weight}\n                    if sample_weight is not None and supports_sw\n                    else None\n                )\n                pred_method = partial(\n                    cross_val_predict,\n                    estimator=this_estimator,\n                    X=X,\n                    y=y,\n                    cv=cv,\n                    method=method_name,\n                    n_jobs=self.n_jobs,\n                    fit_params=fit_params,\n                )\n                predictions = _compute_predictions(\n                    pred_method, method_name, X, n_classes\n                )\n\n                if sample_weight is not None and supports_sw:\n                    this_estimator.fit(X, y, sample_weight=sample_weight)\n                else:\n                    this_estimator.fit(X, y)\n                # Note: Here we don't pass on fit_params because the supported\n                # calibrators don't support fit_params anyway\n                calibrated_classifier = _fit_calibrator(\n                    this_estimator,\n                    predictions,\n                    y,\n                    self.classes_,\n                    self.method,\n                    sample_weight,\n                )\n                self.calibrated_classifiers_.append(calibrated_classifier)\n\n        first_clf = self.calibrated_classifiers_[0].estimator\n        if hasattr(first_clf, \"n_features_in_\"):\n            self.n_features_in_ = first_clf.n_features_in_\n        if hasattr(first_clf, \"feature_names_in_\"):\n            self.feature_names_in_ = first_clf.feature_names_in_\n        return self\n\n    def predict_proba(self, X):\n        \"\"\"Calibrated probabilities of classification.\n\n        This function returns calibrated probabilities of classification\n        according to each class on an array of test vectors X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The samples, as accepted by `estimator.predict_proba`.\n\n        Returns\n        -------\n        C : ndarray of shape (n_samples, n_classes)\n            The predicted probas.\n        \"\"\"\n        check_is_fitted(self)\n        # Compute the arithmetic mean of the predictions of the calibrated\n        # classifiers\n        mean_proba = np.zeros((_num_samples(X), len(self.classes_)))\n        for calibrated_classifier in self.calibrated_classifiers_:\n            proba = calibrated_classifier.predict_proba(X)\n            mean_proba += proba\n\n        mean_proba /= len(self.calibrated_classifiers_)\n\n        return mean_proba\n\n    def predict(self, X):\n        \"\"\"Predict the target of new samples.\n\n        The predicted class is the class that has the highest probability,\n        and can thus be different from the prediction of the uncalibrated classifier.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The samples, as accepted by `estimator.predict`.\n\n        Returns\n        -------\n        C : ndarray of shape (n_samples,)\n            The predicted class.\n        \"\"\"\n        check_is_fitted(self)\n        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n\n    def _more_tags(self):\n        return {\n            \"_xfail_checks\": {\n                \"check_sample_weights_invariance\": (\n                    \"Due to the cross-validation and sample ordering, removing a sample\"\n                    \" is not strictly equal to putting is weight to zero. Specific unit\"\n                    \" tests are added for CalibratedClassifierCV specifically.\"\n                ),\n            }\n        }\n\n\ndef _fit_classifier_calibrator_pair(\n    estimator,\n    X,\n    y,\n    train,\n    test,\n    supports_sw,\n    method,\n    classes,\n    sample_weight=None,\n    **fit_params,\n):\n    \"\"\"Fit a classifier/calibration pair on a given train/test split.\n\n    Fit the classifier on the train set, compute its predictions on the test\n    set and use the predictions as input to fit the calibrator along with the\n    test labels.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Cloned base estimator.\n\n    X : array-like, shape (n_samples, n_features)\n        Sample data.\n\n    y : array-like, shape (n_samples,)\n        Targets.\n\n    train : ndarray, shape (n_train_indices,)\n        Indices of the training subset.\n\n    test : ndarray, shape (n_test_indices,)\n        Indices of the testing subset.\n\n    supports_sw : bool\n        Whether or not the `estimator` supports sample weights.\n\n    method : {'sigmoid', 'isotonic'}\n        Method to use for calibration.\n\n    classes : ndarray, shape (n_classes,)\n        The target classes.\n\n    sample_weight : array-like, default=None\n        Sample weights for `X`.\n\n    **fit_params : dict\n        Parameters to pass to the `fit` method of the underlying\n        classifier.\n\n    Returns\n    -------\n    calibrated_classifier : _CalibratedClassifier instance\n    \"\"\"\n    fit_params_train = _check_fit_params(X, fit_params, train)\n    X_train, y_train = _safe_indexing(X, train), _safe_indexing(y, train)\n    X_test, y_test = _safe_indexing(X, test), _safe_indexing(y, test)\n\n    if sample_weight is not None and supports_sw:\n        sw_train = _safe_indexing(sample_weight, train)\n        estimator.fit(X_train, y_train, sample_weight=sw_train, **fit_params_train)\n    else:\n        estimator.fit(X_train, y_train, **fit_params_train)\n\n    n_classes = len(classes)\n    pred_method, method_name = _get_prediction_method(estimator)\n    predictions = _compute_predictions(pred_method, method_name, X_test, n_classes)\n\n    sw_test = None if sample_weight is None else _safe_indexing(sample_weight, test)\n    calibrated_classifier = _fit_calibrator(\n        estimator, predictions, y_test, classes, method, sample_weight=sw_test\n    )\n    return calibrated_classifier\n\n\ndef _get_prediction_method(clf):\n    \"\"\"Return prediction method.\n\n    `decision_function` method of `clf` returned, if it\n    exists, otherwise `predict_proba` method returned.\n\n    Parameters\n    ----------\n    clf : Estimator instance\n        Fitted classifier to obtain the prediction method from.\n\n    Returns\n    -------\n    prediction_method : callable\n        The prediction method.\n    method_name : str\n        The name of the prediction method.\n    \"\"\"\n    if hasattr(clf, \"decision_function\"):\n        method = getattr(clf, \"decision_function\")\n        return method, \"decision_function\"\n\n    if hasattr(clf, \"predict_proba\"):\n        method = getattr(clf, \"predict_proba\")\n        return method, \"predict_proba\"\n\n\ndef _compute_predictions(pred_method, method_name, X, n_classes):\n    \"\"\"Return predictions for `X` and reshape binary outputs to shape\n    (n_samples, 1).\n\n    Parameters\n    ----------\n    pred_method : callable\n        Prediction method.\n\n    method_name: str\n        Name of the prediction method\n\n    X : array-like or None\n        Data used to obtain predictions.\n\n    n_classes : int\n        Number of classes present.\n\n    Returns\n    -------\n    predictions : array-like, shape (X.shape[0], len(clf.classes_))\n        The predictions. Note if there are 2 classes, array is of shape\n        (X.shape[0], 1).\n    \"\"\"\n    predictions = pred_method(X=X)\n\n    if method_name == \"decision_function\":\n        if predictions.ndim == 1:\n            predictions = predictions[:, np.newaxis]\n    elif method_name == \"predict_proba\":\n        if n_classes == 2:\n            predictions = predictions[:, 1:]\n    else:  # pragma: no cover\n        # this branch should be unreachable.\n        raise ValueError(f\"Invalid prediction method: {method_name}\")\n    return predictions\n\n\ndef _fit_calibrator(clf, predictions, y, classes, method, sample_weight=None):\n    \"\"\"Fit calibrator(s) and return a `_CalibratedClassifier`\n    instance.\n\n    `n_classes` (i.e. `len(clf.classes_)`) calibrators are fitted.\n    However, if `n_classes` equals 2, one calibrator is fitted.\n\n    Parameters\n    ----------\n    clf : estimator instance\n        Fitted classifier.\n\n    predictions : array-like, shape (n_samples, n_classes) or (n_samples, 1) \\\n                    when binary.\n        Raw predictions returned by the un-calibrated base classifier.\n\n    y : array-like, shape (n_samples,)\n        The targets.\n\n    classes : ndarray, shape (n_classes,)\n        All the prediction classes.\n\n    method : {'sigmoid', 'isotonic'}\n        The method to use for calibration.\n\n    sample_weight : ndarray, shape (n_samples,), default=None\n        Sample weights. If None, then samples are equally weighted.\n\n    Returns\n    -------\n    pipeline : _CalibratedClassifier instance\n    \"\"\"\n    Y = label_binarize(y, classes=classes)\n    label_encoder = LabelEncoder().fit(classes)\n    pos_class_indices = label_encoder.transform(clf.classes_)\n    calibrators = []\n    for class_idx, this_pred in zip(pos_class_indices, predictions.T):\n        if method == \"isotonic\":\n            calibrator = IsotonicRegression(out_of_bounds=\"clip\")\n        else:  # \"sigmoid\"\n            calibrator = _SigmoidCalibration()\n        calibrator.fit(this_pred, Y[:, class_idx], sample_weight)\n        calibrators.append(calibrator)\n\n    pipeline = _CalibratedClassifier(clf, calibrators, method=method, classes=classes)\n    return pipeline\n\n\nclass _CalibratedClassifier:\n    \"\"\"Pipeline-like chaining a fitted classifier and its fitted calibrators.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Fitted classifier.\n\n    calibrators : list of fitted estimator instances\n        List of fitted calibrators (either 'IsotonicRegression' or\n        '_SigmoidCalibration'). The number of calibrators equals the number of\n        classes. However, if there are 2 classes, the list contains only one\n        fitted calibrator.\n\n    classes : array-like of shape (n_classes,)\n        All the prediction classes.\n\n    method : {'sigmoid', 'isotonic'}, default='sigmoid'\n        The method to use for calibration. Can be 'sigmoid' which\n        corresponds to Platt's method or 'isotonic' which is a\n        non-parametric approach based on isotonic regression.\n    \"\"\"\n\n    def __init__(self, estimator, calibrators, *, classes, method=\"sigmoid\"):\n        self.estimator = estimator\n        self.calibrators = calibrators\n        self.classes = classes\n        self.method = method\n\n    def predict_proba(self, X):\n        \"\"\"Calculate calibrated probabilities.\n\n        Calculates classification calibrated probabilities\n        for each class, in a one-vs-all manner, for `X`.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            The sample data.\n\n        Returns\n        -------\n        proba : array, shape (n_samples, n_classes)\n            The predicted probabilities. Can be exact zeros.\n        \"\"\"\n        n_classes = len(self.classes)\n        pred_method, method_name = _get_prediction_method(self.estimator)\n        predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n\n        label_encoder = LabelEncoder().fit(self.classes)\n        pos_class_indices = label_encoder.transform(self.estimator.classes_)\n\n        proba = np.zeros((_num_samples(X), n_classes))\n        for class_idx, this_pred, calibrator in zip(\n            pos_class_indices, predictions.T, self.calibrators\n        ):\n            if n_classes == 2:\n                # When binary, `predictions` consists only of predictions for\n                # clf.classes_[1] but `pos_class_indices` = 0\n                class_idx += 1\n            proba[:, class_idx] = calibrator.predict(this_pred)\n\n        # Normalize the probabilities\n        if n_classes == 2:\n            proba[:, 0] = 1.0 - proba[:, 1]\n        else:\n            denominator = np.sum(proba, axis=1)[:, np.newaxis]\n            # In the edge case where for each class calibrator returns a null\n            # probability for a given sample, use the uniform distribution\n            # instead.\n            uniform_proba = np.full_like(proba, 1 / n_classes)\n            proba = np.divide(\n                proba, denominator, out=uniform_proba, where=denominator != 0\n            )\n\n        # Deal with cases where the predicted probability minimally exceeds 1.0\n        proba[(1.0 < proba) & (proba <= 1.0 + 1e-5)] = 1.0\n\n        return proba\n\n\ndef _sigmoid_calibration(predictions, y, sample_weight=None):\n    \"\"\"Probability Calibration with sigmoid method (Platt 2000)\n\n    Parameters\n    ----------\n    predictions : ndarray of shape (n_samples,)\n        The decision function or predict proba for the samples.\n\n    y : ndarray of shape (n_samples,)\n        The targets.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Sample weights. If None, then samples are equally weighted.\n\n    Returns\n    -------\n    a : float\n        The slope.\n\n    b : float\n        The intercept.\n\n    References\n    ----------\n    Platt, \"Probabilistic Outputs for Support Vector Machines\"\n    \"\"\"\n    predictions = column_or_1d(predictions)\n    y = column_or_1d(y)\n\n    F = predictions  # F follows Platt's notations\n\n    # Bayesian priors (see Platt end of section 2.2):\n    # It corresponds to the number of samples, taking into account the\n    # `sample_weight`.\n    mask_negative_samples = y <= 0\n    if sample_weight is not None:\n        prior0 = (sample_weight[mask_negative_samples]).sum()\n        prior1 = (sample_weight[~mask_negative_samples]).sum()\n    else:\n        prior0 = float(np.sum(mask_negative_samples))\n        prior1 = y.shape[0] - prior0\n    T = np.zeros_like(y, dtype=np.float64)\n    T[y > 0] = (prior1 + 1.0) / (prior1 + 2.0)\n    T[y <= 0] = 1.0 / (prior0 + 2.0)\n    T1 = 1.0 - T\n\n    def objective(AB):\n        # From Platt (beginning of Section 2.2)\n        P = expit(-(AB[0] * F + AB[1]))\n        loss = -(xlogy(T, P) + xlogy(T1, 1.0 - P))\n        if sample_weight is not None:\n            return (sample_weight * loss).sum()\n        else:\n            return loss.sum()\n\n    def grad(AB):\n        # gradient of the objective function\n        P = expit(-(AB[0] * F + AB[1]))\n        TEP_minus_T1P = T - P\n        if sample_weight is not None:\n            TEP_minus_T1P *= sample_weight\n        dA = np.dot(TEP_minus_T1P, F)\n        dB = np.sum(TEP_minus_T1P)\n        return np.array([dA, dB])\n\n    AB0 = np.array([0.0, log((prior0 + 1.0) / (prior1 + 1.0))])\n    AB_ = fmin_bfgs(objective, AB0, fprime=grad, disp=False)\n    return AB_[0], AB_[1]\n\n\nclass _SigmoidCalibration(RegressorMixin, BaseEstimator):\n    \"\"\"Sigmoid regression model.\n\n    Attributes\n    ----------\n    a_ : float\n        The slope.\n\n    b_ : float\n        The intercept.\n    \"\"\"\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples,)\n            Training data.\n\n        y : array-like of shape (n_samples,)\n            Training target.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted.\n\n        Returns\n        -------\n        self : object\n            Returns an instance of self.\n        \"\"\"\n        X = column_or_1d(X)\n        y = column_or_1d(y)\n        X, y = indexable(X, y)\n\n        self.a_, self.b_ = _sigmoid_calibration(X, y, sample_weight)\n        return self\n\n    def predict(self, T):\n        \"\"\"Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,)\n            Data to predict from.\n\n        Returns\n        -------\n        T_ : ndarray of shape (n_samples,)\n            The predicted data.\n        \"\"\"\n        T = column_or_1d(T)\n        return expit(-(self.a_ * T + self.b_))\n\n\ndef calibration_curve(\n    y_true,\n    y_prob,\n    *,\n    pos_label=None,\n    normalize=\"deprecated\",\n    n_bins=5,\n    strategy=\"uniform\",\n):\n    \"\"\"Compute true and predicted probabilities for a calibration curve.\n\n    The method assumes the inputs come from a binary classifier, and\n    discretize the [0, 1] interval into bins.\n\n    Calibration curves may also be referred to as reliability diagrams.\n\n    Read more in the :ref:`User Guide <calibration>`.\n\n    Parameters\n    ----------\n    y_true : array-like of shape (n_samples,)\n        True targets.\n\n    y_prob : array-like of shape (n_samples,)\n        Probabilities of the positive class.\n\n    pos_label : int or str, default=None\n        The label of the positive class.\n\n        .. versionadded:: 1.1\n\n    normalize : bool, default=\"deprecated\"\n        Whether y_prob needs to be normalized into the [0, 1] interval, i.e.\n        is not a proper probability. If True, the smallest value in y_prob\n        is linearly mapped onto 0 and the largest one onto 1.\n\n        .. deprecated:: 1.1\n            The normalize argument is deprecated in v1.1 and will be removed in v1.3.\n            Explicitly normalizing `y_prob` will reproduce this behavior, but it is\n            recommended that a proper probability is used (i.e. a classifier's\n            `predict_proba` positive class).\n\n    n_bins : int, default=5\n        Number of bins to discretize the [0, 1] interval. A bigger number\n        requires more data. Bins with no samples (i.e. without\n        corresponding values in `y_prob`) will not be returned, thus the\n        returned arrays may have less than `n_bins` values.\n\n    strategy : {'uniform', 'quantile'}, default='uniform'\n        Strategy used to define the widths of the bins.\n\n        uniform\n            The bins have identical widths.\n        quantile\n            The bins have the same number of samples and depend on `y_prob`.\n\n    Returns\n    -------\n    prob_true : ndarray of shape (n_bins,) or smaller\n        The proportion of samples whose class is the positive class, in each\n        bin (fraction of positives).\n\n    prob_pred : ndarray of shape (n_bins,) or smaller\n        The mean predicted probability in each bin.\n\n    References\n    ----------\n    Alexandru Niculescu-Mizil and Rich Caruana (2005) Predicting Good\n    Probabilities With Supervised Learning, in Proceedings of the 22nd\n    International Conference on Machine Learning (ICML).\n    See section 4 (Qualitative Analysis of Predictions).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.calibration import calibration_curve\n    >>> y_true = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1])\n    >>> y_pred = np.array([0.1, 0.2, 0.3, 0.4, 0.65, 0.7, 0.8, 0.9,  1.])\n    >>> prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=3)\n    >>> prob_true\n    array([0. , 0.5, 1. ])\n    >>> prob_pred\n    array([0.2  , 0.525, 0.85 ])\n    \"\"\"\n    y_true = column_or_1d(y_true)\n    y_prob = column_or_1d(y_prob)\n    check_consistent_length(y_true, y_prob)\n    pos_label = _check_pos_label_consistency(pos_label, y_true)\n\n    # TODO(1.3): Remove normalize conditional block.\n    if normalize != \"deprecated\":\n        warnings.warn(\n            \"The normalize argument is deprecated in v1.1 and will be removed in v1.3.\"\n            \" Explicitly normalizing y_prob will reproduce this behavior, but it is\"\n            \" recommended that a proper probability is used (i.e. a classifier's\"\n            \" `predict_proba` positive class or `decision_function` output calibrated\"\n            \" with `CalibratedClassifierCV`).\",\n            FutureWarning,\n        )\n        if normalize:  # Normalize predicted values into interval [0, 1]\n            y_prob = (y_prob - y_prob.min()) / (y_prob.max() - y_prob.min())\n\n    if y_prob.min() < 0 or y_prob.max() > 1:\n        raise ValueError(\"y_prob has values outside [0, 1].\")\n\n    labels = np.unique(y_true)\n    if len(labels) > 2:\n        raise ValueError(\n            f\"Only binary classification is supported. Provided labels {labels}.\"\n        )\n    y_true = y_true == pos_label\n\n    if strategy == \"quantile\":  # Determine bin edges by distribution of data\n        quantiles = np.linspace(0, 1, n_bins + 1)\n        bins = np.percentile(y_prob, quantiles * 100)\n    elif strategy == \"uniform\":\n        bins = np.linspace(0.0, 1.0, n_bins + 1)\n    else:\n        raise ValueError(\n            \"Invalid entry to 'strategy' input. Strategy \"\n            \"must be either 'quantile' or 'uniform'.\"\n        )\n\n    binids = np.searchsorted(bins[1:-1], y_prob)\n\n    bin_sums = np.bincount(binids, weights=y_prob, minlength=len(bins))\n    bin_true = np.bincount(binids, weights=y_true, minlength=len(bins))\n    bin_total = np.bincount(binids, minlength=len(bins))\n\n    nonzero = bin_total != 0\n    prob_true = bin_true[nonzero] / bin_total[nonzero]\n    prob_pred = bin_sums[nonzero] / bin_total[nonzero]\n\n    return prob_true, prob_pred\n\n\nclass CalibrationDisplay:\n    \"\"\"Calibration curve (also known as reliability diagram) visualization.\n\n    It is recommended to use\n    :func:`~sklearn.calibration.CalibrationDisplay.from_estimator` or\n    :func:`~sklearn.calibration.CalibrationDisplay.from_predictions`\n    to create a `CalibrationDisplay`. All parameters are stored as attributes.\n\n    Read more about calibration in the :ref:`User Guide <calibration>` and\n    more about the scikit-learn visualization API in :ref:`visualizations`.\n\n    .. versionadded:: 1.0\n\n    Parameters\n    ----------\n    prob_true : ndarray of shape (n_bins,)\n        The proportion of samples whose class is the positive class (fraction\n        of positives), in each bin.\n\n    prob_pred : ndarray of shape (n_bins,)\n        The mean predicted probability in each bin.\n\n    y_prob : ndarray of shape (n_samples,)\n        Probability estimates for the positive class, for each sample.\n\n    estimator_name : str, default=None\n        Name of estimator. If None, the estimator name is not shown.\n\n    pos_label : str or int, default=None\n        The positive class when computing the calibration curve.\n        By default, `estimators.classes_[1]` is considered as the\n        positive class.\n\n        .. versionadded:: 1.1\n\n    Attributes\n    ----------\n    line_ : matplotlib Artist\n        Calibration curve.\n\n    ax_ : matplotlib Axes\n        Axes with calibration curve.\n\n    figure_ : matplotlib Figure\n        Figure containing the curve.\n\n    See Also\n    --------\n    calibration_curve : Compute true and predicted probabilities for a\n        calibration curve.\n    CalibrationDisplay.from_predictions : Plot calibration curve using true\n        and predicted labels.\n    CalibrationDisplay.from_estimator : Plot calibration curve using an\n        estimator and data.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_classification\n    >>> from sklearn.model_selection import train_test_split\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.calibration import calibration_curve, CalibrationDisplay\n    >>> X, y = make_classification(random_state=0)\n    >>> X_train, X_test, y_train, y_test = train_test_split(\n    ...     X, y, random_state=0)\n    >>> clf = LogisticRegression(random_state=0)\n    >>> clf.fit(X_train, y_train)\n    LogisticRegression(random_state=0)\n    >>> y_prob = clf.predict_proba(X_test)[:, 1]\n    >>> prob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10)\n    >>> disp = CalibrationDisplay(prob_true, prob_pred, y_prob)\n    >>> disp.plot()\n    <...>\n    \"\"\"\n\n    def __init__(\n        self, prob_true, prob_pred, y_prob, *, estimator_name=None, pos_label=None\n    ):\n        self.prob_true = prob_true\n        self.prob_pred = prob_pred\n        self.y_prob = y_prob\n        self.estimator_name = estimator_name\n        self.pos_label = pos_label\n\n    def plot(self, *, ax=None, name=None, ref_line=True, **kwargs):\n        \"\"\"Plot visualization.\n\n        Extra keyword arguments will be passed to\n        :func:`matplotlib.pyplot.plot`.\n\n        Parameters\n        ----------\n        ax : Matplotlib Axes, default=None\n            Axes object to plot on. If `None`, a new figure and axes is\n            created.\n\n        name : str, default=None\n            Name for labeling curve. If `None`, use `estimator_name` if\n            not `None`, otherwise no labeling is shown.\n\n        ref_line : bool, default=True\n            If `True`, plots a reference line representing a perfectly\n            calibrated classifier.\n\n        **kwargs : dict\n            Keyword arguments to be passed to :func:`matplotlib.pyplot.plot`.\n\n        Returns\n        -------\n        display : :class:`~sklearn.calibration.CalibrationDisplay`\n            Object that stores computed values.\n        \"\"\"\n        check_matplotlib_support(\"CalibrationDisplay.plot\")\n        import matplotlib.pyplot as plt\n\n        if ax is None:\n            fig, ax = plt.subplots()\n\n        name = self.estimator_name if name is None else name\n        info_pos_label = (\n            f\"(Positive class: {self.pos_label})\" if self.pos_label is not None else \"\"\n        )\n\n        line_kwargs = {}\n        if name is not None:\n            line_kwargs[\"label\"] = name\n        line_kwargs.update(**kwargs)\n\n        ref_line_label = \"Perfectly calibrated\"\n        existing_ref_line = ref_line_label in ax.get_legend_handles_labels()[1]\n        if ref_line and not existing_ref_line:\n            ax.plot([0, 1], [0, 1], \"k:\", label=ref_line_label)\n        self.line_ = ax.plot(self.prob_pred, self.prob_true, \"s-\", **line_kwargs)[0]\n\n        # We always have to show the legend for at least the reference line\n        ax.legend(loc=\"lower right\")\n\n        xlabel = f\"Mean predicted probability {info_pos_label}\"\n        ylabel = f\"Fraction of positives {info_pos_label}\"\n        ax.set(xlabel=xlabel, ylabel=ylabel)\n\n        self.ax_ = ax\n        self.figure_ = ax.figure\n        return self\n\n    @classmethod\n    def from_estimator(\n        cls,\n        estimator,\n        X,\n        y,\n        *,\n        n_bins=5,\n        strategy=\"uniform\",\n        pos_label=None,\n        name=None,\n        ref_line=True,\n        ax=None,\n        **kwargs,\n    ):\n        \"\"\"Plot calibration curve using a binary classifier and data.\n\n        A calibration curve, also known as a reliability diagram, uses inputs\n        from a binary classifier and plots the average predicted probability\n        for each bin against the fraction of positive classes, on the\n        y-axis.\n\n        Extra keyword arguments will be passed to\n        :func:`matplotlib.pyplot.plot`.\n\n        Read more about calibration in the :ref:`User Guide <calibration>` and\n        more about the scikit-learn visualization API in :ref:`visualizations`.\n\n        .. versionadded:: 1.0\n\n        Parameters\n        ----------\n        estimator : estimator instance\n            Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\n            in which the last estimator is a classifier. The classifier must\n            have a :term:`predict_proba` method.\n\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input values.\n\n        y : array-like of shape (n_samples,)\n            Binary target values.\n\n        n_bins : int, default=5\n            Number of bins to discretize the [0, 1] interval into when\n            calculating the calibration curve. A bigger number requires more\n            data.\n\n        strategy : {'uniform', 'quantile'}, default='uniform'\n            Strategy used to define the widths of the bins.\n\n            - `'uniform'`: The bins have identical widths.\n            - `'quantile'`: The bins have the same number of samples and depend\n              on predicted probabilities.\n\n        pos_label : str or int, default=None\n            The positive class when computing the calibration curve.\n            By default, `estimators.classes_[1]` is considered as the\n            positive class.\n\n            .. versionadded:: 1.1\n\n        name : str, default=None\n            Name for labeling curve. If `None`, the name of the estimator is\n            used.\n\n        ref_line : bool, default=True\n            If `True`, plots a reference line representing a perfectly\n            calibrated classifier.\n\n        ax : matplotlib axes, default=None\n            Axes object to plot on. If `None`, a new figure and axes is\n            created.\n\n        **kwargs : dict\n            Keyword arguments to be passed to :func:`matplotlib.pyplot.plot`.\n\n        Returns\n        -------\n        display : :class:`~sklearn.calibration.CalibrationDisplay`.\n            Object that stores computed values.\n\n        See Also\n        --------\n        CalibrationDisplay.from_predictions : Plot calibration curve using true\n            and predicted labels.\n\n        Examples\n        --------\n        >>> import matplotlib.pyplot as plt\n        >>> from sklearn.datasets import make_classification\n        >>> from sklearn.model_selection import train_test_split\n        >>> from sklearn.linear_model import LogisticRegression\n        >>> from sklearn.calibration import CalibrationDisplay\n        >>> X, y = make_classification(random_state=0)\n        >>> X_train, X_test, y_train, y_test = train_test_split(\n        ...     X, y, random_state=0)\n        >>> clf = LogisticRegression(random_state=0)\n        >>> clf.fit(X_train, y_train)\n        LogisticRegression(random_state=0)\n        >>> disp = CalibrationDisplay.from_estimator(clf, X_test, y_test)\n        >>> plt.show()\n        \"\"\"\n        method_name = f\"{cls.__name__}.from_estimator\"\n        check_matplotlib_support(method_name)\n\n        if not is_classifier(estimator):\n            raise ValueError(\"'estimator' should be a fitted classifier.\")\n\n        y_prob, pos_label = _get_response(\n            X, estimator, response_method=\"predict_proba\", pos_label=pos_label\n        )\n\n        name = name if name is not None else estimator.__class__.__name__\n        return cls.from_predictions(\n            y,\n            y_prob,\n            n_bins=n_bins,\n            strategy=strategy,\n            pos_label=pos_label,\n            name=name,\n            ref_line=ref_line,\n            ax=ax,\n            **kwargs,\n        )\n\n    @classmethod\n    def from_predictions(\n        cls,\n        y_true,\n        y_prob,\n        *,\n        n_bins=5,\n        strategy=\"uniform\",\n        pos_label=None,\n        name=None,\n        ref_line=True,\n        ax=None,\n        **kwargs,\n    ):\n        \"\"\"Plot calibration curve using true labels and predicted probabilities.\n\n        Calibration curve, also known as reliability diagram, uses inputs\n        from a binary classifier and plots the average predicted probability\n        for each bin against the fraction of positive classes, on the\n        y-axis.\n\n        Extra keyword arguments will be passed to\n        :func:`matplotlib.pyplot.plot`.\n\n        Read more about calibration in the :ref:`User Guide <calibration>` and\n        more about the scikit-learn visualization API in :ref:`visualizations`.\n\n        .. versionadded:: 1.0\n\n        Parameters\n        ----------\n        y_true : array-like of shape (n_samples,)\n            True labels.\n\n        y_prob : array-like of shape (n_samples,)\n            The predicted probabilities of the positive class.\n\n        n_bins : int, default=5\n            Number of bins to discretize the [0, 1] interval into when\n            calculating the calibration curve. A bigger number requires more\n            data.\n\n        strategy : {'uniform', 'quantile'}, default='uniform'\n            Strategy used to define the widths of the bins.\n\n            - `'uniform'`: The bins have identical widths.\n            - `'quantile'`: The bins have the same number of samples and depend\n              on predicted probabilities.\n\n        pos_label : str or int, default=None\n            The positive class when computing the calibration curve.\n            By default, `estimators.classes_[1]` is considered as the\n            positive class.\n\n            .. versionadded:: 1.1\n\n        name : str, default=None\n            Name for labeling curve.\n\n        ref_line : bool, default=True\n            If `True`, plots a reference line representing a perfectly\n            calibrated classifier.\n\n        ax : matplotlib axes, default=None\n            Axes object to plot on. If `None`, a new figure and axes is\n            created.\n\n        **kwargs : dict\n            Keyword arguments to be passed to :func:`matplotlib.pyplot.plot`.\n\n        Returns\n        -------\n        display : :class:`~sklearn.calibration.CalibrationDisplay`.\n            Object that stores computed values.\n\n        See Also\n        --------\n        CalibrationDisplay.from_estimator : Plot calibration curve using an\n            estimator and data.\n\n        Examples\n        --------\n        >>> import matplotlib.pyplot as plt\n        >>> from sklearn.datasets import make_classification\n        >>> from sklearn.model_selection import train_test_split\n        >>> from sklearn.linear_model import LogisticRegression\n        >>> from sklearn.calibration import CalibrationDisplay\n        >>> X, y = make_classification(random_state=0)\n        >>> X_train, X_test, y_train, y_test = train_test_split(\n        ...     X, y, random_state=0)\n        >>> clf = LogisticRegression(random_state=0)\n        >>> clf.fit(X_train, y_train)\n        LogisticRegression(random_state=0)\n        >>> y_prob = clf.predict_proba(X_test)[:, 1]\n        >>> disp = CalibrationDisplay.from_predictions(y_test, y_prob)\n        >>> plt.show()\n        \"\"\"\n        method_name = f\"{cls.__name__}.from_estimator\"\n        check_matplotlib_support(method_name)\n\n        prob_true, prob_pred = calibration_curve(\n            y_true, y_prob, n_bins=n_bins, strategy=strategy, pos_label=pos_label\n        )\n        name = \"Classifier\" if name is None else name\n        pos_label = _check_pos_label_consistency(pos_label, y_true)\n\n        disp = cls(\n            prob_true=prob_true,\n            prob_pred=prob_pred,\n            y_prob=y_prob,\n            estimator_name=name,\n            pos_label=pos_label,\n        )\n        return disp.plot(ax=ax, ref_line=ref_line, **kwargs)", "# Authors: Fabian Pedregosa <fabian@fseoane.net>\n#          Alexandre Gramfort <alexandre.gramfort@inria.fr>\n#          Nelle Varoquaux <nelle.varoquaux@gmail.com>\n# License: BSD 3 clause\n\nimport numpy as np\nfrom scipy import interpolate\nfrom scipy.stats import spearmanr\nfrom numbers import Real\nimport warnings\nimport math\n\nfrom .base import BaseEstimator, TransformerMixin, RegressorMixin\nfrom .utils import check_array, check_consistent_length\nfrom .utils.validation import _check_sample_weight, check_is_fitted\nfrom .utils._param_validation import Interval, StrOptions\nfrom ._isotonic import _inplace_contiguous_isotonic_regression, _make_unique\n\n\n__all__ = [\"check_increasing\", \"isotonic_regression\", \"IsotonicRegression\"]\n\n\ndef check_increasing(x, y):\n    \"\"\"Determine whether y is monotonically correlated with x.\n\n    y is found increasing or decreasing with respect to x based on a Spearman\n    correlation test.\n\n    Parameters\n    ----------\n    x : array-like of shape (n_samples,)\n            Training data.\n\n    y : array-like of shape (n_samples,)\n        Training target.\n\n    Returns\n    -------\n    increasing_bool : boolean\n        Whether the relationship is increasing or decreasing.\n\n    Notes\n    -----\n    The Spearman correlation coefficient is estimated from the data, and the\n    sign of the resulting estimate is used as the result.\n\n    In the event that the 95% confidence interval based on Fisher transform\n    spans zero, a warning is raised.\n\n    References\n    ----------\n    Fisher transformation. Wikipedia.\n    https://en.wikipedia.org/wiki/Fisher_transformation\n    \"\"\"\n\n    # Calculate Spearman rho estimate and set return accordingly.\n    rho, _ = spearmanr(x, y)\n    increasing_bool = rho >= 0\n\n    # Run Fisher transform to get the rho CI, but handle rho=+/-1\n    if rho not in [-1.0, 1.0] and len(x) > 3:\n        F = 0.5 * math.log((1.0 + rho) / (1.0 - rho))\n        F_se = 1 / math.sqrt(len(x) - 3)\n\n        # Use a 95% CI, i.e., +/-1.96 S.E.\n        # https://en.wikipedia.org/wiki/Fisher_transformation\n        rho_0 = math.tanh(F - 1.96 * F_se)\n        rho_1 = math.tanh(F + 1.96 * F_se)\n\n        # Warn if the CI spans zero.\n        if np.sign(rho_0) != np.sign(rho_1):\n            warnings.warn(\n                \"Confidence interval of the Spearman \"\n                \"correlation coefficient spans zero. \"\n                \"Determination of ``increasing`` may be \"\n                \"suspect.\"\n            )\n\n    return increasing_bool\n\n\ndef isotonic_regression(\n    y, *, sample_weight=None, y_min=None, y_max=None, increasing=True\n):\n    \"\"\"Solve the isotonic regression model.\n\n    Read more in the :ref:`User Guide <isotonic>`.\n\n    Parameters\n    ----------\n    y : array-like of shape (n_samples,)\n        The data.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Weights on each point of the regression.\n        If None, weight is set to 1 (equal weights).\n\n    y_min : float, default=None\n        Lower bound on the lowest predicted value (the minimum value may\n        still be higher). If not set, defaults to -inf.\n\n    y_max : float, default=None\n        Upper bound on the highest predicted value (the maximum may still be\n        lower). If not set, defaults to +inf.\n\n    increasing : bool, default=True\n        Whether to compute ``y_`` is increasing (if set to True) or decreasing\n        (if set to False).\n\n    Returns\n    -------\n    y_ : list of floats\n        Isotonic fit of y.\n\n    References\n    ----------\n    \"Active set algorithms for isotonic regression; A unifying framework\"\n    by Michael J. Best and Nilotpal Chakravarti, section 3.\n    \"\"\"\n    order = np.s_[:] if increasing else np.s_[::-1]\n    y = check_array(y, ensure_2d=False, input_name=\"y\", dtype=[np.float64, np.float32])\n    y = np.array(y[order], dtype=y.dtype)\n    sample_weight = _check_sample_weight(sample_weight, y, dtype=y.dtype, copy=True)\n    sample_weight = np.ascontiguousarray(sample_weight[order])\n\n    _inplace_contiguous_isotonic_regression(y, sample_weight)\n    if y_min is not None or y_max is not None:\n        # Older versions of np.clip don't accept None as a bound, so use np.inf\n        if y_min is None:\n            y_min = -np.inf\n        if y_max is None:\n            y_max = np.inf\n        np.clip(y, y_min, y_max, y)\n    return y[order]\n\n\nclass IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n    \"\"\"Isotonic regression model.\n\n    Read more in the :ref:`User Guide <isotonic>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    y_min : float, default=None\n        Lower bound on the lowest predicted value (the minimum value may\n        still be higher). If not set, defaults to -inf.\n\n    y_max : float, default=None\n        Upper bound on the highest predicted value (the maximum may still be\n        lower). If not set, defaults to +inf.\n\n    increasing : bool or 'auto', default=True\n        Determines whether the predictions should be constrained to increase\n        or decrease with `X`. 'auto' will decide based on the Spearman\n        correlation estimate's sign.\n\n    out_of_bounds : {'nan', 'clip', 'raise'}, default='nan'\n        Handles how `X` values outside of the training domain are handled\n        during prediction.\n\n        - 'nan', predictions will be NaN.\n        - 'clip', predictions will be set to the value corresponding to\n          the nearest train interval endpoint.\n        - 'raise', a `ValueError` is raised.\n\n    Attributes\n    ----------\n    X_min_ : float\n        Minimum value of input array `X_` for left bound.\n\n    X_max_ : float\n        Maximum value of input array `X_` for right bound.\n\n    X_thresholds_ : ndarray of shape (n_thresholds,)\n        Unique ascending `X` values used to interpolate\n        the y = f(X) monotonic function.\n\n        .. versionadded:: 0.24\n\n    y_thresholds_ : ndarray of shape (n_thresholds,)\n        De-duplicated `y` values suitable to interpolate the y = f(X)\n        monotonic function.\n\n        .. versionadded:: 0.24\n\n    f_ : function\n        The stepwise interpolating function that covers the input domain ``X``.\n\n    increasing_ : bool\n        Inferred value for ``increasing``.\n\n    See Also\n    --------\n    sklearn.linear_model.LinearRegression : Ordinary least squares Linear\n        Regression.\n    sklearn.ensemble.HistGradientBoostingRegressor : Gradient boosting that\n        is a non-parametric model accepting monotonicity constraints.\n    isotonic_regression : Function to solve the isotonic regression model.\n\n    Notes\n    -----\n    Ties are broken using the secondary method from de Leeuw, 1977.\n\n    References\n    ----------\n    Isotonic Median Regression: A Linear Programming Approach\n    Nilotpal Chakravarti\n    Mathematics of Operations Research\n    Vol. 14, No. 2 (May, 1989), pp. 303-308\n\n    Isotone Optimization in R : Pool-Adjacent-Violators\n    Algorithm (PAVA) and Active Set Methods\n    de Leeuw, Hornik, Mair\n    Journal of Statistical Software 2009\n\n    Correctness of Kruskal's algorithms for monotone regression with ties\n    de Leeuw, Psychometrica, 1977\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.isotonic import IsotonicRegression\n    >>> X, y = make_regression(n_samples=10, n_features=1, random_state=41)\n    >>> iso_reg = IsotonicRegression().fit(X, y)\n    >>> iso_reg.predict([.1, .2])\n    array([1.8628..., 3.7256...])\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"y_min\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"y_max\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"increasing\": [\"boolean\", StrOptions({\"auto\"})],\n        \"out_of_bounds\": [StrOptions({\"nan\", \"clip\", \"raise\"})],\n    }\n\n    def __init__(self, *, y_min=None, y_max=None, increasing=True, out_of_bounds=\"nan\"):\n        self.y_min = y_min\n        self.y_max = y_max\n        self.increasing = increasing\n        self.out_of_bounds = out_of_bounds\n\n    def _check_input_data_shape(self, X):\n        if not (X.ndim == 1 or (X.ndim == 2 and X.shape[1] == 1)):\n            msg = (\n                \"Isotonic regression input X should be a 1d array or \"\n                \"2d array with 1 feature\"\n            )\n            raise ValueError(msg)\n\n    def _build_f(self, X, y):\n        \"\"\"Build the f_ interp1d function.\"\"\"\n\n        bounds_error = self.out_of_bounds == \"raise\"\n        if len(y) == 1:\n            # single y, constant prediction\n            self.f_ = lambda x: y.repeat(x.shape)\n        else:\n            self.f_ = interpolate.interp1d(\n                X, y, kind=\"linear\", bounds_error=bounds_error\n            )\n\n    def _build_y(self, X, y, sample_weight, trim_duplicates=True):\n        \"\"\"Build the y_ IsotonicRegression.\"\"\"\n        self._check_input_data_shape(X)\n        X = X.reshape(-1)  # use 1d view\n\n        # Determine increasing if auto-determination requested\n        if self.increasing == \"auto\":\n            self.increasing_ = check_increasing(X, y)\n        else:\n            self.increasing_ = self.increasing\n\n        # If sample_weights is passed, removed zero-weight values and clean\n        # order\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        mask = sample_weight > 0\n        X, y, sample_weight = X[mask], y[mask], sample_weight[mask]\n\n        order = np.lexsort((y, X))\n        X, y, sample_weight = [array[order] for array in [X, y, sample_weight]]\n        unique_X, unique_y, unique_sample_weight = _make_unique(X, y, sample_weight)\n\n        X = unique_X\n        y = isotonic_regression(\n            unique_y,\n            sample_weight=unique_sample_weight,\n            y_min=self.y_min,\n            y_max=self.y_max,\n            increasing=self.increasing_,\n        )\n\n        # Handle the left and right bounds on X\n        self.X_min_, self.X_max_ = np.min(X), np.max(X)\n\n        if trim_duplicates:\n            # Remove unnecessary points for faster prediction\n            keep_data = np.ones((len(y),), dtype=bool)\n            # Aside from the 1st and last point, remove points whose y values\n            # are equal to both the point before and the point after it.\n            keep_data[1:-1] = np.logical_or(\n                np.not_equal(y[1:-1], y[:-2]), np.not_equal(y[1:-1], y[2:])\n            )\n            return X[keep_data], y[keep_data]\n        else:\n            # The ability to turn off trim_duplicates is only used to it make\n            # easier to unit test that removing duplicates in y does not have\n            # any impact the resulting interpolation function (besides\n            # prediction speed).\n            return X, y\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples,) or (n_samples, 1)\n            Training data.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        y : array-like of shape (n_samples,)\n            Training target.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Weights. If set to None, all weights will be set to 1 (equal\n            weights).\n\n        Returns\n        -------\n        self : object\n            Returns an instance of self.\n\n        Notes\n        -----\n        X is stored for future use, as :meth:`transform` needs X to interpolate\n        new input data.\n        \"\"\"\n        self._validate_params()\n        check_params = dict(accept_sparse=False, ensure_2d=False)\n        X = check_array(\n            X, input_name=\"X\", dtype=[np.float64, np.float32], **check_params\n        )\n        y = check_array(y, input_name=\"y\", dtype=X.dtype, **check_params)\n        check_consistent_length(X, y, sample_weight)\n\n        # Transform y by running the isotonic regression algorithm and\n        # transform X accordingly.\n        X, y = self._build_y(X, y, sample_weight)\n\n        # It is necessary to store the non-redundant part of the training set\n        # on the model to make it possible to support model persistence via\n        # the pickle module as the object built by scipy.interp1d is not\n        # picklable directly.\n        self.X_thresholds_, self.y_thresholds_ = X, y\n\n        # Build the interpolation function\n        self._build_f(X, y)\n        return self\n\n    def transform(self, T):\n        \"\"\"Transform new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            The transformed data.\n        \"\"\"\n\n        if hasattr(self, \"X_thresholds_\"):\n            dtype = self.X_thresholds_.dtype\n        else:\n            dtype = np.float64\n\n        T = check_array(T, dtype=dtype, ensure_2d=False)\n\n        self._check_input_data_shape(T)\n        T = T.reshape(-1)  # use 1d view\n\n        if self.out_of_bounds == \"clip\":\n            T = np.clip(T, self.X_min_, self.X_max_)\n\n        res = self.f_(T)\n\n        # on scipy 0.17, interp1d up-casts to float64, so we cast back\n        res = res.astype(T.dtype)\n\n        return res\n\n    def predict(self, T):\n        \"\"\"Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            Transformed data.\n        \"\"\"\n        return self.transform(T)\n\n    # We implement get_feature_names_out here instead of using\n    # `ClassNamePrefixFeaturesOutMixin`` because `input_features` are ignored.\n    # `input_features` are ignored because `IsotonicRegression` accepts 1d\n    # arrays and the semantics of `feature_names_in_` are not clear for 1d arrays.\n    def get_feature_names_out(self, input_features=None):\n        \"\"\"Get output feature names for transformation.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Ignored.\n\n        Returns\n        -------\n        feature_names_out : ndarray of str objects\n            An ndarray with one string i.e. [\"isotonicregression0\"].\n        \"\"\"\n        check_is_fitted(self, \"f_\")\n        class_name = self.__class__.__name__.lower()\n        return np.asarray([f\"{class_name}0\"], dtype=object)\n\n    def __getstate__(self):\n        \"\"\"Pickle-protocol - return state of the estimator.\"\"\"\n        state = super().__getstate__()\n        # remove interpolation method\n        state.pop(\"f_\", None)\n        return state\n\n    def __setstate__(self, state):\n        \"\"\"Pickle-protocol - set state of the estimator.\n\n        We need to rebuild the interpolation function.\n        \"\"\"\n        super().__setstate__(state)\n        if hasattr(self, \"X_thresholds_\") and hasattr(self, \"y_thresholds_\"):\n            self._build_f(self.X_thresholds_, self.y_thresholds_)\n\n    def _more_tags(self):\n        return {\"X_types\": [\"1darray\"]}", "# Authors: Fabian Pedregosa <fabian@fseoane.net>\n#          Alexandre Gramfort <alexandre.gramfort@inria.fr>\n#          Nelle Varoquaux <nelle.varoquaux@gmail.com>\n# License: BSD 3 clause\n\nimport numpy as np\nfrom scipy import interpolate\nfrom scipy.stats import spearmanr\nfrom numbers import Real\nimport warnings\nimport math\n\nfrom .base import BaseEstimator, TransformerMixin, RegressorMixin\nfrom .utils import check_array, check_consistent_length\nfrom .utils.validation import _check_sample_weight, check_is_fitted\nfrom .utils._param_validation import Interval, StrOptions\nfrom ._isotonic import _inplace_contiguous_isotonic_regression, _make_unique\n\n\n__all__ = [\"check_increasing\", \"isotonic_regression\", \"IsotonicRegression\"]\n\n\ndef check_increasing(x, y):\n    \"\"\"Determine whether y is monotonically correlated with x.\n\n    y is found increasing or decreasing with respect to x based on a Spearman\n    correlation test.\n\n    Parameters\n    ----------\n    x : array-like of shape (n_samples,)\n            Training data.\n\n    y : array-like of shape (n_samples,)\n        Training target.\n\n    Returns\n    -------\n    increasing_bool : boolean\n        Whether the relationship is increasing or decreasing.\n\n    Notes\n    -----\n    The Spearman correlation coefficient is estimated from the data, and the\n    sign of the resulting estimate is used as the result.\n\n    In the event that the 95% confidence interval based on Fisher transform\n    spans zero, a warning is raised.\n\n    References\n    ----------\n    Fisher transformation. Wikipedia.\n    https://en.wikipedia.org/wiki/Fisher_transformation\n    \"\"\"\n\n    # Calculate Spearman rho estimate and set return accordingly.\n    rho, _ = spearmanr(x, y)\n    increasing_bool = rho >= 0\n\n    # Run Fisher transform to get the rho CI, but handle rho=+/-1\n    if rho not in [-1.0, 1.0] and len(x) > 3:\n        F = 0.5 * math.log((1.0 + rho) / (1.0 - rho))\n        F_se = 1 / math.sqrt(len(x) - 3)\n\n        # Use a 95% CI, i.e., +/-1.96 S.E.\n        # https://en.wikipedia.org/wiki/Fisher_transformation\n        rho_0 = math.tanh(F - 1.96 * F_se)\n        rho_1 = math.tanh(F + 1.96 * F_se)\n\n        # Warn if the CI spans zero.\n        if np.sign(rho_0) != np.sign(rho_1):\n            warnings.warn(\n                \"Confidence interval of the Spearman \"\n                \"correlation coefficient spans zero. \"\n                \"Determination of ``increasing`` may be \"\n                \"suspect.\"\n            )\n\n    return increasing_bool\n\n\ndef isotonic_regression(\n    y, *, sample_weight=None, y_min=None, y_max=None, increasing=True\n):\n    \"\"\"Solve the isotonic regression model.\n\n    Read more in the :ref:`User Guide <isotonic>`.\n\n    Parameters\n    ----------\n    y : array-like of shape (n_samples,)\n        The data.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Weights on each point of the regression.\n        If None, weight is set to 1 (equal weights).\n\n    y_min : float, default=None\n        Lower bound on the lowest predicted value (the minimum value may\n        still be higher). If not set, defaults to -inf.\n\n    y_max : float, default=None\n        Upper bound on the highest predicted value (the maximum may still be\n        lower). If not set, defaults to +inf.\n\n    increasing : bool, default=True\n        Whether to compute ``y_`` is increasing (if set to True) or decreasing\n        (if set to False).\n\n    Returns\n    -------\n    y_ : list of floats\n        Isotonic fit of y.\n\n    References\n    ----------\n    \"Active set algorithms for isotonic regression; A unifying framework\"\n    by Michael J. Best and Nilotpal Chakravarti, section 3.\n    \"\"\"\n    order = np.s_[:] if increasing else np.s_[::-1]\n    y = check_array(y, ensure_2d=False, input_name=\"y\", dtype=[np.float64, np.float32])\n    y = np.array(y[order], dtype=y.dtype)\n    sample_weight = _check_sample_weight(sample_weight, y, dtype=y.dtype, copy=True)\n    sample_weight = np.ascontiguousarray(sample_weight[order])\n\n    _inplace_contiguous_isotonic_regression(y, sample_weight)\n    if y_min is not None or y_max is not None:\n        # Older versions of np.clip don't accept None as a bound, so use np.inf\n        if y_min is None:\n            y_min = -np.inf\n        if y_max is None:\n            y_max = np.inf\n        np.clip(y, y_min, y_max, y)\n    return y[order]\n\n\nclass IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n    \"\"\"Isotonic regression model.\n\n    Read more in the :ref:`User Guide <isotonic>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    y_min : float, default=None\n        Lower bound on the lowest predicted value (the minimum value may\n        still be higher). If not set, defaults to -inf.\n\n    y_max : float, default=None\n        Upper bound on the highest predicted value (the maximum may still be\n        lower). If not set, defaults to +inf.\n\n    increasing : bool or 'auto', default=True\n        Determines whether the predictions should be constrained to increase\n        or decrease with `X`. 'auto' will decide based on the Spearman\n        correlation estimate's sign.\n\n    out_of_bounds : {'nan', 'clip', 'raise'}, default='nan'\n        Handles how `X` values outside of the training domain are handled\n        during prediction.\n\n        - 'nan', predictions will be NaN.\n        - 'clip', predictions will be set to the value corresponding to\n          the nearest train interval endpoint.\n        - 'raise', a `ValueError` is raised.\n\n    Attributes\n    ----------\n    X_min_ : float\n        Minimum value of input array `X_` for left bound.\n\n    X_max_ : float\n        Maximum value of input array `X_` for right bound.\n\n    X_thresholds_ : ndarray of shape (n_thresholds,)\n        Unique ascending `X` values used to interpolate\n        the y = f(X) monotonic function.\n\n        .. versionadded:: 0.24\n\n    y_thresholds_ : ndarray of shape (n_thresholds,)\n        De-duplicated `y` values suitable to interpolate the y = f(X)\n        monotonic function.\n\n        .. versionadded:: 0.24\n\n    f_ : function\n        The stepwise interpolating function that covers the input domain ``X``.\n\n    increasing_ : bool\n        Inferred value for ``increasing``.\n\n    See Also\n    --------\n    sklearn.linear_model.LinearRegression : Ordinary least squares Linear\n        Regression.\n    sklearn.ensemble.HistGradientBoostingRegressor : Gradient boosting that\n        is a non-parametric model accepting monotonicity constraints.\n    isotonic_regression : Function to solve the isotonic regression model.\n\n    Notes\n    -----\n    Ties are broken using the secondary method from de Leeuw, 1977.\n\n    References\n    ----------\n    Isotonic Median Regression: A Linear Programming Approach\n    Nilotpal Chakravarti\n    Mathematics of Operations Research\n    Vol. 14, No. 2 (May, 1989), pp. 303-308\n\n    Isotone Optimization in R : Pool-Adjacent-Violators\n    Algorithm (PAVA) and Active Set Methods\n    de Leeuw, Hornik, Mair\n    Journal of Statistical Software 2009\n\n    Correctness of Kruskal's algorithms for monotone regression with ties\n    de Leeuw, Psychometrica, 1977\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.isotonic import IsotonicRegression\n    >>> X, y = make_regression(n_samples=10, n_features=1, random_state=41)\n    >>> iso_reg = IsotonicRegression().fit(X, y)\n    >>> iso_reg.predict([.1, .2])\n    array([1.8628..., 3.7256...])\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"y_min\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"y_max\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"increasing\": [\"boolean\", StrOptions({\"auto\"})],\n        \"out_of_bounds\": [StrOptions({\"nan\", \"clip\", \"raise\"})],\n    }\n\n    def __init__(self, *, y_min=None, y_max=None, increasing=True, out_of_bounds=\"nan\"):\n        self.y_min = y_min\n        self.y_max = y_max\n        self.increasing = increasing\n        self.out_of_bounds = out_of_bounds\n\n    def _check_input_data_shape(self, X):\n        if not (X.ndim == 1 or (X.ndim == 2 and X.shape[1] == 1)):\n            msg = (\n                \"Isotonic regression input X should be a 1d array or \"\n                \"2d array with 1 feature\"\n            )\n            raise ValueError(msg)\n\n    def _build_f(self, X, y):\n        \"\"\"Build the f_ interp1d function.\"\"\"\n\n        bounds_error = self.out_of_bounds == \"raise\"\n        if len(y) == 1:\n            # single y, constant prediction\n            self.f_ = lambda x: y.repeat(x.shape)\n        else:\n            self.f_ = interpolate.interp1d(\n                X, y, kind=\"linear\", bounds_error=bounds_error\n            )\n\n    def _build_y(self, X, y, sample_weight, trim_duplicates=True):\n        \"\"\"Build the y_ IsotonicRegression.\"\"\"\n        self._check_input_data_shape(X)\n        X = X.reshape(-1)  # use 1d view\n\n        # Determine increasing if auto-determination requested\n        if self.increasing == \"auto\":\n            self.increasing_ = check_increasing(X, y)\n        else:\n            self.increasing_ = self.increasing\n\n        # If sample_weights is passed, removed zero-weight values and clean\n        # order\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        mask = sample_weight > 0\n        X, y, sample_weight = X[mask], y[mask], sample_weight[mask]\n\n        order = np.lexsort((y, X))\n        X, y, sample_weight = [array[order] for array in [X, y, sample_weight]]\n        unique_X, unique_y, unique_sample_weight = _make_unique(X, y, sample_weight)\n\n        X = unique_X\n        y = isotonic_regression(\n            unique_y,\n            sample_weight=unique_sample_weight,\n            y_min=self.y_min,\n            y_max=self.y_max,\n            increasing=self.increasing_,\n        )\n\n        # Handle the left and right bounds on X\n        self.X_min_, self.X_max_ = np.min(X), np.max(X)\n\n        if trim_duplicates:\n            # Remove unnecessary points for faster prediction\n            keep_data = np.ones((len(y),), dtype=bool)\n            # Aside from the 1st and last point, remove points whose y values\n            # are equal to both the point before and the point after it.\n            keep_data[1:-1] = np.logical_or(\n                np.not_equal(y[1:-1], y[:-2]), np.not_equal(y[1:-1], y[2:])\n            )\n            return X[keep_data], y[keep_data]\n        else:\n            # The ability to turn off trim_duplicates is only used to it make\n            # easier to unit test that removing duplicates in y does not have\n            # any impact the resulting interpolation function (besides\n            # prediction speed).\n            return X, y\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples,) or (n_samples, 1)\n            Training data.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        y : array-like of shape (n_samples,)\n            Training target.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Weights. If set to None, all weights will be set to 1 (equal\n            weights).\n\n        Returns\n        -------\n        self : object\n            Returns an instance of self.\n\n        Notes\n        -----\n        X is stored for future use, as :meth:`transform` needs X to interpolate\n        new input data.\n        \"\"\"\n        self._validate_params()\n        check_params = dict(accept_sparse=False, ensure_2d=False)\n        X = check_array(\n            X, input_name=\"X\", dtype=[np.float64, np.float32], **check_params\n        )\n        y = check_array(y, input_name=\"y\", dtype=X.dtype, **check_params)\n        check_consistent_length(X, y, sample_weight)\n\n        # Transform y by running the isotonic regression algorithm and\n        # transform X accordingly.\n        X, y = self._build_y(X, y, sample_weight)\n\n        # It is necessary to store the non-redundant part of the training set\n        # on the model to make it possible to support model persistence via\n        # the pickle module as the object built by scipy.interp1d is not\n        # picklable directly.\n        self.X_thresholds_, self.y_thresholds_ = X, y\n\n        # Build the interpolation function\n        self._build_f(X, y)\n        return self\n\n    def transform(self, T):\n        \"\"\"Transform new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            The transformed data.\n        \"\"\"\n\n        if hasattr(self, \"X_thresholds_\"):\n            dtype = self.X_thresholds_.dtype\n        else:\n            dtype = np.float64\n\n        T = check_array(T, dtype=dtype, ensure_2d=False)\n\n        self._check_input_data_shape(T)\n        T = T.reshape(-1)  # use 1d view\n\n        if self.out_of_bounds == \"clip\":\n            T = np.clip(T, self.X_min_, self.X_max_)\n\n        res = self.f_(T)\n\n        # on scipy 0.17, interp1d up-casts to float64, so we cast back\n        res = res.astype(T.dtype)\n\n        return res\n\n    def predict(self, T):\n        \"\"\"Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            Transformed data.\n        \"\"\"\n        return self.transform(T)\n\n    # We implement get_feature_names_out here instead of using\n    # `ClassNamePrefixFeaturesOutMixin`` because `input_features` are ignored.\n    # `input_features` are ignored because `IsotonicRegression` accepts 1d\n    # arrays and the semantics of `feature_names_in_` are not clear for 1d arrays.\n    def get_feature_names_out(self, input_features=None):\n        \"\"\"Get output feature names for transformation.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Ignored.\n\n        Returns\n        -------\n        feature_names_out : ndarray of str objects\n            An ndarray with one string i.e. [\"isotonicregression0\"].\n        \"\"\"\n        check_is_fitted(self, \"f_\")\n        class_name = self.__class__.__name__.lower()\n        return np.asarray([f\"{class_name}0\"], dtype=object)\n\n    def __getstate__(self):\n        \"\"\"Pickle-protocol - return state of the estimator.\"\"\"\n        state = super().__getstate__()\n        # remove interpolation method\n        state.pop(\"f_\", None)\n        return state\n\n    def __setstate__(self, state):\n        \"\"\"Pickle-protocol - set state of the estimator.\n\n        We need to rebuild the interpolation function.\n        \"\"\"\n        super().__setstate__(state)\n        if hasattr(self, \"X_thresholds_\") and hasattr(self, \"y_thresholds_\"):\n            self._build_f(self.X_thresholds_, self.y_thresholds_)\n\n    def _more_tags(self):\n        return {\"X_types\": [\"1darray\"]}", "# Authors: Fabian Pedregosa <fabian@fseoane.net>\n#          Alexandre Gramfort <alexandre.gramfort@inria.fr>\n#          Nelle Varoquaux <nelle.varoquaux@gmail.com>\n# License: BSD 3 clause\n\nimport numpy as np\nfrom scipy import interpolate\nfrom scipy.stats import spearmanr\nfrom numbers import Real\nimport warnings\nimport math\n\nfrom .base import BaseEstimator, TransformerMixin, RegressorMixin\nfrom .utils import check_array, check_consistent_length\nfrom .utils.validation import _check_sample_weight, check_is_fitted\nfrom .utils._param_validation import Interval, StrOptions\nfrom ._isotonic import _inplace_contiguous_isotonic_regression, _make_unique\n\n\n__all__ = [\"check_increasing\", \"isotonic_regression\", \"IsotonicRegression\"]\n\n\ndef check_increasing(x, y):\n    \"\"\"Determine whether y is monotonically correlated with x.\n\n    y is found increasing or decreasing with respect to x based on a Spearman\n    correlation test.\n\n    Parameters\n    ----------\n    x : array-like of shape (n_samples,)\n            Training data.\n\n    y : array-like of shape (n_samples,)\n        Training target.\n\n    Returns\n    -------\n    increasing_bool : boolean\n        Whether the relationship is increasing or decreasing.\n\n    Notes\n    -----\n    The Spearman correlation coefficient is estimated from the data, and the\n    sign of the resulting estimate is used as the result.\n\n    In the event that the 95% confidence interval based on Fisher transform\n    spans zero, a warning is raised.\n\n    References\n    ----------\n    Fisher transformation. Wikipedia.\n    https://en.wikipedia.org/wiki/Fisher_transformation\n    \"\"\"\n\n    # Calculate Spearman rho estimate and set return accordingly.\n    rho, _ = spearmanr(x, y)\n    increasing_bool = rho >= 0\n\n    # Run Fisher transform to get the rho CI, but handle rho=+/-1\n    if rho not in [-1.0, 1.0] and len(x) > 3:\n        F = 0.5 * math.log((1.0 + rho) / (1.0 - rho))\n        F_se = 1 / math.sqrt(len(x) - 3)\n\n        # Use a 95% CI, i.e., +/-1.96 S.E.\n        # https://en.wikipedia.org/wiki/Fisher_transformation\n        rho_0 = math.tanh(F - 1.96 * F_se)\n        rho_1 = math.tanh(F + 1.96 * F_se)\n\n        # Warn if the CI spans zero.\n        if np.sign(rho_0) != np.sign(rho_1):\n            warnings.warn(\n                \"Confidence interval of the Spearman \"\n                \"correlation coefficient spans zero. \"\n                \"Determination of ``increasing`` may be \"\n                \"suspect.\"\n            )\n\n    return increasing_bool\n\n\ndef isotonic_regression(\n    y, *, sample_weight=None, y_min=None, y_max=None, increasing=True\n):\n    \"\"\"Solve the isotonic regression model.\n\n    Read more in the :ref:`User Guide <isotonic>`.\n\n    Parameters\n    ----------\n    y : array-like of shape (n_samples,)\n        The data.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Weights on each point of the regression.\n        If None, weight is set to 1 (equal weights).\n\n    y_min : float, default=None\n        Lower bound on the lowest predicted value (the minimum value may\n        still be higher). If not set, defaults to -inf.\n\n    y_max : float, default=None\n        Upper bound on the highest predicted value (the maximum may still be\n        lower). If not set, defaults to +inf.\n\n    increasing : bool, default=True\n        Whether to compute ``y_`` is increasing (if set to True) or decreasing\n        (if set to False).\n\n    Returns\n    -------\n    y_ : list of floats\n        Isotonic fit of y.\n\n    References\n    ----------\n    \"Active set algorithms for isotonic regression; A unifying framework\"\n    by Michael J. Best and Nilotpal Chakravarti, section 3.\n    \"\"\"\n    order = np.s_[:] if increasing else np.s_[::-1]\n    y = check_array(y, ensure_2d=False, input_name=\"y\", dtype=[np.float64, np.float32])\n    y = np.array(y[order], dtype=y.dtype)\n    sample_weight = _check_sample_weight(sample_weight, y, dtype=y.dtype, copy=True)\n    sample_weight = np.ascontiguousarray(sample_weight[order])\n\n    _inplace_contiguous_isotonic_regression(y, sample_weight)\n    if y_min is not None or y_max is not None:\n        # Older versions of np.clip don't accept None as a bound, so use np.inf\n        if y_min is None:\n            y_min = -np.inf\n        if y_max is None:\n            y_max = np.inf\n        np.clip(y, y_min, y_max, y)\n    return y[order]\n\n\nclass IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n    \"\"\"Isotonic regression model.\n\n    Read more in the :ref:`User Guide <isotonic>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    y_min : float, default=None\n        Lower bound on the lowest predicted value (the minimum value may\n        still be higher). If not set, defaults to -inf.\n\n    y_max : float, default=None\n        Upper bound on the highest predicted value (the maximum may still be\n        lower). If not set, defaults to +inf.\n\n    increasing : bool or 'auto', default=True\n        Determines whether the predictions should be constrained to increase\n        or decrease with `X`. 'auto' will decide based on the Spearman\n        correlation estimate's sign.\n\n    out_of_bounds : {'nan', 'clip', 'raise'}, default='nan'\n        Handles how `X` values outside of the training domain are handled\n        during prediction.\n\n        - 'nan', predictions will be NaN.\n        - 'clip', predictions will be set to the value corresponding to\n          the nearest train interval endpoint.\n        - 'raise', a `ValueError` is raised.\n\n    Attributes\n    ----------\n    X_min_ : float\n        Minimum value of input array `X_` for left bound.\n\n    X_max_ : float\n        Maximum value of input array `X_` for right bound.\n\n    X_thresholds_ : ndarray of shape (n_thresholds,)\n        Unique ascending `X` values used to interpolate\n        the y = f(X) monotonic function.\n\n        .. versionadded:: 0.24\n\n    y_thresholds_ : ndarray of shape (n_thresholds,)\n        De-duplicated `y` values suitable to interpolate the y = f(X)\n        monotonic function.\n\n        .. versionadded:: 0.24\n\n    f_ : function\n        The stepwise interpolating function that covers the input domain ``X``.\n\n    increasing_ : bool\n        Inferred value for ``increasing``.\n\n    See Also\n    --------\n    sklearn.linear_model.LinearRegression : Ordinary least squares Linear\n        Regression.\n    sklearn.ensemble.HistGradientBoostingRegressor : Gradient boosting that\n        is a non-parametric model accepting monotonicity constraints.\n    isotonic_regression : Function to solve the isotonic regression model.\n\n    Notes\n    -----\n    Ties are broken using the secondary method from de Leeuw, 1977.\n\n    References\n    ----------\n    Isotonic Median Regression: A Linear Programming Approach\n    Nilotpal Chakravarti\n    Mathematics of Operations Research\n    Vol. 14, No. 2 (May, 1989), pp. 303-308\n\n    Isotone Optimization in R : Pool-Adjacent-Violators\n    Algorithm (PAVA) and Active Set Methods\n    de Leeuw, Hornik, Mair\n    Journal of Statistical Software 2009\n\n    Correctness of Kruskal's algorithms for monotone regression with ties\n    de Leeuw, Psychometrica, 1977\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.isotonic import IsotonicRegression\n    >>> X, y = make_regression(n_samples=10, n_features=1, random_state=41)\n    >>> iso_reg = IsotonicRegression().fit(X, y)\n    >>> iso_reg.predict([.1, .2])\n    array([1.8628..., 3.7256...])\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"y_min\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"y_max\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"increasing\": [\"boolean\", StrOptions({\"auto\"})],\n        \"out_of_bounds\": [StrOptions({\"nan\", \"clip\", \"raise\"})],\n    }\n\n    def __init__(self, *, y_min=None, y_max=None, increasing=True, out_of_bounds=\"nan\"):\n        self.y_min = y_min\n        self.y_max = y_max\n        self.increasing = increasing\n        self.out_of_bounds = out_of_bounds\n\n    def _check_input_data_shape(self, X):\n        if not (X.ndim == 1 or (X.ndim == 2 and X.shape[1] == 1)):\n            msg = (\n                \"Isotonic regression input X should be a 1d array or \"\n                \"2d array with 1 feature\"\n            )\n            raise ValueError(msg)\n\n    def _build_f(self, X, y):\n        \"\"\"Build the f_ interp1d function.\"\"\"\n\n        bounds_error = self.out_of_bounds == \"raise\"\n        if len(y) == 1:\n            # single y, constant prediction\n            self.f_ = lambda x: y.repeat(x.shape)\n        else:\n            self.f_ = interpolate.interp1d(\n                X, y, kind=\"linear\", bounds_error=bounds_error\n            )\n\n    def _build_y(self, X, y, sample_weight, trim_duplicates=True):\n        \"\"\"Build the y_ IsotonicRegression.\"\"\"\n        self._check_input_data_shape(X)\n        X = X.reshape(-1)  # use 1d view\n\n        # Determine increasing if auto-determination requested\n        if self.increasing == \"auto\":\n            self.increasing_ = check_increasing(X, y)\n        else:\n            self.increasing_ = self.increasing\n\n        # If sample_weights is passed, removed zero-weight values and clean\n        # order\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        mask = sample_weight > 0\n        X, y, sample_weight = X[mask], y[mask], sample_weight[mask]\n\n        order = np.lexsort((y, X))\n        X, y, sample_weight = [array[order] for array in [X, y, sample_weight]]\n        unique_X, unique_y, unique_sample_weight = _make_unique(X, y, sample_weight)\n\n        X = unique_X\n        y = isotonic_regression(\n            unique_y,\n            sample_weight=unique_sample_weight,\n            y_min=self.y_min,\n            y_max=self.y_max,\n            increasing=self.increasing_,\n        )\n\n        # Handle the left and right bounds on X\n        self.X_min_, self.X_max_ = np.min(X), np.max(X)\n\n        if trim_duplicates:\n            # Remove unnecessary points for faster prediction\n            keep_data = np.ones((len(y),), dtype=bool)\n            # Aside from the 1st and last point, remove points whose y values\n            # are equal to both the point before and the point after it.\n            keep_data[1:-1] = np.logical_or(\n                np.not_equal(y[1:-1], y[:-2]), np.not_equal(y[1:-1], y[2:])\n            )\n            return X[keep_data], y[keep_data]\n        else:\n            # The ability to turn off trim_duplicates is only used to it make\n            # easier to unit test that removing duplicates in y does not have\n            # any impact the resulting interpolation function (besides\n            # prediction speed).\n            return X, y\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples,) or (n_samples, 1)\n            Training data.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        y : array-like of shape (n_samples,)\n            Training target.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Weights. If set to None, all weights will be set to 1 (equal\n            weights).\n\n        Returns\n        -------\n        self : object\n            Returns an instance of self.\n\n        Notes\n        -----\n        X is stored for future use, as :meth:`transform` needs X to interpolate\n        new input data.\n        \"\"\"\n        self._validate_params()\n        check_params = dict(accept_sparse=False, ensure_2d=False)\n        X = check_array(\n            X, input_name=\"X\", dtype=[np.float64, np.float32], **check_params\n        )\n        y = check_array(y, input_name=\"y\", dtype=X.dtype, **check_params)\n        check_consistent_length(X, y, sample_weight)\n\n        # Transform y by running the isotonic regression algorithm and\n        # transform X accordingly.\n        X, y = self._build_y(X, y, sample_weight)\n\n        # It is necessary to store the non-redundant part of the training set\n        # on the model to make it possible to support model persistence via\n        # the pickle module as the object built by scipy.interp1d is not\n        # picklable directly.\n        self.X_thresholds_, self.y_thresholds_ = X, y\n\n        # Build the interpolation function\n        self._build_f(X, y)\n        return self\n\n    def transform(self, T):\n        \"\"\"Transform new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            The transformed data.\n        \"\"\"\n\n        if hasattr(self, \"X_thresholds_\"):\n            dtype = self.X_thresholds_.dtype\n        else:\n            dtype = np.float64\n\n        T = check_array(T, dtype=dtype, ensure_2d=False)\n\n        self._check_input_data_shape(T)\n        T = T.reshape(-1)  # use 1d view\n\n        if self.out_of_bounds == \"clip\":\n            T = np.clip(T, self.X_min_, self.X_max_)\n\n        res = self.f_(T)\n\n        # on scipy 0.17, interp1d up-casts to float64, so we cast back\n        res = res.astype(T.dtype)\n\n        return res\n\n    def predict(self, T):\n        \"\"\"Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            Transformed data.\n        \"\"\"\n        return self.transform(T)\n\n    # We implement get_feature_names_out here instead of using\n    # `ClassNamePrefixFeaturesOutMixin`` because `input_features` are ignored.\n    # `input_features` are ignored because `IsotonicRegression` accepts 1d\n    # arrays and the semantics of `feature_names_in_` are not clear for 1d arrays.\n    def get_feature_names_out(self, input_features=None):\n        \"\"\"Get output feature names for transformation.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Ignored.\n\n        Returns\n        -------\n        feature_names_out : ndarray of str objects\n            An ndarray with one string i.e. [\"isotonicregression0\"].\n        \"\"\"\n        check_is_fitted(self, \"f_\")\n        class_name = self.__class__.__name__.lower()\n        return np.asarray([f\"{class_name}0\"], dtype=object)\n\n    def __getstate__(self):\n        \"\"\"Pickle-protocol - return state of the estimator.\"\"\"\n        state = super().__getstate__()\n        # remove interpolation method\n        state.pop(\"f_\", None)\n        return state\n\n    def __setstate__(self, state):\n        \"\"\"Pickle-protocol - set state of the estimator.\n\n        We need to rebuild the interpolation function.\n        \"\"\"\n        super().__setstate__(state)\n        if hasattr(self, \"X_thresholds_\") and hasattr(self, \"y_thresholds_\"):\n            self._build_f(self.X_thresholds_, self.y_thresholds_)\n\n    def _more_tags(self):\n        return {\"X_types\": [\"1darray\"]}", "\"\"\"Calibration of predicted probabilities.\"\"\"\n\n# Author: Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>\n#         Balazs Kegl <balazs.kegl@gmail.com>\n#         Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n#         Mathieu Blondel <mathieu@mblondel.org>\n#\n# License: BSD 3 clause\n\nfrom numbers import Integral\nimport warnings\nfrom inspect import signature\nfrom functools import partial\n\nfrom math import log\nimport numpy as np\n\nfrom scipy.special import expit\nfrom scipy.special import xlogy\nfrom scipy.optimize import fmin_bfgs\n\nfrom .base import (\n    BaseEstimator,\n    ClassifierMixin,\n    RegressorMixin,\n    clone,\n    MetaEstimatorMixin,\n    is_classifier,\n)\nfrom .preprocessing import label_binarize, LabelEncoder\nfrom .utils import (\n    column_or_1d,\n    indexable,\n    check_matplotlib_support,\n)\n\nfrom .utils.multiclass import check_classification_targets\nfrom .utils.parallel import delayed, Parallel\nfrom .utils._param_validation import StrOptions, HasMethods, Hidden\nfrom .utils.validation import (\n    _check_fit_params,\n    _check_sample_weight,\n    _num_samples,\n    check_consistent_length,\n    check_is_fitted,\n)\nfrom .utils import _safe_indexing\nfrom .isotonic import IsotonicRegression\nfrom .svm import LinearSVC\nfrom .model_selection import check_cv, cross_val_predict\nfrom .metrics._base import _check_pos_label_consistency\nfrom .metrics._plot.base import _get_response\n\n\nclass CalibratedClassifierCV(ClassifierMixin, MetaEstimatorMixin, BaseEstimator):\n    \"\"\"Probability calibration with isotonic regression or logistic regression.\n\n    This class uses cross-validation to both estimate the parameters of a\n    classifier and subsequently calibrate a classifier. With default\n    `ensemble=True`, for each cv split it\n    fits a copy of the base estimator to the training subset, and calibrates it\n    using the testing subset. For prediction, predicted probabilities are\n    averaged across these individual calibrated classifiers. When\n    `ensemble=False`, cross-validation is used to obtain unbiased predictions,\n    via :func:`~sklearn.model_selection.cross_val_predict`, which are then\n    used for calibration. For prediction, the base estimator, trained using all\n    the data, is used. This is the method implemented when `probabilities=True`\n    for :mod:`sklearn.svm` estimators.\n\n    Already fitted classifiers can be calibrated via the parameter\n    `cv=\"prefit\"`. In this case, no cross-validation is used and all provided\n    data is used for calibration. The user has to take care manually that data\n    for model fitting and calibration are disjoint.\n\n    The calibration is based on the :term:`decision_function` method of the\n    `estimator` if it exists, else on :term:`predict_proba`.\n\n    Read more in the :ref:`User Guide <calibration>`.\n\n    Parameters\n    ----------\n    estimator : estimator instance, default=None\n        The classifier whose output need to be calibrated to provide more\n        accurate `predict_proba` outputs. The default classifier is\n        a :class:`~sklearn.svm.LinearSVC`.\n\n        .. versionadded:: 1.2\n\n    method : {'sigmoid', 'isotonic'}, default='sigmoid'\n        The method to use for calibration. Can be 'sigmoid' which\n        corresponds to Platt's method (i.e. a logistic regression model) or\n        'isotonic' which is a non-parametric approach. It is not advised to\n        use isotonic calibration with too few calibration samples\n        ``(<<1000)`` since it tends to overfit.\n\n    cv : int, cross-validation generator, iterable or \"prefit\", \\\n            default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 5-fold cross-validation,\n        - integer, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if ``y`` is binary or multiclass,\n        :class:`~sklearn.model_selection.StratifiedKFold` is used. If ``y`` is\n        neither binary nor multiclass, :class:`~sklearn.model_selection.KFold`\n        is used.\n\n        Refer to the :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        If \"prefit\" is passed, it is assumed that `estimator` has been\n        fitted already and all data is used for calibration.\n\n        .. versionchanged:: 0.22\n            ``cv`` default value if None changed from 3-fold to 5-fold.\n\n    n_jobs : int, default=None\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors.\n\n        Base estimator clones are fitted in parallel across cross-validation\n        iterations. Therefore parallelism happens only when `cv != \"prefit\"`.\n\n        See :term:`Glossary <n_jobs>` for more details.\n\n        .. versionadded:: 0.24\n\n    ensemble : bool, default=True\n        Determines how the calibrator is fitted when `cv` is not `'prefit'`.\n        Ignored if `cv='prefit'`.\n\n        If `True`, the `estimator` is fitted using training data, and\n        calibrated using testing data, for each `cv` fold. The final estimator\n        is an ensemble of `n_cv` fitted classifier and calibrator pairs, where\n        `n_cv` is the number of cross-validation folds. The output is the\n        average predicted probabilities of all pairs.\n\n        If `False`, `cv` is used to compute unbiased predictions, via\n        :func:`~sklearn.model_selection.cross_val_predict`, which are then\n        used for calibration. At prediction time, the classifier used is the\n        `estimator` trained on all the data.\n        Note that this method is also internally implemented  in\n        :mod:`sklearn.svm` estimators with the `probabilities=True` parameter.\n\n        .. versionadded:: 0.24\n\n    base_estimator : estimator instance\n        This parameter is deprecated. Use `estimator` instead.\n\n        .. deprecated:: 1.2\n           The parameter `base_estimator` is deprecated in 1.2 and will be\n           removed in 1.4. Use `estimator` instead.\n\n    Attributes\n    ----------\n    classes_ : ndarray of shape (n_classes,)\n        The class labels.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`. Only defined if the\n        underlying estimator exposes such an attribute when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Only defined if the\n        underlying estimator exposes such an attribute when fit.\n\n        .. versionadded:: 1.0\n\n    calibrated_classifiers_ : list (len() equal to cv or 1 if `cv=\"prefit\"` \\\n            or `ensemble=False`)\n        The list of classifier and calibrator pairs.\n\n        - When `cv=\"prefit\"`, the fitted `estimator` and fitted\n          calibrator.\n        - When `cv` is not \"prefit\" and `ensemble=True`, `n_cv` fitted\n          `estimator` and calibrator pairs. `n_cv` is the number of\n          cross-validation folds.\n        - When `cv` is not \"prefit\" and `ensemble=False`, the `estimator`,\n          fitted on all the data, and fitted calibrator.\n\n        .. versionchanged:: 0.24\n            Single calibrated classifier case when `ensemble=False`.\n\n    See Also\n    --------\n    calibration_curve : Compute true and predicted probabilities\n        for a calibration curve.\n\n    References\n    ----------\n    .. [1] Obtaining calibrated probability estimates from decision trees\n           and naive Bayesian classifiers, B. Zadrozny & C. Elkan, ICML 2001\n\n    .. [2] Transforming Classifier Scores into Accurate Multiclass\n           Probability Estimates, B. Zadrozny & C. Elkan, (KDD 2002)\n\n    .. [3] Probabilistic Outputs for Support Vector Machines and Comparisons to\n           Regularized Likelihood Methods, J. Platt, (1999)\n\n    .. [4] Predicting Good Probabilities with Supervised Learning,\n           A. Niculescu-Mizil & R. Caruana, ICML 2005\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_classification\n    >>> from sklearn.naive_bayes import GaussianNB\n    >>> from sklearn.calibration import CalibratedClassifierCV\n    >>> X, y = make_classification(n_samples=100, n_features=2,\n    ...                            n_redundant=0, random_state=42)\n    >>> base_clf = GaussianNB()\n    >>> calibrated_clf = CalibratedClassifierCV(base_clf, cv=3)\n    >>> calibrated_clf.fit(X, y)\n    CalibratedClassifierCV(...)\n    >>> len(calibrated_clf.calibrated_classifiers_)\n    3\n    >>> calibrated_clf.predict_proba(X)[:5, :]\n    array([[0.110..., 0.889...],\n           [0.072..., 0.927...],\n           [0.928..., 0.071...],\n           [0.928..., 0.071...],\n           [0.071..., 0.928...]])\n    >>> from sklearn.model_selection import train_test_split\n    >>> X, y = make_classification(n_samples=100, n_features=2,\n    ...                            n_redundant=0, random_state=42)\n    >>> X_train, X_calib, y_train, y_calib = train_test_split(\n    ...        X, y, random_state=42\n    ... )\n    >>> base_clf = GaussianNB()\n    >>> base_clf.fit(X_train, y_train)\n    GaussianNB()\n    >>> calibrated_clf = CalibratedClassifierCV(base_clf, cv=\"prefit\")\n    >>> calibrated_clf.fit(X_calib, y_calib)\n    CalibratedClassifierCV(...)\n    >>> len(calibrated_clf.calibrated_classifiers_)\n    1\n    >>> calibrated_clf.predict_proba([[-0.5, 0.5]])\n    array([[0.936..., 0.063...]])\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"estimator\": [\n            HasMethods([\"fit\", \"predict_proba\"]),\n            HasMethods([\"fit\", \"decision_function\"]),\n            None,\n        ],\n        \"method\": [StrOptions({\"isotonic\", \"sigmoid\"})],\n        \"cv\": [\"cv_object\", StrOptions({\"prefit\"})],\n        \"n_jobs\": [Integral, None],\n        \"ensemble\": [\"boolean\"],\n        \"base_estimator\": [\n            HasMethods([\"fit\", \"predict_proba\"]),\n            HasMethods([\"fit\", \"decision_function\"]),\n            None,\n            Hidden(StrOptions({\"deprecated\"})),\n        ],\n    }\n\n    def __init__(\n        self,\n        estimator=None,\n        *,\n        method=\"sigmoid\",\n        cv=None,\n        n_jobs=None,\n        ensemble=True,\n        base_estimator=\"deprecated\",\n    ):\n        self.estimator = estimator\n        self.method = method\n        self.cv = cv\n        self.n_jobs = n_jobs\n        self.ensemble = ensemble\n        self.base_estimator = base_estimator\n\n    def fit(self, X, y, sample_weight=None, **fit_params):\n        \"\"\"Fit the calibrated model.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted.\n\n        **fit_params : dict\n            Parameters to pass to the `fit` method of the underlying\n            classifier.\n\n        Returns\n        -------\n        self : object\n            Returns an instance of self.\n        \"\"\"\n        self._validate_params()\n\n        check_classification_targets(y)\n        X, y = indexable(X, y)\n        if sample_weight is not None:\n            sample_weight = _check_sample_weight(sample_weight, X)\n\n        for sample_aligned_params in fit_params.values():\n            check_consistent_length(y, sample_aligned_params)\n\n        # TODO(1.4): Remove when base_estimator is removed\n        if self.base_estimator != \"deprecated\":\n            if self.estimator is not None:\n                raise ValueError(\n                    \"Both `base_estimator` and `estimator` are set. Only set \"\n                    \"`estimator` since `base_estimator` is deprecated.\"\n                )\n            warnings.warn(\n                \"`base_estimator` was renamed to `estimator` in version 1.2 and \"\n                \"will be removed in 1.4.\",\n                FutureWarning,\n            )\n            estimator = self.base_estimator\n        else:\n            estimator = self.estimator\n\n        if estimator is None:\n            # we want all classifiers that don't expose a random_state\n            # to be deterministic (and we don't want to expose this one).\n            estimator = LinearSVC(random_state=0)\n\n        self.calibrated_classifiers_ = []\n        if self.cv == \"prefit\":\n            # `classes_` should be consistent with that of estimator\n            check_is_fitted(self.estimator, attributes=[\"classes_\"])\n            self.classes_ = self.estimator.classes_\n\n            pred_method, method_name = _get_prediction_method(estimator)\n            n_classes = len(self.classes_)\n            predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n\n            calibrated_classifier = _fit_calibrator(\n                estimator,\n                predictions,\n                y,\n                self.classes_,\n                self.method,\n                sample_weight,\n            )\n            self.calibrated_classifiers_.append(calibrated_classifier)\n        else:\n            # Set `classes_` using all `y`\n            label_encoder_ = LabelEncoder().fit(y)\n            self.classes_ = label_encoder_.classes_\n            n_classes = len(self.classes_)\n\n            # sample_weight checks\n            fit_parameters = signature(estimator.fit).parameters\n            supports_sw = \"sample_weight\" in fit_parameters\n            if sample_weight is not None and not supports_sw:\n                estimator_name = type(estimator).__name__\n                warnings.warn(\n                    f\"Since {estimator_name} does not appear to accept sample_weight, \"\n                    \"sample weights will only be used for the calibration itself. This \"\n                    \"can be caused by a limitation of the current scikit-learn API. \"\n                    \"See the following issue for more details: \"\n                    \"https://github.com/scikit-learn/scikit-learn/issues/21134. Be \"\n                    \"warned that the result of the calibration is likely to be \"\n                    \"incorrect.\"\n                )\n\n            # Check that each cross-validation fold can have at least one\n            # example per class\n            if isinstance(self.cv, int):\n                n_folds = self.cv\n            elif hasattr(self.cv, \"n_splits\"):\n                n_folds = self.cv.n_splits\n            else:\n                n_folds = None\n            if n_folds and np.any(\n                [np.sum(y == class_) < n_folds for class_ in self.classes_]\n            ):\n                raise ValueError(\n                    f\"Requesting {n_folds}-fold \"\n                    \"cross-validation but provided less than \"\n                    f\"{n_folds} examples for at least one class.\"\n                )\n            cv = check_cv(self.cv, y, classifier=True)\n\n            if self.ensemble:\n                parallel = Parallel(n_jobs=self.n_jobs)\n                self.calibrated_classifiers_ = parallel(\n                    delayed(_fit_classifier_calibrator_pair)(\n                        clone(estimator),\n                        X,\n                        y,\n                        train=train,\n                        test=test,\n                        method=self.method,\n                        classes=self.classes_,\n                        supports_sw=supports_sw,\n                        sample_weight=sample_weight,\n                        **fit_params,\n                    )\n                    for train, test in cv.split(X, y)\n                )\n            else:\n                this_estimator = clone(estimator)\n                _, method_name = _get_prediction_method(this_estimator)\n                fit_params = (\n                    {\"sample_weight\": sample_weight}\n                    if sample_weight is not None and supports_sw\n                    else None\n                )\n                pred_method = partial(\n                    cross_val_predict,\n                    estimator=this_estimator,\n                    X=X,\n                    y=y,\n                    cv=cv,\n                    method=method_name,\n                    n_jobs=self.n_jobs,\n                    fit_params=fit_params,\n                )\n                predictions = _compute_predictions(\n                    pred_method, method_name, X, n_classes\n                )\n\n                if sample_weight is not None and supports_sw:\n                    this_estimator.fit(X, y, sample_weight=sample_weight)\n                else:\n                    this_estimator.fit(X, y)\n                # Note: Here we don't pass on fit_params because the supported\n                # calibrators don't support fit_params anyway\n                calibrated_classifier = _fit_calibrator(\n                    this_estimator,\n                    predictions,\n                    y,\n                    self.classes_,\n                    self.method,\n                    sample_weight,\n                )\n                self.calibrated_classifiers_.append(calibrated_classifier)\n\n        first_clf = self.calibrated_classifiers_[0].estimator\n        if hasattr(first_clf, \"n_features_in_\"):\n            self.n_features_in_ = first_clf.n_features_in_\n        if hasattr(first_clf, \"feature_names_in_\"):\n            self.feature_names_in_ = first_clf.feature_names_in_\n        return self\n\n    def predict_proba(self, X):\n        \"\"\"Calibrated probabilities of classification.\n\n        This function returns calibrated probabilities of classification\n        according to each class on an array of test vectors X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The samples, as accepted by `estimator.predict_proba`.\n\n        Returns\n        -------\n        C : ndarray of shape (n_samples, n_classes)\n            The predicted probas.\n        \"\"\"\n        check_is_fitted(self)\n        # Compute the arithmetic mean of the predictions of the calibrated\n        # classifiers\n        mean_proba = np.zeros((_num_samples(X), len(self.classes_)))\n        for calibrated_classifier in self.calibrated_classifiers_:\n            proba = calibrated_classifier.predict_proba(X)\n            mean_proba += proba\n\n        mean_proba /= len(self.calibrated_classifiers_)\n\n        return mean_proba\n\n    def predict(self, X):\n        \"\"\"Predict the target of new samples.\n\n        The predicted class is the class that has the highest probability,\n        and can thus be different from the prediction of the uncalibrated classifier.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The samples, as accepted by `estimator.predict`.\n\n        Returns\n        -------\n        C : ndarray of shape (n_samples,)\n            The predicted class.\n        \"\"\"\n        check_is_fitted(self)\n        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n\n    def _more_tags(self):\n        return {\n            \"_xfail_checks\": {\n                \"check_sample_weights_invariance\": (\n                    \"Due to the cross-validation and sample ordering, removing a sample\"\n                    \" is not strictly equal to putting is weight to zero. Specific unit\"\n                    \" tests are added for CalibratedClassifierCV specifically.\"\n                ),\n            }\n        }\n\n\ndef _fit_classifier_calibrator_pair(\n    estimator,\n    X,\n    y,\n    train,\n    test,\n    supports_sw,\n    method,\n    classes,\n    sample_weight=None,\n    **fit_params,\n):\n    \"\"\"Fit a classifier/calibration pair on a given train/test split.\n\n    Fit the classifier on the train set, compute its predictions on the test\n    set and use the predictions as input to fit the calibrator along with the\n    test labels.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Cloned base estimator.\n\n    X : array-like, shape (n_samples, n_features)\n        Sample data.\n\n    y : array-like, shape (n_samples,)\n        Targets.\n\n    train : ndarray, shape (n_train_indices,)\n        Indices of the training subset.\n\n    test : ndarray, shape (n_test_indices,)\n        Indices of the testing subset.\n\n    supports_sw : bool\n        Whether or not the `estimator` supports sample weights.\n\n    method : {'sigmoid', 'isotonic'}\n        Method to use for calibration.\n\n    classes : ndarray, shape (n_classes,)\n        The target classes.\n\n    sample_weight : array-like, default=None\n        Sample weights for `X`.\n\n    **fit_params : dict\n        Parameters to pass to the `fit` method of the underlying\n        classifier.\n\n    Returns\n    -------\n    calibrated_classifier : _CalibratedClassifier instance\n    \"\"\"\n    fit_params_train = _check_fit_params(X, fit_params, train)\n    X_train, y_train = _safe_indexing(X, train), _safe_indexing(y, train)\n    X_test, y_test = _safe_indexing(X, test), _safe_indexing(y, test)\n\n    if sample_weight is not None and supports_sw:\n        sw_train = _safe_indexing(sample_weight, train)\n        estimator.fit(X_train, y_train, sample_weight=sw_train, **fit_params_train)\n    else:\n        estimator.fit(X_train, y_train, **fit_params_train)\n\n    n_classes = len(classes)\n    pred_method, method_name = _get_prediction_method(estimator)\n    predictions = _compute_predictions(pred_method, method_name, X_test, n_classes)\n\n    sw_test = None if sample_weight is None else _safe_indexing(sample_weight, test)\n    calibrated_classifier = _fit_calibrator(\n        estimator, predictions, y_test, classes, method, sample_weight=sw_test\n    )\n    return calibrated_classifier\n\n\ndef _get_prediction_method(clf):\n    \"\"\"Return prediction method.\n\n    `decision_function` method of `clf` returned, if it\n    exists, otherwise `predict_proba` method returned.\n\n    Parameters\n    ----------\n    clf : Estimator instance\n        Fitted classifier to obtain the prediction method from.\n\n    Returns\n    -------\n    prediction_method : callable\n        The prediction method.\n    method_name : str\n        The name of the prediction method.\n    \"\"\"\n    if hasattr(clf, \"decision_function\"):\n        method = getattr(clf, \"decision_function\")\n        return method, \"decision_function\"\n\n    if hasattr(clf, \"predict_proba\"):\n        method = getattr(clf, \"predict_proba\")\n        return method, \"predict_proba\"\n\n\ndef _compute_predictions(pred_method, method_name, X, n_classes):\n    \"\"\"Return predictions for `X` and reshape binary outputs to shape\n    (n_samples, 1).\n\n    Parameters\n    ----------\n    pred_method : callable\n        Prediction method.\n\n    method_name: str\n        Name of the prediction method\n\n    X : array-like or None\n        Data used to obtain predictions.\n\n    n_classes : int\n        Number of classes present.\n\n    Returns\n    -------\n    predictions : array-like, shape (X.shape[0], len(clf.classes_))\n        The predictions. Note if there are 2 classes, array is of shape\n        (X.shape[0], 1).\n    \"\"\"\n    predictions = pred_method(X=X)\n\n    if method_name == \"decision_function\":\n        if predictions.ndim == 1:\n            predictions = predictions[:, np.newaxis]\n    elif method_name == \"predict_proba\":\n        if n_classes == 2:\n            predictions = predictions[:, 1:]\n    else:  # pragma: no cover\n        # this branch should be unreachable.\n        raise ValueError(f\"Invalid prediction method: {method_name}\")\n    return predictions\n\n\ndef _fit_calibrator(clf, predictions, y, classes, method, sample_weight=None):\n    \"\"\"Fit calibrator(s) and return a `_CalibratedClassifier`\n    instance.\n\n    `n_classes` (i.e. `len(clf.classes_)`) calibrators are fitted.\n    However, if `n_classes` equals 2, one calibrator is fitted.\n\n    Parameters\n    ----------\n    clf : estimator instance\n        Fitted classifier.\n\n    predictions : array-like, shape (n_samples, n_classes) or (n_samples, 1) \\\n                    when binary.\n        Raw predictions returned by the un-calibrated base classifier.\n\n    y : array-like, shape (n_samples,)\n        The targets.\n\n    classes : ndarray, shape (n_classes,)\n        All the prediction classes.\n\n    method : {'sigmoid', 'isotonic'}\n        The method to use for calibration.\n\n    sample_weight : ndarray, shape (n_samples,), default=None\n        Sample weights. If None, then samples are equally weighted.\n\n    Returns\n    -------\n    pipeline : _CalibratedClassifier instance\n    \"\"\"\n    Y = label_binarize(y, classes=classes)\n    label_encoder = LabelEncoder().fit(classes)\n    pos_class_indices = label_encoder.transform(clf.classes_)\n    calibrators = []\n    for class_idx, this_pred in zip(pos_class_indices, predictions.T):\n        if method == \"isotonic\":\n            calibrator = IsotonicRegression(out_of_bounds=\"clip\")\n        else:  # \"sigmoid\"\n            calibrator = _SigmoidCalibration()\n        calibrator.fit(this_pred, Y[:, class_idx], sample_weight)\n        calibrators.append(calibrator)\n\n    pipeline = _CalibratedClassifier(clf, calibrators, method=method, classes=classes)\n    return pipeline\n\n\nclass _CalibratedClassifier:\n    \"\"\"Pipeline-like chaining a fitted classifier and its fitted calibrators.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Fitted classifier.\n\n    calibrators : list of fitted estimator instances\n        List of fitted calibrators (either 'IsotonicRegression' or\n        '_SigmoidCalibration'). The number of calibrators equals the number of\n        classes. However, if there are 2 classes, the list contains only one\n        fitted calibrator.\n\n    classes : array-like of shape (n_classes,)\n        All the prediction classes.\n\n    method : {'sigmoid', 'isotonic'}, default='sigmoid'\n        The method to use for calibration. Can be 'sigmoid' which\n        corresponds to Platt's method or 'isotonic' which is a\n        non-parametric approach based on isotonic regression.\n    \"\"\"\n\n    def __init__(self, estimator, calibrators, *, classes, method=\"sigmoid\"):\n        self.estimator = estimator\n        self.calibrators = calibrators\n        self.classes = classes\n        self.method = method\n\n    def predict_proba(self, X):\n        \"\"\"Calculate calibrated probabilities.\n\n        Calculates classification calibrated probabilities\n        for each class, in a one-vs-all manner, for `X`.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            The sample data.\n\n        Returns\n        -------\n        proba : array, shape (n_samples, n_classes)\n            The predicted probabilities. Can be exact zeros.\n        \"\"\"\n        n_classes = len(self.classes)\n        pred_method, method_name = _get_prediction_method(self.estimator)\n        predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n\n        label_encoder = LabelEncoder().fit(self.classes)\n        pos_class_indices = label_encoder.transform(self.estimator.classes_)\n\n        proba = np.zeros((_num_samples(X), n_classes))\n        for class_idx, this_pred, calibrator in zip(\n            pos_class_indices, predictions.T, self.calibrators\n        ):\n            if n_classes == 2:\n                # When binary, `predictions` consists only of predictions for\n                # clf.classes_[1] but `pos_class_indices` = 0\n                class_idx += 1\n            proba[:, class_idx] = calibrator.predict(this_pred)\n\n        # Normalize the probabilities\n        if n_classes == 2:\n            proba[:, 0] = 1.0 - proba[:, 1]\n        else:\n            denominator = np.sum(proba, axis=1)[:, np.newaxis]\n            # In the edge case where for each class calibrator returns a null\n            # probability for a given sample, use the uniform distribution\n            # instead.\n            uniform_proba = np.full_like(proba, 1 / n_classes)\n            proba = np.divide(\n                proba, denominator, out=uniform_proba, where=denominator != 0\n            )\n\n        # Deal with cases where the predicted probability minimally exceeds 1.0\n        proba[(1.0 < proba) & (proba <= 1.0 + 1e-5)] = 1.0\n\n        return proba\n\n\ndef _sigmoid_calibration(predictions, y, sample_weight=None):\n    \"\"\"Probability Calibration with sigmoid method (Platt 2000)\n\n    Parameters\n    ----------\n    predictions : ndarray of shape (n_samples,)\n        The decision function or predict proba for the samples.\n\n    y : ndarray of shape (n_samples,)\n        The targets.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Sample weights. If None, then samples are equally weighted.\n\n    Returns\n    -------\n    a : float\n        The slope.\n\n    b : float\n        The intercept.\n\n    References\n    ----------\n    Platt, \"Probabilistic Outputs for Support Vector Machines\"\n    \"\"\"\n    predictions = column_or_1d(predictions)\n    y = column_or_1d(y)\n\n    F = predictions  # F follows Platt's notations\n\n    # Bayesian priors (see Platt end of section 2.2):\n    # It corresponds to the number of samples, taking into account the\n    # `sample_weight`.\n    mask_negative_samples = y <= 0\n    if sample_weight is not None:\n        prior0 = (sample_weight[mask_negative_samples]).sum()\n        prior1 = (sample_weight[~mask_negative_samples]).sum()\n    else:\n        prior0 = float(np.sum(mask_negative_samples))\n        prior1 = y.shape[0] - prior0\n    T = np.zeros_like(y, dtype=np.float64)\n    T[y > 0] = (prior1 + 1.0) / (prior1 + 2.0)\n    T[y <= 0] = 1.0 / (prior0 + 2.0)\n    T1 = 1.0 - T\n\n    def objective(AB):\n        # From Platt (beginning of Section 2.2)\n        P = expit(-(AB[0] * F + AB[1]))\n        loss = -(xlogy(T, P) + xlogy(T1, 1.0 - P))\n        if sample_weight is not None:\n            return (sample_weight * loss).sum()\n        else:\n            return loss.sum()\n\n    def grad(AB):\n        # gradient of the objective function\n        P = expit(-(AB[0] * F + AB[1]))\n        TEP_minus_T1P = T - P\n        if sample_weight is not None:\n            TEP_minus_T1P *= sample_weight\n        dA = np.dot(TEP_minus_T1P, F)\n        dB = np.sum(TEP_minus_T1P)\n        return np.array([dA, dB])\n\n    AB0 = np.array([0.0, log((prior0 + 1.0) / (prior1 + 1.0))])\n    AB_ = fmin_bfgs(objective, AB0, fprime=grad, disp=False)\n    return AB_[0], AB_[1]\n\n\nclass _SigmoidCalibration(RegressorMixin, BaseEstimator):\n    \"\"\"Sigmoid regression model.\n\n    Attributes\n    ----------\n    a_ : float\n        The slope.\n\n    b_ : float\n        The intercept.\n    \"\"\"\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples,)\n            Training data.\n\n        y : array-like of shape (n_samples,)\n            Training target.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted.\n\n        Returns\n        -------\n        self : object\n            Returns an instance of self.\n        \"\"\"\n        X = column_or_1d(X)\n        y = column_or_1d(y)\n        X, y = indexable(X, y)\n\n        self.a_, self.b_ = _sigmoid_calibration(X, y, sample_weight)\n        return self\n\n    def predict(self, T):\n        \"\"\"Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,)\n            Data to predict from.\n\n        Returns\n        -------\n        T_ : ndarray of shape (n_samples,)\n            The predicted data.\n        \"\"\"\n        T = column_or_1d(T)\n        return expit(-(self.a_ * T + self.b_))\n\n\ndef calibration_curve(\n    y_true,\n    y_prob,\n    *,\n    pos_label=None,\n    normalize=\"deprecated\",\n    n_bins=5,\n    strategy=\"uniform\",\n):\n    \"\"\"Compute true and predicted probabilities for a calibration curve.\n\n    The method assumes the inputs come from a binary classifier, and\n    discretize the [0, 1] interval into bins.\n\n    Calibration curves may also be referred to as reliability diagrams.\n\n    Read more in the :ref:`User Guide <calibration>`.\n\n    Parameters\n    ----------\n    y_true : array-like of shape (n_samples,)\n        True targets.\n\n    y_prob : array-like of shape (n_samples,)\n        Probabilities of the positive class.\n\n    pos_label : int or str, default=None\n        The label of the positive class.\n\n        .. versionadded:: 1.1\n\n    normalize : bool, default=\"deprecated\"\n        Whether y_prob needs to be normalized into the [0, 1] interval, i.e.\n        is not a proper probability. If True, the smallest value in y_prob\n        is linearly mapped onto 0 and the largest one onto 1.\n\n        .. deprecated:: 1.1\n            The normalize argument is deprecated in v1.1 and will be removed in v1.3.\n            Explicitly normalizing `y_prob` will reproduce this behavior, but it is\n            recommended that a proper probability is used (i.e. a classifier's\n            `predict_proba` positive class).\n\n    n_bins : int, default=5\n        Number of bins to discretize the [0, 1] interval. A bigger number\n        requires more data. Bins with no samples (i.e. without\n        corresponding values in `y_prob`) will not be returned, thus the\n        returned arrays may have less than `n_bins` values.\n\n    strategy : {'uniform', 'quantile'}, default='uniform'\n        Strategy used to define the widths of the bins.\n\n        uniform\n            The bins have identical widths.\n        quantile\n            The bins have the same number of samples and depend on `y_prob`.\n\n    Returns\n    -------\n    prob_true : ndarray of shape (n_bins,) or smaller\n        The proportion of samples whose class is the positive class, in each\n        bin (fraction of positives).\n\n    prob_pred : ndarray of shape (n_bins,) or smaller\n        The mean predicted probability in each bin.\n\n    References\n    ----------\n    Alexandru Niculescu-Mizil and Rich Caruana (2005) Predicting Good\n    Probabilities With Supervised Learning, in Proceedings of the 22nd\n    International Conference on Machine Learning (ICML).\n    See section 4 (Qualitative Analysis of Predictions).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.calibration import calibration_curve\n    >>> y_true = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1])\n    >>> y_pred = np.array([0.1, 0.2, 0.3, 0.4, 0.65, 0.7, 0.8, 0.9,  1.])\n    >>> prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=3)\n    >>> prob_true\n    array([0. , 0.5, 1. ])\n    >>> prob_pred\n    array([0.2  , 0.525, 0.85 ])\n    \"\"\"\n    y_true = column_or_1d(y_true)\n    y_prob = column_or_1d(y_prob)\n    check_consistent_length(y_true, y_prob)\n    pos_label = _check_pos_label_consistency(pos_label, y_true)\n\n    # TODO(1.3): Remove normalize conditional block.\n    if normalize != \"deprecated\":\n        warnings.warn(\n            \"The normalize argument is deprecated in v1.1 and will be removed in v1.3.\"\n            \" Explicitly normalizing y_prob will reproduce this behavior, but it is\"\n            \" recommended that a proper probability is used (i.e. a classifier's\"\n            \" `predict_proba` positive class or `decision_function` output calibrated\"\n            \" with `CalibratedClassifierCV`).\",\n            FutureWarning,\n        )\n        if normalize:  # Normalize predicted values into interval [0, 1]\n            y_prob = (y_prob - y_prob.min()) / (y_prob.max() - y_prob.min())\n\n    if y_prob.min() < 0 or y_prob.max() > 1:\n        raise ValueError(\"y_prob has values outside [0, 1].\")\n\n    labels = np.unique(y_true)\n    if len(labels) > 2:\n        raise ValueError(\n            f\"Only binary classification is supported. Provided labels {labels}.\"\n        )\n    y_true = y_true == pos_label\n\n    if strategy == \"quantile\":  # Determine bin edges by distribution of data\n        quantiles = np.linspace(0, 1, n_bins + 1)\n        bins = np.percentile(y_prob, quantiles * 100)\n    elif strategy == \"uniform\":\n        bins = np.linspace(0.0, 1.0, n_bins + 1)\n    else:\n        raise ValueError(\n            \"Invalid entry to 'strategy' input. Strategy \"\n            \"must be either 'quantile' or 'uniform'.\"\n        )\n\n    binids = np.searchsorted(bins[1:-1], y_prob)\n\n    bin_sums = np.bincount(binids, weights=y_prob, minlength=len(bins))\n    bin_true = np.bincount(binids, weights=y_true, minlength=len(bins))\n    bin_total = np.bincount(binids, minlength=len(bins))\n\n    nonzero = bin_total != 0\n    prob_true = bin_true[nonzero] / bin_total[nonzero]\n    prob_pred = bin_sums[nonzero] / bin_total[nonzero]\n\n    return prob_true, prob_pred\n\n\nclass CalibrationDisplay:\n    \"\"\"Calibration curve (also known as reliability diagram) visualization.\n\n    It is recommended to use\n    :func:`~sklearn.calibration.CalibrationDisplay.from_estimator` or\n    :func:`~sklearn.calibration.CalibrationDisplay.from_predictions`\n    to create a `CalibrationDisplay`. All parameters are stored as attributes.\n\n    Read more about calibration in the :ref:`User Guide <calibration>` and\n    more about the scikit-learn visualization API in :ref:`visualizations`.\n\n    .. versionadded:: 1.0\n\n    Parameters\n    ----------\n    prob_true : ndarray of shape (n_bins,)\n        The proportion of samples whose class is the positive class (fraction\n        of positives), in each bin.\n\n    prob_pred : ndarray of shape (n_bins,)\n        The mean predicted probability in each bin.\n\n    y_prob : ndarray of shape (n_samples,)\n        Probability estimates for the positive class, for each sample.\n\n    estimator_name : str, default=None\n        Name of estimator. If None, the estimator name is not shown.\n\n    pos_label : str or int, default=None\n        The positive class when computing the calibration curve.\n        By default, `estimators.classes_[1]` is considered as the\n        positive class.\n\n        .. versionadded:: 1.1\n\n    Attributes\n    ----------\n    line_ : matplotlib Artist\n        Calibration curve.\n\n    ax_ : matplotlib Axes\n        Axes with calibration curve.\n\n    figure_ : matplotlib Figure\n        Figure containing the curve.\n\n    See Also\n    --------\n    calibration_curve : Compute true and predicted probabilities for a\n        calibration curve.\n    CalibrationDisplay.from_predictions : Plot calibration curve using true\n        and predicted labels.\n    CalibrationDisplay.from_estimator : Plot calibration curve using an\n        estimator and data.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_classification\n    >>> from sklearn.model_selection import train_test_split\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.calibration import calibration_curve, CalibrationDisplay\n    >>> X, y = make_classification(random_state=0)\n    >>> X_train, X_test, y_train, y_test = train_test_split(\n    ...     X, y, random_state=0)\n    >>> clf = LogisticRegression(random_state=0)\n    >>> clf.fit(X_train, y_train)\n    LogisticRegression(random_state=0)\n    >>> y_prob = clf.predict_proba(X_test)[:, 1]\n    >>> prob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10)\n    >>> disp = CalibrationDisplay(prob_true, prob_pred, y_prob)\n    >>> disp.plot()\n    <...>\n    \"\"\"\n\n    def __init__(\n        self, prob_true, prob_pred, y_prob, *, estimator_name=None, pos_label=None\n    ):\n        self.prob_true = prob_true\n        self.prob_pred = prob_pred\n        self.y_prob = y_prob\n        self.estimator_name = estimator_name\n        self.pos_label = pos_label\n\n    def plot(self, *, ax=None, name=None, ref_line=True, **kwargs):\n        \"\"\"Plot visualization.\n\n        Extra keyword arguments will be passed to\n        :func:`matplotlib.pyplot.plot`.\n\n        Parameters\n        ----------\n        ax : Matplotlib Axes, default=None\n            Axes object to plot on. If `None`, a new figure and axes is\n            created.\n\n        name : str, default=None\n            Name for labeling curve. If `None`, use `estimator_name` if\n            not `None`, otherwise no labeling is shown.\n\n        ref_line : bool, default=True\n            If `True`, plots a reference line representing a perfectly\n            calibrated classifier.\n\n        **kwargs : dict\n            Keyword arguments to be passed to :func:`matplotlib.pyplot.plot`.\n\n        Returns\n        -------\n        display : :class:`~sklearn.calibration.CalibrationDisplay`\n            Object that stores computed values.\n        \"\"\"\n        check_matplotlib_support(\"CalibrationDisplay.plot\")\n        import matplotlib.pyplot as plt\n\n        if ax is None:\n            fig, ax = plt.subplots()\n\n        name = self.estimator_name if name is None else name\n        info_pos_label = (\n            f\"(Positive class: {self.pos_label})\" if self.pos_label is not None else \"\"\n        )\n\n        line_kwargs = {}\n        if name is not None:\n            line_kwargs[\"label\"] = name\n        line_kwargs.update(**kwargs)\n\n        ref_line_label = \"Perfectly calibrated\"\n        existing_ref_line = ref_line_label in ax.get_legend_handles_labels()[1]\n        if ref_line and not existing_ref_line:\n            ax.plot([0, 1], [0, 1], \"k:\", label=ref_line_label)\n        self.line_ = ax.plot(self.prob_pred, self.prob_true, \"s-\", **line_kwargs)[0]\n\n        # We always have to show the legend for at least the reference line\n        ax.legend(loc=\"lower right\")\n\n        xlabel = f\"Mean predicted probability {info_pos_label}\"\n        ylabel = f\"Fraction of positives {info_pos_label}\"\n        ax.set(xlabel=xlabel, ylabel=ylabel)\n\n        self.ax_ = ax\n        self.figure_ = ax.figure\n        return self\n\n    @classmethod\n    def from_estimator(\n        cls,\n        estimator,\n        X,\n        y,\n        *,\n        n_bins=5,\n        strategy=\"uniform\",\n        pos_label=None,\n        name=None,\n        ref_line=True,\n        ax=None,\n        **kwargs,\n    ):\n        \"\"\"Plot calibration curve using a binary classifier and data.\n\n        A calibration curve, also known as a reliability diagram, uses inputs\n        from a binary classifier and plots the average predicted probability\n        for each bin against the fraction of positive classes, on the\n        y-axis.\n\n        Extra keyword arguments will be passed to\n        :func:`matplotlib.pyplot.plot`.\n\n        Read more about calibration in the :ref:`User Guide <calibration>` and\n        more about the scikit-learn visualization API in :ref:`visualizations`.\n\n        .. versionadded:: 1.0\n\n        Parameters\n        ----------\n        estimator : estimator instance\n            Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\n            in which the last estimator is a classifier. The classifier must\n            have a :term:`predict_proba` method.\n\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input values.\n\n        y : array-like of shape (n_samples,)\n            Binary target values.\n\n        n_bins : int, default=5\n            Number of bins to discretize the [0, 1] interval into when\n            calculating the calibration curve. A bigger number requires more\n            data.\n\n        strategy : {'uniform', 'quantile'}, default='uniform'\n            Strategy used to define the widths of the bins.\n\n            - `'uniform'`: The bins have identical widths.\n            - `'quantile'`: The bins have the same number of samples and depend\n              on predicted probabilities.\n\n        pos_label : str or int, default=None\n            The positive class when computing the calibration curve.\n            By default, `estimators.classes_[1]` is considered as the\n            positive class.\n\n            .. versionadded:: 1.1\n\n        name : str, default=None\n            Name for labeling curve. If `None`, the name of the estimator is\n            used.\n\n        ref_line : bool, default=True\n            If `True`, plots a reference line representing a perfectly\n            calibrated classifier.\n\n        ax : matplotlib axes, default=None\n            Axes object to plot on. If `None`, a new figure and axes is\n            created.\n\n        **kwargs : dict\n            Keyword arguments to be passed to :func:`matplotlib.pyplot.plot`.\n\n        Returns\n        -------\n        display : :class:`~sklearn.calibration.CalibrationDisplay`.\n            Object that stores computed values.\n\n        See Also\n        --------\n        CalibrationDisplay.from_predictions : Plot calibration curve using true\n            and predicted labels.\n\n        Examples\n        --------\n        >>> import matplotlib.pyplot as plt\n        >>> from sklearn.datasets import make_classification\n        >>> from sklearn.model_selection import train_test_split\n        >>> from sklearn.linear_model import LogisticRegression\n        >>> from sklearn.calibration import CalibrationDisplay\n        >>> X, y = make_classification(random_state=0)\n        >>> X_train, X_test, y_train, y_test = train_test_split(\n        ...     X, y, random_state=0)\n        >>> clf = LogisticRegression(random_state=0)\n        >>> clf.fit(X_train, y_train)\n        LogisticRegression(random_state=0)\n        >>> disp = CalibrationDisplay.from_estimator(clf, X_test, y_test)\n        >>> plt.show()\n        \"\"\"\n        method_name = f\"{cls.__name__}.from_estimator\"\n        check_matplotlib_support(method_name)\n\n        if not is_classifier(estimator):\n            raise ValueError(\"'estimator' should be a fitted classifier.\")\n\n        y_prob, pos_label = _get_response(\n            X, estimator, response_method=\"predict_proba\", pos_label=pos_label\n        )\n\n        name = name if name is not None else estimator.__class__.__name__\n        return cls.from_predictions(\n            y,\n            y_prob,\n            n_bins=n_bins,\n            strategy=strategy,\n            pos_label=pos_label,\n            name=name,\n            ref_line=ref_line,\n            ax=ax,\n            **kwargs,\n        )\n\n    @classmethod\n    def from_predictions(\n        cls,\n        y_true,\n        y_prob,\n        *,\n        n_bins=5,\n        strategy=\"uniform\",\n        pos_label=None,\n        name=None,\n        ref_line=True,\n        ax=None,\n        **kwargs,\n    ):\n        \"\"\"Plot calibration curve using true labels and predicted probabilities.\n\n        Calibration curve, also known as reliability diagram, uses inputs\n        from a binary classifier and plots the average predicted probability\n        for each bin against the fraction of positive classes, on the\n        y-axis.\n\n        Extra keyword arguments will be passed to\n        :func:`matplotlib.pyplot.plot`.\n\n        Read more about calibration in the :ref:`User Guide <calibration>` and\n        more about the scikit-learn visualization API in :ref:`visualizations`.\n\n        .. versionadded:: 1.0\n\n        Parameters\n        ----------\n        y_true : array-like of shape (n_samples,)\n            True labels.\n\n        y_prob : array-like of shape (n_samples,)\n            The predicted probabilities of the positive class.\n\n        n_bins : int, default=5\n            Number of bins to discretize the [0, 1] interval into when\n            calculating the calibration curve. A bigger number requires more\n            data.\n\n        strategy : {'uniform', 'quantile'}, default='uniform'\n            Strategy used to define the widths of the bins.\n\n            - `'uniform'`: The bins have identical widths.\n            - `'quantile'`: The bins have the same number of samples and depend\n              on predicted probabilities.\n\n        pos_label : str or int, default=None\n            The positive class when computing the calibration curve.\n            By default, `estimators.classes_[1]` is considered as the\n            positive class.\n\n            .. versionadded:: 1.1\n\n        name : str, default=None\n            Name for labeling curve.\n\n        ref_line : bool, default=True\n            If `True`, plots a reference line representing a perfectly\n            calibrated classifier.\n\n        ax : matplotlib axes, default=None\n            Axes object to plot on. If `None`, a new figure and axes is\n            created.\n\n        **kwargs : dict\n            Keyword arguments to be passed to :func:`matplotlib.pyplot.plot`.\n\n        Returns\n        -------\n        display : :class:`~sklearn.calibration.CalibrationDisplay`.\n            Object that stores computed values.\n\n        See Also\n        --------\n        CalibrationDisplay.from_estimator : Plot calibration curve using an\n            estimator and data.\n\n        Examples\n        --------\n        >>> import matplotlib.pyplot as plt\n        >>> from sklearn.datasets import make_classification\n        >>> from sklearn.model_selection import train_test_split\n        >>> from sklearn.linear_model import LogisticRegression\n        >>> from sklearn.calibration import CalibrationDisplay\n        >>> X, y = make_classification(random_state=0)\n        >>> X_train, X_test, y_train, y_test = train_test_split(\n        ...     X, y, random_state=0)\n        >>> clf = LogisticRegression(random_state=0)\n        >>> clf.fit(X_train, y_train)\n        LogisticRegression(random_state=0)\n        >>> y_prob = clf.predict_proba(X_test)[:, 1]\n        >>> disp = CalibrationDisplay.from_predictions(y_test, y_prob)\n        >>> plt.show()\n        \"\"\"\n        method_name = f\"{cls.__name__}.from_estimator\"\n        check_matplotlib_support(method_name)\n\n        prob_true, prob_pred = calibration_curve(\n            y_true, y_prob, n_bins=n_bins, strategy=strategy, pos_label=pos_label\n        )\n        name = \"Classifier\" if name is None else name\n        pos_label = _check_pos_label_consistency(pos_label, y_true)\n\n        disp = cls(\n            prob_true=prob_true,\n            prob_pred=prob_pred,\n            y_prob=y_prob,\n            estimator_name=name,\n            pos_label=pos_label,\n        )\n        return disp.plot(ax=ax, ref_line=ref_line, **kwargs)", "\"\"\"Calibration of predicted probabilities.\"\"\"\n\n# Author: Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>\n#         Balazs Kegl <balazs.kegl@gmail.com>\n#         Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n#         Mathieu Blondel <mathieu@mblondel.org>\n#\n# License: BSD 3 clause\n\nfrom numbers import Integral\nimport warnings\nfrom inspect import signature\nfrom functools import partial\n\nfrom math import log\nimport numpy as np\n\nfrom scipy.special import expit\nfrom scipy.special import xlogy\nfrom scipy.optimize import fmin_bfgs\n\nfrom .base import (\n    BaseEstimator,\n    ClassifierMixin,\n    RegressorMixin,\n    clone,\n    MetaEstimatorMixin,\n    is_classifier,\n)\nfrom .preprocessing import label_binarize, LabelEncoder\nfrom .utils import (\n    column_or_1d,\n    indexable,\n    check_matplotlib_support,\n)\n\nfrom .utils.multiclass import check_classification_targets\nfrom .utils.parallel import delayed, Parallel\nfrom .utils._param_validation import StrOptions, HasMethods, Hidden\nfrom .utils.validation import (\n    _check_fit_params,\n    _check_sample_weight,\n    _num_samples,\n    check_consistent_length,\n    check_is_fitted,\n)\nfrom .utils import _safe_indexing\nfrom .isotonic import IsotonicRegression\nfrom .svm import LinearSVC\nfrom .model_selection import check_cv, cross_val_predict\nfrom .metrics._base import _check_pos_label_consistency\nfrom .metrics._plot.base import _get_response\n\n\nclass CalibratedClassifierCV(ClassifierMixin, MetaEstimatorMixin, BaseEstimator):\n    \"\"\"Probability calibration with isotonic regression or logistic regression.\n\n    This class uses cross-validation to both estimate the parameters of a\n    classifier and subsequently calibrate a classifier. With default\n    `ensemble=True`, for each cv split it\n    fits a copy of the base estimator to the training subset, and calibrates it\n    using the testing subset. For prediction, predicted probabilities are\n    averaged across these individual calibrated classifiers. When\n    `ensemble=False`, cross-validation is used to obtain unbiased predictions,\n    via :func:`~sklearn.model_selection.cross_val_predict`, which are then\n    used for calibration. For prediction, the base estimator, trained using all\n    the data, is used. This is the method implemented when `probabilities=True`\n    for :mod:`sklearn.svm` estimators.\n\n    Already fitted classifiers can be calibrated via the parameter\n    `cv=\"prefit\"`. In this case, no cross-validation is used and all provided\n    data is used for calibration. The user has to take care manually that data\n    for model fitting and calibration are disjoint.\n\n    The calibration is based on the :term:`decision_function` method of the\n    `estimator` if it exists, else on :term:`predict_proba`.\n\n    Read more in the :ref:`User Guide <calibration>`.\n\n    Parameters\n    ----------\n    estimator : estimator instance, default=None\n        The classifier whose output need to be calibrated to provide more\n        accurate `predict_proba` outputs. The default classifier is\n        a :class:`~sklearn.svm.LinearSVC`.\n\n        .. versionadded:: 1.2\n\n    method : {'sigmoid', 'isotonic'}, default='sigmoid'\n        The method to use for calibration. Can be 'sigmoid' which\n        corresponds to Platt's method (i.e. a logistic regression model) or\n        'isotonic' which is a non-parametric approach. It is not advised to\n        use isotonic calibration with too few calibration samples\n        ``(<<1000)`` since it tends to overfit.\n\n    cv : int, cross-validation generator, iterable or \"prefit\", \\\n            default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 5-fold cross-validation,\n        - integer, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if ``y`` is binary or multiclass,\n        :class:`~sklearn.model_selection.StratifiedKFold` is used. If ``y`` is\n        neither binary nor multiclass, :class:`~sklearn.model_selection.KFold`\n        is used.\n\n        Refer to the :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        If \"prefit\" is passed, it is assumed that `estimator` has been\n        fitted already and all data is used for calibration.\n\n        .. versionchanged:: 0.22\n            ``cv`` default value if None changed from 3-fold to 5-fold.\n\n    n_jobs : int, default=None\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors.\n\n        Base estimator clones are fitted in parallel across cross-validation\n        iterations. Therefore parallelism happens only when `cv != \"prefit\"`.\n\n        See :term:`Glossary <n_jobs>` for more details.\n\n        .. versionadded:: 0.24\n\n    ensemble : bool, default=True\n        Determines how the calibrator is fitted when `cv` is not `'prefit'`.\n        Ignored if `cv='prefit'`.\n\n        If `True`, the `estimator` is fitted using training data, and\n        calibrated using testing data, for each `cv` fold. The final estimator\n        is an ensemble of `n_cv` fitted classifier and calibrator pairs, where\n        `n_cv` is the number of cross-validation folds. The output is the\n        average predicted probabilities of all pairs.\n\n        If `False`, `cv` is used to compute unbiased predictions, via\n        :func:`~sklearn.model_selection.cross_val_predict`, which are then\n        used for calibration. At prediction time, the classifier used is the\n        `estimator` trained on all the data.\n        Note that this method is also internally implemented  in\n        :mod:`sklearn.svm` estimators with the `probabilities=True` parameter.\n\n        .. versionadded:: 0.24\n\n    base_estimator : estimator instance\n        This parameter is deprecated. Use `estimator` instead.\n\n        .. deprecated:: 1.2\n           The parameter `base_estimator` is deprecated in 1.2 and will be\n           removed in 1.4. Use `estimator` instead.\n\n    Attributes\n    ----------\n    classes_ : ndarray of shape (n_classes,)\n        The class labels.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`. Only defined if the\n        underlying estimator exposes such an attribute when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Only defined if the\n        underlying estimator exposes such an attribute when fit.\n\n        .. versionadded:: 1.0\n\n    calibrated_classifiers_ : list (len() equal to cv or 1 if `cv=\"prefit\"` \\\n            or `ensemble=False`)\n        The list of classifier and calibrator pairs.\n\n        - When `cv=\"prefit\"`, the fitted `estimator` and fitted\n          calibrator.\n        - When `cv` is not \"prefit\" and `ensemble=True`, `n_cv` fitted\n          `estimator` and calibrator pairs. `n_cv` is the number of\n          cross-validation folds.\n        - When `cv` is not \"prefit\" and `ensemble=False`, the `estimator`,\n          fitted on all the data, and fitted calibrator.\n\n        .. versionchanged:: 0.24\n            Single calibrated classifier case when `ensemble=False`.\n\n    See Also\n    --------\n    calibration_curve : Compute true and predicted probabilities\n        for a calibration curve.\n\n    References\n    ----------\n    .. [1] Obtaining calibrated probability estimates from decision trees\n           and naive Bayesian classifiers, B. Zadrozny & C. Elkan, ICML 2001\n\n    .. [2] Transforming Classifier Scores into Accurate Multiclass\n           Probability Estimates, B. Zadrozny & C. Elkan, (KDD 2002)\n\n    .. [3] Probabilistic Outputs for Support Vector Machines and Comparisons to\n           Regularized Likelihood Methods, J. Platt, (1999)\n\n    .. [4] Predicting Good Probabilities with Supervised Learning,\n           A. Niculescu-Mizil & R. Caruana, ICML 2005\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_classification\n    >>> from sklearn.naive_bayes import GaussianNB\n    >>> from sklearn.calibration import CalibratedClassifierCV\n    >>> X, y = make_classification(n_samples=100, n_features=2,\n    ...                            n_redundant=0, random_state=42)\n    >>> base_clf = GaussianNB()\n    >>> calibrated_clf = CalibratedClassifierCV(base_clf, cv=3)\n    >>> calibrated_clf.fit(X, y)\n    CalibratedClassifierCV(...)\n    >>> len(calibrated_clf.calibrated_classifiers_)\n    3\n    >>> calibrated_clf.predict_proba(X)[:5, :]\n    array([[0.110..., 0.889...],\n           [0.072..., 0.927...],\n           [0.928..., 0.071...],\n           [0.928..., 0.071...],\n           [0.071..., 0.928...]])\n    >>> from sklearn.model_selection import train_test_split\n    >>> X, y = make_classification(n_samples=100, n_features=2,\n    ...                            n_redundant=0, random_state=42)\n    >>> X_train, X_calib, y_train, y_calib = train_test_split(\n    ...        X, y, random_state=42\n    ... )\n    >>> base_clf = GaussianNB()\n    >>> base_clf.fit(X_train, y_train)\n    GaussianNB()\n    >>> calibrated_clf = CalibratedClassifierCV(base_clf, cv=\"prefit\")\n    >>> calibrated_clf.fit(X_calib, y_calib)\n    CalibratedClassifierCV(...)\n    >>> len(calibrated_clf.calibrated_classifiers_)\n    1\n    >>> calibrated_clf.predict_proba([[-0.5, 0.5]])\n    array([[0.936..., 0.063...]])\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"estimator\": [\n            HasMethods([\"fit\", \"predict_proba\"]),\n            HasMethods([\"fit\", \"decision_function\"]),\n            None,\n        ],\n        \"method\": [StrOptions({\"isotonic\", \"sigmoid\"})],\n        \"cv\": [\"cv_object\", StrOptions({\"prefit\"})],\n        \"n_jobs\": [Integral, None],\n        \"ensemble\": [\"boolean\"],\n        \"base_estimator\": [\n            HasMethods([\"fit\", \"predict_proba\"]),\n            HasMethods([\"fit\", \"decision_function\"]),\n            None,\n            Hidden(StrOptions({\"deprecated\"})),\n        ],\n    }\n\n    def __init__(\n        self,\n        estimator=None,\n        *,\n        method=\"sigmoid\",\n        cv=None,\n        n_jobs=None,\n        ensemble=True,\n        base_estimator=\"deprecated\",\n    ):\n        self.estimator = estimator\n        self.method = method\n        self.cv = cv\n        self.n_jobs = n_jobs\n        self.ensemble = ensemble\n        self.base_estimator = base_estimator\n\n    def fit(self, X, y, sample_weight=None, **fit_params):\n        \"\"\"Fit the calibrated model.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted.\n\n        **fit_params : dict\n            Parameters to pass to the `fit` method of the underlying\n            classifier.\n\n        Returns\n        -------\n        self : object\n            Returns an instance of self.\n        \"\"\"\n        self._validate_params()\n\n        check_classification_targets(y)\n        X, y = indexable(X, y)\n        if sample_weight is not None:\n            sample_weight = _check_sample_weight(sample_weight, X)\n\n        for sample_aligned_params in fit_params.values():\n            check_consistent_length(y, sample_aligned_params)\n\n        # TODO(1.4): Remove when base_estimator is removed\n        if self.base_estimator != \"deprecated\":\n            if self.estimator is not None:\n                raise ValueError(\n                    \"Both `base_estimator` and `estimator` are set. Only set \"\n                    \"`estimator` since `base_estimator` is deprecated.\"\n                )\n            warnings.warn(\n                \"`base_estimator` was renamed to `estimator` in version 1.2 and \"\n                \"will be removed in 1.4.\",\n                FutureWarning,\n            )\n            estimator = self.base_estimator\n        else:\n            estimator = self.estimator\n\n        if estimator is None:\n            # we want all classifiers that don't expose a random_state\n            # to be deterministic (and we don't want to expose this one).\n            estimator = LinearSVC(random_state=0)\n\n        self.calibrated_classifiers_ = []\n        if self.cv == \"prefit\":\n            # `classes_` should be consistent with that of estimator\n            check_is_fitted(self.estimator, attributes=[\"classes_\"])\n            self.classes_ = self.estimator.classes_\n\n            pred_method, method_name = _get_prediction_method(estimator)\n            n_classes = len(self.classes_)\n            predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n\n            calibrated_classifier = _fit_calibrator(\n                estimator,\n                predictions,\n                y,\n                self.classes_,\n                self.method,\n                sample_weight,\n            )\n            self.calibrated_classifiers_.append(calibrated_classifier)\n        else:\n            # Set `classes_` using all `y`\n            label_encoder_ = LabelEncoder().fit(y)\n            self.classes_ = label_encoder_.classes_\n            n_classes = len(self.classes_)\n\n            # sample_weight checks\n            fit_parameters = signature(estimator.fit).parameters\n            supports_sw = \"sample_weight\" in fit_parameters\n            if sample_weight is not None and not supports_sw:\n                estimator_name = type(estimator).__name__\n                warnings.warn(\n                    f\"Since {estimator_name} does not appear to accept sample_weight, \"\n                    \"sample weights will only be used for the calibration itself. This \"\n                    \"can be caused by a limitation of the current scikit-learn API. \"\n                    \"See the following issue for more details: \"\n                    \"https://github.com/scikit-learn/scikit-learn/issues/21134. Be \"\n                    \"warned that the result of the calibration is likely to be \"\n                    \"incorrect.\"\n                )\n\n            # Check that each cross-validation fold can have at least one\n            # example per class\n            if isinstance(self.cv, int):\n                n_folds = self.cv\n            elif hasattr(self.cv, \"n_splits\"):\n                n_folds = self.cv.n_splits\n            else:\n                n_folds = None\n            if n_folds and np.any(\n                [np.sum(y == class_) < n_folds for class_ in self.classes_]\n            ):\n                raise ValueError(\n                    f\"Requesting {n_folds}-fold \"\n                    \"cross-validation but provided less than \"\n                    f\"{n_folds} examples for at least one class.\"\n                )\n            cv = check_cv(self.cv, y, classifier=True)\n\n            if self.ensemble:\n                parallel = Parallel(n_jobs=self.n_jobs)\n                self.calibrated_classifiers_ = parallel(\n                    delayed(_fit_classifier_calibrator_pair)(\n                        clone(estimator),\n                        X,\n                        y,\n                        train=train,\n                        test=test,\n                        method=self.method,\n                        classes=self.classes_,\n                        supports_sw=supports_sw,\n                        sample_weight=sample_weight,\n                        **fit_params,\n                    )\n                    for train, test in cv.split(X, y)\n                )\n            else:\n                this_estimator = clone(estimator)\n                _, method_name = _get_prediction_method(this_estimator)\n                fit_params = (\n                    {\"sample_weight\": sample_weight}\n                    if sample_weight is not None and supports_sw\n                    else None\n                )\n                pred_method = partial(\n                    cross_val_predict,\n                    estimator=this_estimator,\n                    X=X,\n                    y=y,\n                    cv=cv,\n                    method=method_name,\n                    n_jobs=self.n_jobs,\n                    fit_params=fit_params,\n                )\n                predictions = _compute_predictions(\n                    pred_method, method_name, X, n_classes\n                )\n\n                if sample_weight is not None and supports_sw:\n                    this_estimator.fit(X, y, sample_weight=sample_weight)\n                else:\n                    this_estimator.fit(X, y)\n                # Note: Here we don't pass on fit_params because the supported\n                # calibrators don't support fit_params anyway\n                calibrated_classifier = _fit_calibrator(\n                    this_estimator,\n                    predictions,\n                    y,\n                    self.classes_,\n                    self.method,\n                    sample_weight,\n                )\n                self.calibrated_classifiers_.append(calibrated_classifier)\n\n        first_clf = self.calibrated_classifiers_[0].estimator\n        if hasattr(first_clf, \"n_features_in_\"):\n            self.n_features_in_ = first_clf.n_features_in_\n        if hasattr(first_clf, \"feature_names_in_\"):\n            self.feature_names_in_ = first_clf.feature_names_in_\n        return self\n\n    def predict_proba(self, X):\n        \"\"\"Calibrated probabilities of classification.\n\n        This function returns calibrated probabilities of classification\n        according to each class on an array of test vectors X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The samples, as accepted by `estimator.predict_proba`.\n\n        Returns\n        -------\n        C : ndarray of shape (n_samples, n_classes)\n            The predicted probas.\n        \"\"\"\n        check_is_fitted(self)\n        # Compute the arithmetic mean of the predictions of the calibrated\n        # classifiers\n        mean_proba = np.zeros((_num_samples(X), len(self.classes_)))\n        for calibrated_classifier in self.calibrated_classifiers_:\n            proba = calibrated_classifier.predict_proba(X)\n            mean_proba += proba\n\n        mean_proba /= len(self.calibrated_classifiers_)\n\n        return mean_proba\n\n    def predict(self, X):\n        \"\"\"Predict the target of new samples.\n\n        The predicted class is the class that has the highest probability,\n        and can thus be different from the prediction of the uncalibrated classifier.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The samples, as accepted by `estimator.predict`.\n\n        Returns\n        -------\n        C : ndarray of shape (n_samples,)\n            The predicted class.\n        \"\"\"\n        check_is_fitted(self)\n        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n\n    def _more_tags(self):\n        return {\n            \"_xfail_checks\": {\n                \"check_sample_weights_invariance\": (\n                    \"Due to the cross-validation and sample ordering, removing a sample\"\n                    \" is not strictly equal to putting is weight to zero. Specific unit\"\n                    \" tests are added for CalibratedClassifierCV specifically.\"\n                ),\n            }\n        }\n\n\ndef _fit_classifier_calibrator_pair(\n    estimator,\n    X,\n    y,\n    train,\n    test,\n    supports_sw,\n    method,\n    classes,\n    sample_weight=None,\n    **fit_params,\n):\n    \"\"\"Fit a classifier/calibration pair on a given train/test split.\n\n    Fit the classifier on the train set, compute its predictions on the test\n    set and use the predictions as input to fit the calibrator along with the\n    test labels.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Cloned base estimator.\n\n    X : array-like, shape (n_samples, n_features)\n        Sample data.\n\n    y : array-like, shape (n_samples,)\n        Targets.\n\n    train : ndarray, shape (n_train_indices,)\n        Indices of the training subset.\n\n    test : ndarray, shape (n_test_indices,)\n        Indices of the testing subset.\n\n    supports_sw : bool\n        Whether or not the `estimator` supports sample weights.\n\n    method : {'sigmoid', 'isotonic'}\n        Method to use for calibration.\n\n    classes : ndarray, shape (n_classes,)\n        The target classes.\n\n    sample_weight : array-like, default=None\n        Sample weights for `X`.\n\n    **fit_params : dict\n        Parameters to pass to the `fit` method of the underlying\n        classifier.\n\n    Returns\n    -------\n    calibrated_classifier : _CalibratedClassifier instance\n    \"\"\"\n    fit_params_train = _check_fit_params(X, fit_params, train)\n    X_train, y_train = _safe_indexing(X, train), _safe_indexing(y, train)\n    X_test, y_test = _safe_indexing(X, test), _safe_indexing(y, test)\n\n    if sample_weight is not None and supports_sw:\n        sw_train = _safe_indexing(sample_weight, train)\n        estimator.fit(X_train, y_train, sample_weight=sw_train, **fit_params_train)\n    else:\n        estimator.fit(X_train, y_train, **fit_params_train)\n\n    n_classes = len(classes)\n    pred_method, method_name = _get_prediction_method(estimator)\n    predictions = _compute_predictions(pred_method, method_name, X_test, n_classes)\n\n    sw_test = None if sample_weight is None else _safe_indexing(sample_weight, test)\n    calibrated_classifier = _fit_calibrator(\n        estimator, predictions, y_test, classes, method, sample_weight=sw_test\n    )\n    return calibrated_classifier\n\n\ndef _get_prediction_method(clf):\n    \"\"\"Return prediction method.\n\n    `decision_function` method of `clf` returned, if it\n    exists, otherwise `predict_proba` method returned.\n\n    Parameters\n    ----------\n    clf : Estimator instance\n        Fitted classifier to obtain the prediction method from.\n\n    Returns\n    -------\n    prediction_method : callable\n        The prediction method.\n    method_name : str\n        The name of the prediction method.\n    \"\"\"\n    if hasattr(clf, \"decision_function\"):\n        method = getattr(clf, \"decision_function\")\n        return method, \"decision_function\"\n\n    if hasattr(clf, \"predict_proba\"):\n        method = getattr(clf, \"predict_proba\")\n        return method, \"predict_proba\"\n\n\ndef _compute_predictions(pred_method, method_name, X, n_classes):\n    \"\"\"Return predictions for `X` and reshape binary outputs to shape\n    (n_samples, 1).\n\n    Parameters\n    ----------\n    pred_method : callable\n        Prediction method.\n\n    method_name: str\n        Name of the prediction method\n\n    X : array-like or None\n        Data used to obtain predictions.\n\n    n_classes : int\n        Number of classes present.\n\n    Returns\n    -------\n    predictions : array-like, shape (X.shape[0], len(clf.classes_))\n        The predictions. Note if there are 2 classes, array is of shape\n        (X.shape[0], 1).\n    \"\"\"\n    predictions = pred_method(X=X)\n\n    if method_name == \"decision_function\":\n        if predictions.ndim == 1:\n            predictions = predictions[:, np.newaxis]\n    elif method_name == \"predict_proba\":\n        if n_classes == 2:\n            predictions = predictions[:, 1:]\n    else:  # pragma: no cover\n        # this branch should be unreachable.\n        raise ValueError(f\"Invalid prediction method: {method_name}\")\n    return predictions\n\n\ndef _fit_calibrator(clf, predictions, y, classes, method, sample_weight=None):\n    \"\"\"Fit calibrator(s) and return a `_CalibratedClassifier`\n    instance.\n\n    `n_classes` (i.e. `len(clf.classes_)`) calibrators are fitted.\n    However, if `n_classes` equals 2, one calibrator is fitted.\n\n    Parameters\n    ----------\n    clf : estimator instance\n        Fitted classifier.\n\n    predictions : array-like, shape (n_samples, n_classes) or (n_samples, 1) \\\n                    when binary.\n        Raw predictions returned by the un-calibrated base classifier.\n\n    y : array-like, shape (n_samples,)\n        The targets.\n\n    classes : ndarray, shape (n_classes,)\n        All the prediction classes.\n\n    method : {'sigmoid', 'isotonic'}\n        The method to use for calibration.\n\n    sample_weight : ndarray, shape (n_samples,), default=None\n        Sample weights. If None, then samples are equally weighted.\n\n    Returns\n    -------\n    pipeline : _CalibratedClassifier instance\n    \"\"\"\n    Y = label_binarize(y, classes=classes)\n    label_encoder = LabelEncoder().fit(classes)\n    pos_class_indices = label_encoder.transform(clf.classes_)\n    calibrators = []\n    for class_idx, this_pred in zip(pos_class_indices, predictions.T):\n        if method == \"isotonic\":\n            calibrator = IsotonicRegression(out_of_bounds=\"clip\")\n        else:  # \"sigmoid\"\n            calibrator = _SigmoidCalibration()\n        calibrator.fit(this_pred, Y[:, class_idx], sample_weight)\n        calibrators.append(calibrator)\n\n    pipeline = _CalibratedClassifier(clf, calibrators, method=method, classes=classes)\n    return pipeline\n\n\nclass _CalibratedClassifier:\n    \"\"\"Pipeline-like chaining a fitted classifier and its fitted calibrators.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Fitted classifier.\n\n    calibrators : list of fitted estimator instances\n        List of fitted calibrators (either 'IsotonicRegression' or\n        '_SigmoidCalibration'). The number of calibrators equals the number of\n        classes. However, if there are 2 classes, the list contains only one\n        fitted calibrator.\n\n    classes : array-like of shape (n_classes,)\n        All the prediction classes.\n\n    method : {'sigmoid', 'isotonic'}, default='sigmoid'\n        The method to use for calibration. Can be 'sigmoid' which\n        corresponds to Platt's method or 'isotonic' which is a\n        non-parametric approach based on isotonic regression.\n    \"\"\"\n\n    def __init__(self, estimator, calibrators, *, classes, method=\"sigmoid\"):\n        self.estimator = estimator\n        self.calibrators = calibrators\n        self.classes = classes\n        self.method = method\n\n    def predict_proba(self, X):\n        \"\"\"Calculate calibrated probabilities.\n\n        Calculates classification calibrated probabilities\n        for each class, in a one-vs-all manner, for `X`.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            The sample data.\n\n        Returns\n        -------\n        proba : array, shape (n_samples, n_classes)\n            The predicted probabilities. Can be exact zeros.\n        \"\"\"\n        n_classes = len(self.classes)\n        pred_method, method_name = _get_prediction_method(self.estimator)\n        predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n\n        label_encoder = LabelEncoder().fit(self.classes)\n        pos_class_indices = label_encoder.transform(self.estimator.classes_)\n\n        proba = np.zeros((_num_samples(X), n_classes))\n        for class_idx, this_pred, calibrator in zip(\n            pos_class_indices, predictions.T, self.calibrators\n        ):\n            if n_classes == 2:\n                # When binary, `predictions` consists only of predictions for\n                # clf.classes_[1] but `pos_class_indices` = 0\n                class_idx += 1\n            proba[:, class_idx] = calibrator.predict(this_pred)\n\n        # Normalize the probabilities\n        if n_classes == 2:\n            proba[:, 0] = 1.0 - proba[:, 1]\n        else:\n            denominator = np.sum(proba, axis=1)[:, np.newaxis]\n            # In the edge case where for each class calibrator returns a null\n            # probability for a given sample, use the uniform distribution\n            # instead.\n            uniform_proba = np.full_like(proba, 1 / n_classes)\n            proba = np.divide(\n                proba, denominator, out=uniform_proba, where=denominator != 0\n            )\n\n        # Deal with cases where the predicted probability minimally exceeds 1.0\n        proba[(1.0 < proba) & (proba <= 1.0 + 1e-5)] = 1.0\n\n        return proba\n\n\ndef _sigmoid_calibration(predictions, y, sample_weight=None):\n    \"\"\"Probability Calibration with sigmoid method (Platt 2000)\n\n    Parameters\n    ----------\n    predictions : ndarray of shape (n_samples,)\n        The decision function or predict proba for the samples.\n\n    y : ndarray of shape (n_samples,)\n        The targets.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Sample weights. If None, then samples are equally weighted.\n\n    Returns\n    -------\n    a : float\n        The slope.\n\n    b : float\n        The intercept.\n\n    References\n    ----------\n    Platt, \"Probabilistic Outputs for Support Vector Machines\"\n    \"\"\"\n    predictions = column_or_1d(predictions)\n    y = column_or_1d(y)\n\n    F = predictions  # F follows Platt's notations\n\n    # Bayesian priors (see Platt end of section 2.2):\n    # It corresponds to the number of samples, taking into account the\n    # `sample_weight`.\n    mask_negative_samples = y <= 0\n    if sample_weight is not None:\n        prior0 = (sample_weight[mask_negative_samples]).sum()\n        prior1 = (sample_weight[~mask_negative_samples]).sum()\n    else:\n        prior0 = float(np.sum(mask_negative_samples))\n        prior1 = y.shape[0] - prior0\n    T = np.zeros_like(y, dtype=np.float64)\n    T[y > 0] = (prior1 + 1.0) / (prior1 + 2.0)\n    T[y <= 0] = 1.0 / (prior0 + 2.0)\n    T1 = 1.0 - T\n\n    def objective(AB):\n        # From Platt (beginning of Section 2.2)\n        P = expit(-(AB[0] * F + AB[1]))\n        loss = -(xlogy(T, P) + xlogy(T1, 1.0 - P))\n        if sample_weight is not None:\n            return (sample_weight * loss).sum()\n        else:\n            return loss.sum()\n\n    def grad(AB):\n        # gradient of the objective function\n        P = expit(-(AB[0] * F + AB[1]))\n        TEP_minus_T1P = T - P\n        if sample_weight is not None:\n            TEP_minus_T1P *= sample_weight\n        dA = np.dot(TEP_minus_T1P, F)\n        dB = np.sum(TEP_minus_T1P)\n        return np.array([dA, dB])\n\n    AB0 = np.array([0.0, log((prior0 + 1.0) / (prior1 + 1.0))])\n    AB_ = fmin_bfgs(objective, AB0, fprime=grad, disp=False)\n    return AB_[0], AB_[1]\n\n\nclass _SigmoidCalibration(RegressorMixin, BaseEstimator):\n    \"\"\"Sigmoid regression model.\n\n    Attributes\n    ----------\n    a_ : float\n        The slope.\n\n    b_ : float\n        The intercept.\n    \"\"\"\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples,)\n            Training data.\n\n        y : array-like of shape (n_samples,)\n            Training target.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted.\n\n        Returns\n        -------\n        self : object\n            Returns an instance of self.\n        \"\"\"\n        X = column_or_1d(X)\n        y = column_or_1d(y)\n        X, y = indexable(X, y)\n\n        self.a_, self.b_ = _sigmoid_calibration(X, y, sample_weight)\n        return self\n\n    def predict(self, T):\n        \"\"\"Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,)\n            Data to predict from.\n\n        Returns\n        -------\n        T_ : ndarray of shape (n_samples,)\n            The predicted data.\n        \"\"\"\n        T = column_or_1d(T)\n        return expit(-(self.a_ * T + self.b_))\n\n\ndef calibration_curve(\n    y_true,\n    y_prob,\n    *,\n    pos_label=None,\n    normalize=\"deprecated\",\n    n_bins=5,\n    strategy=\"uniform\",\n):\n    \"\"\"Compute true and predicted probabilities for a calibration curve.\n\n    The method assumes the inputs come from a binary classifier, and\n    discretize the [0, 1] interval into bins.\n\n    Calibration curves may also be referred to as reliability diagrams.\n\n    Read more in the :ref:`User Guide <calibration>`.\n\n    Parameters\n    ----------\n    y_true : array-like of shape (n_samples,)\n        True targets.\n\n    y_prob : array-like of shape (n_samples,)\n        Probabilities of the positive class.\n\n    pos_label : int or str, default=None\n        The label of the positive class.\n\n        .. versionadded:: 1.1\n\n    normalize : bool, default=\"deprecated\"\n        Whether y_prob needs to be normalized into the [0, 1] interval, i.e.\n        is not a proper probability. If True, the smallest value in y_prob\n        is linearly mapped onto 0 and the largest one onto 1.\n\n        .. deprecated:: 1.1\n            The normalize argument is deprecated in v1.1 and will be removed in v1.3.\n            Explicitly normalizing `y_prob` will reproduce this behavior, but it is\n            recommended that a proper probability is used (i.e. a classifier's\n            `predict_proba` positive class).\n\n    n_bins : int, default=5\n        Number of bins to discretize the [0, 1] interval. A bigger number\n        requires more data. Bins with no samples (i.e. without\n        corresponding values in `y_prob`) will not be returned, thus the\n        returned arrays may have less than `n_bins` values.\n\n    strategy : {'uniform', 'quantile'}, default='uniform'\n        Strategy used to define the widths of the bins.\n\n        uniform\n            The bins have identical widths.\n        quantile\n            The bins have the same number of samples and depend on `y_prob`.\n\n    Returns\n    -------\n    prob_true : ndarray of shape (n_bins,) or smaller\n        The proportion of samples whose class is the positive class, in each\n        bin (fraction of positives).\n\n    prob_pred : ndarray of shape (n_bins,) or smaller\n        The mean predicted probability in each bin.\n\n    References\n    ----------\n    Alexandru Niculescu-Mizil and Rich Caruana (2005) Predicting Good\n    Probabilities With Supervised Learning, in Proceedings of the 22nd\n    International Conference on Machine Learning (ICML).\n    See section 4 (Qualitative Analysis of Predictions).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.calibration import calibration_curve\n    >>> y_true = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1])\n    >>> y_pred = np.array([0.1, 0.2, 0.3, 0.4, 0.65, 0.7, 0.8, 0.9,  1.])\n    >>> prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=3)\n    >>> prob_true\n    array([0. , 0.5, 1. ])\n    >>> prob_pred\n    array([0.2  , 0.525, 0.85 ])\n    \"\"\"\n    y_true = column_or_1d(y_true)\n    y_prob = column_or_1d(y_prob)\n    check_consistent_length(y_true, y_prob)\n    pos_label = _check_pos_label_consistency(pos_label, y_true)\n\n    # TODO(1.3): Remove normalize conditional block.\n    if normalize != \"deprecated\":\n        warnings.warn(\n            \"The normalize argument is deprecated in v1.1 and will be removed in v1.3.\"\n            \" Explicitly normalizing y_prob will reproduce this behavior, but it is\"\n            \" recommended that a proper probability is used (i.e. a classifier's\"\n            \" `predict_proba` positive class or `decision_function` output calibrated\"\n            \" with `CalibratedClassifierCV`).\",\n            FutureWarning,\n        )\n        if normalize:  # Normalize predicted values into interval [0, 1]\n            y_prob = (y_prob - y_prob.min()) / (y_prob.max() - y_prob.min())\n\n    if y_prob.min() < 0 or y_prob.max() > 1:\n        raise ValueError(\"y_prob has values outside [0, 1].\")\n\n    labels = np.unique(y_true)\n    if len(labels) > 2:\n        raise ValueError(\n            f\"Only binary classification is supported. Provided labels {labels}.\"\n        )\n    y_true = y_true == pos_label\n\n    if strategy == \"quantile\":  # Determine bin edges by distribution of data\n        quantiles = np.linspace(0, 1, n_bins + 1)\n        bins = np.percentile(y_prob, quantiles * 100)\n    elif strategy == \"uniform\":\n        bins = np.linspace(0.0, 1.0, n_bins + 1)\n    else:\n        raise ValueError(\n            \"Invalid entry to 'strategy' input. Strategy \"\n            \"must be either 'quantile' or 'uniform'.\"\n        )\n\n    binids = np.searchsorted(bins[1:-1], y_prob)\n\n    bin_sums = np.bincount(binids, weights=y_prob, minlength=len(bins))\n    bin_true = np.bincount(binids, weights=y_true, minlength=len(bins))\n    bin_total = np.bincount(binids, minlength=len(bins))\n\n    nonzero = bin_total != 0\n    prob_true = bin_true[nonzero] / bin_total[nonzero]\n    prob_pred = bin_sums[nonzero] / bin_total[nonzero]\n\n    return prob_true, prob_pred\n\n\nclass CalibrationDisplay:\n    \"\"\"Calibration curve (also known as reliability diagram) visualization.\n\n    It is recommended to use\n    :func:`~sklearn.calibration.CalibrationDisplay.from_estimator` or\n    :func:`~sklearn.calibration.CalibrationDisplay.from_predictions`\n    to create a `CalibrationDisplay`. All parameters are stored as attributes.\n\n    Read more about calibration in the :ref:`User Guide <calibration>` and\n    more about the scikit-learn visualization API in :ref:`visualizations`.\n\n    .. versionadded:: 1.0\n\n    Parameters\n    ----------\n    prob_true : ndarray of shape (n_bins,)\n        The proportion of samples whose class is the positive class (fraction\n        of positives), in each bin.\n\n    prob_pred : ndarray of shape (n_bins,)\n        The mean predicted probability in each bin.\n\n    y_prob : ndarray of shape (n_samples,)\n        Probability estimates for the positive class, for each sample.\n\n    estimator_name : str, default=None\n        Name of estimator. If None, the estimator name is not shown.\n\n    pos_label : str or int, default=None\n        The positive class when computing the calibration curve.\n        By default, `estimators.classes_[1]` is considered as the\n        positive class.\n\n        .. versionadded:: 1.1\n\n    Attributes\n    ----------\n    line_ : matplotlib Artist\n        Calibration curve.\n\n    ax_ : matplotlib Axes\n        Axes with calibration curve.\n\n    figure_ : matplotlib Figure\n        Figure containing the curve.\n\n    See Also\n    --------\n    calibration_curve : Compute true and predicted probabilities for a\n        calibration curve.\n    CalibrationDisplay.from_predictions : Plot calibration curve using true\n        and predicted labels.\n    CalibrationDisplay.from_estimator : Plot calibration curve using an\n        estimator and data.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_classification\n    >>> from sklearn.model_selection import train_test_split\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.calibration import calibration_curve, CalibrationDisplay\n    >>> X, y = make_classification(random_state=0)\n    >>> X_train, X_test, y_train, y_test = train_test_split(\n    ...     X, y, random_state=0)\n    >>> clf = LogisticRegression(random_state=0)\n    >>> clf.fit(X_train, y_train)\n    LogisticRegression(random_state=0)\n    >>> y_prob = clf.predict_proba(X_test)[:, 1]\n    >>> prob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10)\n    >>> disp = CalibrationDisplay(prob_true, prob_pred, y_prob)\n    >>> disp.plot()\n    <...>\n    \"\"\"\n\n    def __init__(\n        self, prob_true, prob_pred, y_prob, *, estimator_name=None, pos_label=None\n    ):\n        self.prob_true = prob_true\n        self.prob_pred = prob_pred\n        self.y_prob = y_prob\n        self.estimator_name = estimator_name\n        self.pos_label = pos_label\n\n    def plot(self, *, ax=None, name=None, ref_line=True, **kwargs):\n        \"\"\"Plot visualization.\n\n        Extra keyword arguments will be passed to\n        :func:`matplotlib.pyplot.plot`.\n\n        Parameters\n        ----------\n        ax : Matplotlib Axes, default=None\n            Axes object to plot on. If `None`, a new figure and axes is\n            created.\n\n        name : str, default=None\n            Name for labeling curve. If `None`, use `estimator_name` if\n            not `None`, otherwise no labeling is shown.\n\n        ref_line : bool, default=True\n            If `True`, plots a reference line representing a perfectly\n            calibrated classifier.\n\n        **kwargs : dict\n            Keyword arguments to be passed to :func:`matplotlib.pyplot.plot`.\n\n        Returns\n        -------\n        display : :class:`~sklearn.calibration.CalibrationDisplay`\n            Object that stores computed values.\n        \"\"\"\n        check_matplotlib_support(\"CalibrationDisplay.plot\")\n        import matplotlib.pyplot as plt\n\n        if ax is None:\n            fig, ax = plt.subplots()\n\n        name = self.estimator_name if name is None else name\n        info_pos_label = (\n            f\"(Positive class: {self.pos_label})\" if self.pos_label is not None else \"\"\n        )\n\n        line_kwargs = {}\n        if name is not None:\n            line_kwargs[\"label\"] = name\n        line_kwargs.update(**kwargs)\n\n        ref_line_label = \"Perfectly calibrated\"\n        existing_ref_line = ref_line_label in ax.get_legend_handles_labels()[1]\n        if ref_line and not existing_ref_line:\n            ax.plot([0, 1], [0, 1], \"k:\", label=ref_line_label)\n        self.line_ = ax.plot(self.prob_pred, self.prob_true, \"s-\", **line_kwargs)[0]\n\n        # We always have to show the legend for at least the reference line\n        ax.legend(loc=\"lower right\")\n\n        xlabel = f\"Mean predicted probability {info_pos_label}\"\n        ylabel = f\"Fraction of positives {info_pos_label}\"\n        ax.set(xlabel=xlabel, ylabel=ylabel)\n\n        self.ax_ = ax\n        self.figure_ = ax.figure\n        return self\n\n    @classmethod\n    def from_estimator(\n        cls,\n        estimator,\n        X,\n        y,\n        *,\n        n_bins=5,\n        strategy=\"uniform\",\n        pos_label=None,\n        name=None,\n        ref_line=True,\n        ax=None,\n        **kwargs,\n    ):\n        \"\"\"Plot calibration curve using a binary classifier and data.\n\n        A calibration curve, also known as a reliability diagram, uses inputs\n        from a binary classifier and plots the average predicted probability\n        for each bin against the fraction of positive classes, on the\n        y-axis.\n\n        Extra keyword arguments will be passed to\n        :func:`matplotlib.pyplot.plot`.\n\n        Read more about calibration in the :ref:`User Guide <calibration>` and\n        more about the scikit-learn visualization API in :ref:`visualizations`.\n\n        .. versionadded:: 1.0\n\n        Parameters\n        ----------\n        estimator : estimator instance\n            Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\n            in which the last estimator is a classifier. The classifier must\n            have a :term:`predict_proba` method.\n\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Input values.\n\n        y : array-like of shape (n_samples,)\n            Binary target values.\n\n        n_bins : int, default=5\n            Number of bins to discretize the [0, 1] interval into when\n            calculating the calibration curve. A bigger number requires more\n            data.\n\n        strategy : {'uniform', 'quantile'}, default='uniform'\n            Strategy used to define the widths of the bins.\n\n            - `'uniform'`: The bins have identical widths.\n            - `'quantile'`: The bins have the same number of samples and depend\n              on predicted probabilities.\n\n        pos_label : str or int, default=None\n            The positive class when computing the calibration curve.\n            By default, `estimators.classes_[1]` is considered as the\n            positive class.\n\n            .. versionadded:: 1.1\n\n        name : str, default=None\n            Name for labeling curve. If `None`, the name of the estimator is\n            used.\n\n        ref_line : bool, default=True\n            If `True`, plots a reference line representing a perfectly\n            calibrated classifier.\n\n        ax : matplotlib axes, default=None\n            Axes object to plot on. If `None`, a new figure and axes is\n            created.\n\n        **kwargs : dict\n            Keyword arguments to be passed to :func:`matplotlib.pyplot.plot`.\n\n        Returns\n        -------\n        display : :class:`~sklearn.calibration.CalibrationDisplay`.\n            Object that stores computed values.\n\n        See Also\n        --------\n        CalibrationDisplay.from_predictions : Plot calibration curve using true\n            and predicted labels.\n\n        Examples\n        --------\n        >>> import matplotlib.pyplot as plt\n        >>> from sklearn.datasets import make_classification\n        >>> from sklearn.model_selection import train_test_split\n        >>> from sklearn.linear_model import LogisticRegression\n        >>> from sklearn.calibration import CalibrationDisplay\n        >>> X, y = make_classification(random_state=0)\n        >>> X_train, X_test, y_train, y_test = train_test_split(\n        ...     X, y, random_state=0)\n        >>> clf = LogisticRegression(random_state=0)\n        >>> clf.fit(X_train, y_train)\n        LogisticRegression(random_state=0)\n        >>> disp = CalibrationDisplay.from_estimator(clf, X_test, y_test)\n        >>> plt.show()\n        \"\"\"\n        method_name = f\"{cls.__name__}.from_estimator\"\n        check_matplotlib_support(method_name)\n\n        if not is_classifier(estimator):\n            raise ValueError(\"'estimator' should be a fitted classifier.\")\n\n        y_prob, pos_label = _get_response(\n            X, estimator, response_method=\"predict_proba\", pos_label=pos_label\n        )\n\n        name = name if name is not None else estimator.__class__.__name__\n        return cls.from_predictions(\n            y,\n            y_prob,\n            n_bins=n_bins,\n            strategy=strategy,\n            pos_label=pos_label,\n            name=name,\n            ref_line=ref_line,\n            ax=ax,\n            **kwargs,\n        )\n\n    @classmethod\n    def from_predictions(\n        cls,\n        y_true,\n        y_prob,\n        *,\n        n_bins=5,\n        strategy=\"uniform\",\n        pos_label=None,\n        name=None,\n        ref_line=True,\n        ax=None,\n        **kwargs,\n    ):\n        \"\"\"Plot calibration curve using true labels and predicted probabilities.\n\n        Calibration curve, also known as reliability diagram, uses inputs\n        from a binary classifier and plots the average predicted probability\n        for each bin against the fraction of positive classes, on the\n        y-axis.\n\n        Extra keyword arguments will be passed to\n        :func:`matplotlib.pyplot.plot`.\n\n        Read more about calibration in the :ref:`User Guide <calibration>` and\n        more about the scikit-learn visualization API in :ref:`visualizations`.\n\n        .. versionadded:: 1.0\n\n        Parameters\n        ----------\n        y_true : array-like of shape (n_samples,)\n            True labels.\n\n        y_prob : array-like of shape (n_samples,)\n            The predicted probabilities of the positive class.\n\n        n_bins : int, default=5\n            Number of bins to discretize the [0, 1] interval into when\n            calculating the calibration curve. A bigger number requires more\n            data.\n\n        strategy : {'uniform', 'quantile'}, default='uniform'\n            Strategy used to define the widths of the bins.\n\n            - `'uniform'`: The bins have identical widths.\n            - `'quantile'`: The bins have the same number of samples and depend\n              on predicted probabilities.\n\n        pos_label : str or int, default=None\n            The positive class when computing the calibration curve.\n            By default, `estimators.classes_[1]` is considered as the\n            positive class.\n\n            .. versionadded:: 1.1\n\n        name : str, default=None\n            Name for labeling curve.\n\n        ref_line : bool, default=True\n            If `True`, plots a reference line representing a perfectly\n            calibrated classifier.\n\n        ax : matplotlib axes, default=None\n            Axes object to plot on. If `None`, a new figure and axes is\n            created.\n\n        **kwargs : dict\n            Keyword arguments to be passed to :func:`matplotlib.pyplot.plot`.\n\n        Returns\n        -------\n        display : :class:`~sklearn.calibration.CalibrationDisplay`.\n            Object that stores computed values.\n\n        See Also\n        --------\n        CalibrationDisplay.from_estimator : Plot calibration curve using an\n            estimator and data.\n\n        Examples\n        --------\n        >>> import matplotlib.pyplot as plt\n        >>> from sklearn.datasets import make_classification\n        >>> from sklearn.model_selection import train_test_split\n        >>> from sklearn.linear_model import LogisticRegression\n        >>> from sklearn.calibration import CalibrationDisplay\n        >>> X, y = make_classification(random_state=0)\n        >>> X_train, X_test, y_train, y_test = train_test_split(\n        ...     X, y, random_state=0)\n        >>> clf = LogisticRegression(random_state=0)\n        >>> clf.fit(X_train, y_train)\n        LogisticRegression(random_state=0)\n        >>> y_prob = clf.predict_proba(X_test)[:, 1]\n        >>> disp = CalibrationDisplay.from_predictions(y_test, y_prob)\n        >>> plt.show()\n        \"\"\"\n        method_name = f\"{cls.__name__}.from_estimator\"\n        check_matplotlib_support(method_name)\n\n        prob_true, prob_pred = calibration_curve(\n            y_true, y_prob, n_bins=n_bins, strategy=strategy, pos_label=pos_label\n        )\n        name = \"Classifier\" if name is None else name\n        pos_label = _check_pos_label_consistency(pos_label, y_true)\n\n        disp = cls(\n            prob_true=prob_true,\n            prob_pred=prob_pred,\n            y_prob=y_prob,\n            estimator_name=name,\n            pos_label=pos_label,\n        )\n        return disp.plot(ax=ax, ref_line=ref_line, **kwargs)", "# Authors: Fabian Pedregosa <fabian@fseoane.net>\n#          Alexandre Gramfort <alexandre.gramfort@inria.fr>\n#          Nelle Varoquaux <nelle.varoquaux@gmail.com>\n# License: BSD 3 clause\n\nimport numpy as np\nfrom scipy import interpolate\nfrom scipy.stats import spearmanr\nfrom numbers import Real\nimport warnings\nimport math\n\nfrom .base import BaseEstimator, TransformerMixin, RegressorMixin\nfrom .utils import check_array, check_consistent_length\nfrom .utils.validation import _check_sample_weight, check_is_fitted\nfrom .utils._param_validation import Interval, StrOptions\nfrom ._isotonic import _inplace_contiguous_isotonic_regression, _make_unique\n\n\n__all__ = [\"check_increasing\", \"isotonic_regression\", \"IsotonicRegression\"]\n\n\ndef check_increasing(x, y):\n    \"\"\"Determine whether y is monotonically correlated with x.\n\n    y is found increasing or decreasing with respect to x based on a Spearman\n    correlation test.\n\n    Parameters\n    ----------\n    x : array-like of shape (n_samples,)\n            Training data.\n\n    y : array-like of shape (n_samples,)\n        Training target.\n\n    Returns\n    -------\n    increasing_bool : boolean\n        Whether the relationship is increasing or decreasing.\n\n    Notes\n    -----\n    The Spearman correlation coefficient is estimated from the data, and the\n    sign of the resulting estimate is used as the result.\n\n    In the event that the 95% confidence interval based on Fisher transform\n    spans zero, a warning is raised.\n\n    References\n    ----------\n    Fisher transformation. Wikipedia.\n    https://en.wikipedia.org/wiki/Fisher_transformation\n    \"\"\"\n\n    # Calculate Spearman rho estimate and set return accordingly.\n    rho, _ = spearmanr(x, y)\n    increasing_bool = rho >= 0\n\n    # Run Fisher transform to get the rho CI, but handle rho=+/-1\n    if rho not in [-1.0, 1.0] and len(x) > 3:\n        F = 0.5 * math.log((1.0 + rho) / (1.0 - rho))\n        F_se = 1 / math.sqrt(len(x) - 3)\n\n        # Use a 95% CI, i.e., +/-1.96 S.E.\n        # https://en.wikipedia.org/wiki/Fisher_transformation\n        rho_0 = math.tanh(F - 1.96 * F_se)\n        rho_1 = math.tanh(F + 1.96 * F_se)\n\n        # Warn if the CI spans zero.\n        if np.sign(rho_0) != np.sign(rho_1):\n            warnings.warn(\n                \"Confidence interval of the Spearman \"\n                \"correlation coefficient spans zero. \"\n                \"Determination of ``increasing`` may be \"\n                \"suspect.\"\n            )\n\n    return increasing_bool\n\n\ndef isotonic_regression(\n    y, *, sample_weight=None, y_min=None, y_max=None, increasing=True\n):\n    \"\"\"Solve the isotonic regression model.\n\n    Read more in the :ref:`User Guide <isotonic>`.\n\n    Parameters\n    ----------\n    y : array-like of shape (n_samples,)\n        The data.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Weights on each point of the regression.\n        If None, weight is set to 1 (equal weights).\n\n    y_min : float, default=None\n        Lower bound on the lowest predicted value (the minimum value may\n        still be higher). If not set, defaults to -inf.\n\n    y_max : float, default=None\n        Upper bound on the highest predicted value (the maximum may still be\n        lower). If not set, defaults to +inf.\n\n    increasing : bool, default=True\n        Whether to compute ``y_`` is increasing (if set to True) or decreasing\n        (if set to False).\n\n    Returns\n    -------\n    y_ : list of floats\n        Isotonic fit of y.\n\n    References\n    ----------\n    \"Active set algorithms for isotonic regression; A unifying framework\"\n    by Michael J. Best and Nilotpal Chakravarti, section 3.\n    \"\"\"\n    order = np.s_[:] if increasing else np.s_[::-1]\n    y = check_array(y, ensure_2d=False, input_name=\"y\", dtype=[np.float64, np.float32])\n    y = np.array(y[order], dtype=y.dtype)\n    sample_weight = _check_sample_weight(sample_weight, y, dtype=y.dtype, copy=True)\n    sample_weight = np.ascontiguousarray(sample_weight[order])\n\n    _inplace_contiguous_isotonic_regression(y, sample_weight)\n    if y_min is not None or y_max is not None:\n        # Older versions of np.clip don't accept None as a bound, so use np.inf\n        if y_min is None:\n            y_min = -np.inf\n        if y_max is None:\n            y_max = np.inf\n        np.clip(y, y_min, y_max, y)\n    return y[order]\n\n\nclass IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n    \"\"\"Isotonic regression model.\n\n    Read more in the :ref:`User Guide <isotonic>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    y_min : float, default=None\n        Lower bound on the lowest predicted value (the minimum value may\n        still be higher). If not set, defaults to -inf.\n\n    y_max : float, default=None\n        Upper bound on the highest predicted value (the maximum may still be\n        lower). If not set, defaults to +inf.\n\n    increasing : bool or 'auto', default=True\n        Determines whether the predictions should be constrained to increase\n        or decrease with `X`. 'auto' will decide based on the Spearman\n        correlation estimate's sign.\n\n    out_of_bounds : {'nan', 'clip', 'raise'}, default='nan'\n        Handles how `X` values outside of the training domain are handled\n        during prediction.\n\n        - 'nan', predictions will be NaN.\n        - 'clip', predictions will be set to the value corresponding to\n          the nearest train interval endpoint.\n        - 'raise', a `ValueError` is raised.\n\n    Attributes\n    ----------\n    X_min_ : float\n        Minimum value of input array `X_` for left bound.\n\n    X_max_ : float\n        Maximum value of input array `X_` for right bound.\n\n    X_thresholds_ : ndarray of shape (n_thresholds,)\n        Unique ascending `X` values used to interpolate\n        the y = f(X) monotonic function.\n\n        .. versionadded:: 0.24\n\n    y_thresholds_ : ndarray of shape (n_thresholds,)\n        De-duplicated `y` values suitable to interpolate the y = f(X)\n        monotonic function.\n\n        .. versionadded:: 0.24\n\n    f_ : function\n        The stepwise interpolating function that covers the input domain ``X``.\n\n    increasing_ : bool\n        Inferred value for ``increasing``.\n\n    See Also\n    --------\n    sklearn.linear_model.LinearRegression : Ordinary least squares Linear\n        Regression.\n    sklearn.ensemble.HistGradientBoostingRegressor : Gradient boosting that\n        is a non-parametric model accepting monotonicity constraints.\n    isotonic_regression : Function to solve the isotonic regression model.\n\n    Notes\n    -----\n    Ties are broken using the secondary method from de Leeuw, 1977.\n\n    References\n    ----------\n    Isotonic Median Regression: A Linear Programming Approach\n    Nilotpal Chakravarti\n    Mathematics of Operations Research\n    Vol. 14, No. 2 (May, 1989), pp. 303-308\n\n    Isotone Optimization in R : Pool-Adjacent-Violators\n    Algorithm (PAVA) and Active Set Methods\n    de Leeuw, Hornik, Mair\n    Journal of Statistical Software 2009\n\n    Correctness of Kruskal's algorithms for monotone regression with ties\n    de Leeuw, Psychometrica, 1977\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.isotonic import IsotonicRegression\n    >>> X, y = make_regression(n_samples=10, n_features=1, random_state=41)\n    >>> iso_reg = IsotonicRegression().fit(X, y)\n    >>> iso_reg.predict([.1, .2])\n    array([1.8628..., 3.7256...])\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"y_min\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"y_max\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"increasing\": [\"boolean\", StrOptions({\"auto\"})],\n        \"out_of_bounds\": [StrOptions({\"nan\", \"clip\", \"raise\"})],\n    }\n\n    def __init__(self, *, y_min=None, y_max=None, increasing=True, out_of_bounds=\"nan\"):\n        self.y_min = y_min\n        self.y_max = y_max\n        self.increasing = increasing\n        self.out_of_bounds = out_of_bounds\n\n    def _check_input_data_shape(self, X):\n        if not (X.ndim == 1 or (X.ndim == 2 and X.shape[1] == 1)):\n            msg = (\n                \"Isotonic regression input X should be a 1d array or \"\n                \"2d array with 1 feature\"\n            )\n            raise ValueError(msg)\n\n    def _build_f(self, X, y):\n        \"\"\"Build the f_ interp1d function.\"\"\"\n\n        bounds_error = self.out_of_bounds == \"raise\"\n        if len(y) == 1:\n            # single y, constant prediction\n            self.f_ = lambda x: y.repeat(x.shape)\n        else:\n            self.f_ = interpolate.interp1d(\n                X, y, kind=\"linear\", bounds_error=bounds_error\n            )\n\n    def _build_y(self, X, y, sample_weight, trim_duplicates=True):\n        \"\"\"Build the y_ IsotonicRegression.\"\"\"\n        self._check_input_data_shape(X)\n        X = X.reshape(-1)  # use 1d view\n\n        # Determine increasing if auto-determination requested\n        if self.increasing == \"auto\":\n            self.increasing_ = check_increasing(X, y)\n        else:\n            self.increasing_ = self.increasing\n\n        # If sample_weights is passed, removed zero-weight values and clean\n        # order\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        mask = sample_weight > 0\n        X, y, sample_weight = X[mask], y[mask], sample_weight[mask]\n\n        order = np.lexsort((y, X))\n        X, y, sample_weight = [array[order] for array in [X, y, sample_weight]]\n        unique_X, unique_y, unique_sample_weight = _make_unique(X, y, sample_weight)\n\n        X = unique_X\n        y = isotonic_regression(\n            unique_y,\n            sample_weight=unique_sample_weight,\n            y_min=self.y_min,\n            y_max=self.y_max,\n            increasing=self.increasing_,\n        )\n\n        # Handle the left and right bounds on X\n        self.X_min_, self.X_max_ = np.min(X), np.max(X)\n\n        if trim_duplicates:\n            # Remove unnecessary points for faster prediction\n            keep_data = np.ones((len(y),), dtype=bool)\n            # Aside from the 1st and last point, remove points whose y values\n            # are equal to both the point before and the point after it.\n            keep_data[1:-1] = np.logical_or(\n                np.not_equal(y[1:-1], y[:-2]), np.not_equal(y[1:-1], y[2:])\n            )\n            return X[keep_data], y[keep_data]\n        else:\n            # The ability to turn off trim_duplicates is only used to it make\n            # easier to unit test that removing duplicates in y does not have\n            # any impact the resulting interpolation function (besides\n            # prediction speed).\n            return X, y\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples,) or (n_samples, 1)\n            Training data.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        y : array-like of shape (n_samples,)\n            Training target.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Weights. If set to None, all weights will be set to 1 (equal\n            weights).\n\n        Returns\n        -------\n        self : object\n            Returns an instance of self.\n\n        Notes\n        -----\n        X is stored for future use, as :meth:`transform` needs X to interpolate\n        new input data.\n        \"\"\"\n        self._validate_params()\n        check_params = dict(accept_sparse=False, ensure_2d=False)\n        X = check_array(\n            X, input_name=\"X\", dtype=[np.float64, np.float32], **check_params\n        )\n        y = check_array(y, input_name=\"y\", dtype=X.dtype, **check_params)\n        check_consistent_length(X, y, sample_weight)\n\n        # Transform y by running the isotonic regression algorithm and\n        # transform X accordingly.\n        X, y = self._build_y(X, y, sample_weight)\n\n        # It is necessary to store the non-redundant part of the training set\n        # on the model to make it possible to support model persistence via\n        # the pickle module as the object built by scipy.interp1d is not\n        # picklable directly.\n        self.X_thresholds_, self.y_thresholds_ = X, y\n\n        # Build the interpolation function\n        self._build_f(X, y)\n        return self\n\n    def transform(self, T):\n        \"\"\"Transform new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            The transformed data.\n        \"\"\"\n\n        if hasattr(self, \"X_thresholds_\"):\n            dtype = self.X_thresholds_.dtype\n        else:\n            dtype = np.float64\n\n        T = check_array(T, dtype=dtype, ensure_2d=False)\n\n        self._check_input_data_shape(T)\n        T = T.reshape(-1)  # use 1d view\n\n        if self.out_of_bounds == \"clip\":\n            T = np.clip(T, self.X_min_, self.X_max_)\n\n        res = self.f_(T)\n\n        # on scipy 0.17, interp1d up-casts to float64, so we cast back\n        res = res.astype(T.dtype)\n\n        return res\n\n    def predict(self, T):\n        \"\"\"Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            Transformed data.\n        \"\"\"\n        return self.transform(T)\n\n    # We implement get_feature_names_out here instead of using\n    # `ClassNamePrefixFeaturesOutMixin`` because `input_features` are ignored.\n    # `input_features` are ignored because `IsotonicRegression` accepts 1d\n    # arrays and the semantics of `feature_names_in_` are not clear for 1d arrays.\n    def get_feature_names_out(self, input_features=None):\n        \"\"\"Get output feature names for transformation.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Ignored.\n\n        Returns\n        -------\n        feature_names_out : ndarray of str objects\n            An ndarray with one string i.e. [\"isotonicregression0\"].\n        \"\"\"\n        check_is_fitted(self, \"f_\")\n        class_name = self.__class__.__name__.lower()\n        return np.asarray([f\"{class_name}0\"], dtype=object)\n\n    def __getstate__(self):\n        \"\"\"Pickle-protocol - return state of the estimator.\"\"\"\n        state = super().__getstate__()\n        # remove interpolation method\n        state.pop(\"f_\", None)\n        return state\n\n    def __setstate__(self, state):\n        \"\"\"Pickle-protocol - set state of the estimator.\n\n        We need to rebuild the interpolation function.\n        \"\"\"\n        super().__setstate__(state)\n        if hasattr(self, \"X_thresholds_\") and hasattr(self, \"y_thresholds_\"):\n            self._build_f(self.X_thresholds_, self.y_thresholds_)\n\n    def _more_tags(self):\n        return {\"X_types\": [\"1darray\"]}", "# Authors: Fabian Pedregosa <fabian@fseoane.net>\n#          Alexandre Gramfort <alexandre.gramfort@inria.fr>\n#          Nelle Varoquaux <nelle.varoquaux@gmail.com>\n# License: BSD 3 clause\n\nimport numpy as np\nfrom scipy import interpolate\nfrom scipy.stats import spearmanr\nfrom numbers import Real\nimport warnings\nimport math\n\nfrom .base import BaseEstimator, TransformerMixin, RegressorMixin\nfrom .utils import check_array, check_consistent_length\nfrom .utils.validation import _check_sample_weight, check_is_fitted\nfrom .utils._param_validation import Interval, StrOptions\nfrom ._isotonic import _inplace_contiguous_isotonic_regression, _make_unique\n\n\n__all__ = [\"check_increasing\", \"isotonic_regression\", \"IsotonicRegression\"]\n\n\ndef check_increasing(x, y):\n    \"\"\"Determine whether y is monotonically correlated with x.\n\n    y is found increasing or decreasing with respect to x based on a Spearman\n    correlation test.\n\n    Parameters\n    ----------\n    x : array-like of shape (n_samples,)\n            Training data.\n\n    y : array-like of shape (n_samples,)\n        Training target.\n\n    Returns\n    -------\n    increasing_bool : boolean\n        Whether the relationship is increasing or decreasing.\n\n    Notes\n    -----\n    The Spearman correlation coefficient is estimated from the data, and the\n    sign of the resulting estimate is used as the result.\n\n    In the event that the 95% confidence interval based on Fisher transform\n    spans zero, a warning is raised.\n\n    References\n    ----------\n    Fisher transformation. Wikipedia.\n    https://en.wikipedia.org/wiki/Fisher_transformation\n    \"\"\"\n\n    # Calculate Spearman rho estimate and set return accordingly.\n    rho, _ = spearmanr(x, y)\n    increasing_bool = rho >= 0\n\n    # Run Fisher transform to get the rho CI, but handle rho=+/-1\n    if rho not in [-1.0, 1.0] and len(x) > 3:\n        F = 0.5 * math.log((1.0 + rho) / (1.0 - rho))\n        F_se = 1 / math.sqrt(len(x) - 3)\n\n        # Use a 95% CI, i.e., +/-1.96 S.E.\n        # https://en.wikipedia.org/wiki/Fisher_transformation\n        rho_0 = math.tanh(F - 1.96 * F_se)\n        rho_1 = math.tanh(F + 1.96 * F_se)\n\n        # Warn if the CI spans zero.\n        if np.sign(rho_0) != np.sign(rho_1):\n            warnings.warn(\n                \"Confidence interval of the Spearman \"\n                \"correlation coefficient spans zero. \"\n                \"Determination of ``increasing`` may be \"\n                \"suspect.\"\n            )\n\n    return increasing_bool\n\n\ndef isotonic_regression(\n    y, *, sample_weight=None, y_min=None, y_max=None, increasing=True\n):\n    \"\"\"Solve the isotonic regression model.\n\n    Read more in the :ref:`User Guide <isotonic>`.\n\n    Parameters\n    ----------\n    y : array-like of shape (n_samples,)\n        The data.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Weights on each point of the regression.\n        If None, weight is set to 1 (equal weights).\n\n    y_min : float, default=None\n        Lower bound on the lowest predicted value (the minimum value may\n        still be higher). If not set, defaults to -inf.\n\n    y_max : float, default=None\n        Upper bound on the highest predicted value (the maximum may still be\n        lower). If not set, defaults to +inf.\n\n    increasing : bool, default=True\n        Whether to compute ``y_`` is increasing (if set to True) or decreasing\n        (if set to False).\n\n    Returns\n    -------\n    y_ : list of floats\n        Isotonic fit of y.\n\n    References\n    ----------\n    \"Active set algorithms for isotonic regression; A unifying framework\"\n    by Michael J. Best and Nilotpal Chakravarti, section 3.\n    \"\"\"\n    order = np.s_[:] if increasing else np.s_[::-1]\n    y = check_array(y, ensure_2d=False, input_name=\"y\", dtype=[np.float64, np.float32])\n    y = np.array(y[order], dtype=y.dtype)\n    sample_weight = _check_sample_weight(sample_weight, y, dtype=y.dtype, copy=True)\n    sample_weight = np.ascontiguousarray(sample_weight[order])\n\n    _inplace_contiguous_isotonic_regression(y, sample_weight)\n    if y_min is not None or y_max is not None:\n        # Older versions of np.clip don't accept None as a bound, so use np.inf\n        if y_min is None:\n            y_min = -np.inf\n        if y_max is None:\n            y_max = np.inf\n        np.clip(y, y_min, y_max, y)\n    return y[order]\n\n\nclass IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n    \"\"\"Isotonic regression model.\n\n    Read more in the :ref:`User Guide <isotonic>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    y_min : float, default=None\n        Lower bound on the lowest predicted value (the minimum value may\n        still be higher). If not set, defaults to -inf.\n\n    y_max : float, default=None\n        Upper bound on the highest predicted value (the maximum may still be\n        lower). If not set, defaults to +inf.\n\n    increasing : bool or 'auto', default=True\n        Determines whether the predictions should be constrained to increase\n        or decrease with `X`. 'auto' will decide based on the Spearman\n        correlation estimate's sign.\n\n    out_of_bounds : {'nan', 'clip', 'raise'}, default='nan'\n        Handles how `X` values outside of the training domain are handled\n        during prediction.\n\n        - 'nan', predictions will be NaN.\n        - 'clip', predictions will be set to the value corresponding to\n          the nearest train interval endpoint.\n        - 'raise', a `ValueError` is raised.\n\n    Attributes\n    ----------\n    X_min_ : float\n        Minimum value of input array `X_` for left bound.\n\n    X_max_ : float\n        Maximum value of input array `X_` for right bound.\n\n    X_thresholds_ : ndarray of shape (n_thresholds,)\n        Unique ascending `X` values used to interpolate\n        the y = f(X) monotonic function.\n\n        .. versionadded:: 0.24\n\n    y_thresholds_ : ndarray of shape (n_thresholds,)\n        De-duplicated `y` values suitable to interpolate the y = f(X)\n        monotonic function.\n\n        .. versionadded:: 0.24\n\n    f_ : function\n        The stepwise interpolating function that covers the input domain ``X``.\n\n    increasing_ : bool\n        Inferred value for ``increasing``.\n\n    See Also\n    --------\n    sklearn.linear_model.LinearRegression : Ordinary least squares Linear\n        Regression.\n    sklearn.ensemble.HistGradientBoostingRegressor : Gradient boosting that\n        is a non-parametric model accepting monotonicity constraints.\n    isotonic_regression : Function to solve the isotonic regression model.\n\n    Notes\n    -----\n    Ties are broken using the secondary method from de Leeuw, 1977.\n\n    References\n    ----------\n    Isotonic Median Regression: A Linear Programming Approach\n    Nilotpal Chakravarti\n    Mathematics of Operations Research\n    Vol. 14, No. 2 (May, 1989), pp. 303-308\n\n    Isotone Optimization in R : Pool-Adjacent-Violators\n    Algorithm (PAVA) and Active Set Methods\n    de Leeuw, Hornik, Mair\n    Journal of Statistical Software 2009\n\n    Correctness of Kruskal's algorithms for monotone regression with ties\n    de Leeuw, Psychometrica, 1977\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.isotonic import IsotonicRegression\n    >>> X, y = make_regression(n_samples=10, n_features=1, random_state=41)\n    >>> iso_reg = IsotonicRegression().fit(X, y)\n    >>> iso_reg.predict([.1, .2])\n    array([1.8628..., 3.7256...])\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"y_min\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"y_max\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"increasing\": [\"boolean\", StrOptions({\"auto\"})],\n        \"out_of_bounds\": [StrOptions({\"nan\", \"clip\", \"raise\"})],\n    }\n\n    def __init__(self, *, y_min=None, y_max=None, increasing=True, out_of_bounds=\"nan\"):\n        self.y_min = y_min\n        self.y_max = y_max\n        self.increasing = increasing\n        self.out_of_bounds = out_of_bounds\n\n    def _check_input_data_shape(self, X):\n        if not (X.ndim == 1 or (X.ndim == 2 and X.shape[1] == 1)):\n            msg = (\n                \"Isotonic regression input X should be a 1d array or \"\n                \"2d array with 1 feature\"\n            )\n            raise ValueError(msg)\n\n    def _build_f(self, X, y):\n        \"\"\"Build the f_ interp1d function.\"\"\"\n\n        bounds_error = self.out_of_bounds == \"raise\"\n        if len(y) == 1:\n            # single y, constant prediction\n            self.f_ = lambda x: y.repeat(x.shape)\n        else:\n            self.f_ = interpolate.interp1d(\n                X, y, kind=\"linear\", bounds_error=bounds_error\n            )\n\n    def _build_y(self, X, y, sample_weight, trim_duplicates=True):\n        \"\"\"Build the y_ IsotonicRegression.\"\"\"\n        self._check_input_data_shape(X)\n        X = X.reshape(-1)  # use 1d view\n\n        # Determine increasing if auto-determination requested\n        if self.increasing == \"auto\":\n            self.increasing_ = check_increasing(X, y)\n        else:\n            self.increasing_ = self.increasing\n\n        # If sample_weights is passed, removed zero-weight values and clean\n        # order\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        mask = sample_weight > 0\n        X, y, sample_weight = X[mask], y[mask], sample_weight[mask]\n\n        order = np.lexsort((y, X))\n        X, y, sample_weight = [array[order] for array in [X, y, sample_weight]]\n        unique_X, unique_y, unique_sample_weight = _make_unique(X, y, sample_weight)\n\n        X = unique_X\n        y = isotonic_regression(\n            unique_y,\n            sample_weight=unique_sample_weight,\n            y_min=self.y_min,\n            y_max=self.y_max,\n            increasing=self.increasing_,\n        )\n\n        # Handle the left and right bounds on X\n        self.X_min_, self.X_max_ = np.min(X), np.max(X)\n\n        if trim_duplicates:\n            # Remove unnecessary points for faster prediction\n            keep_data = np.ones((len(y),), dtype=bool)\n            # Aside from the 1st and last point, remove points whose y values\n            # are equal to both the point before and the point after it.\n            keep_data[1:-1] = np.logical_or(\n                np.not_equal(y[1:-1], y[:-2]), np.not_equal(y[1:-1], y[2:])\n            )\n            return X[keep_data], y[keep_data]\n        else:\n            # The ability to turn off trim_duplicates is only used to it make\n            # easier to unit test that removing duplicates in y does not have\n            # any impact the resulting interpolation function (besides\n            # prediction speed).\n            return X, y\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples,) or (n_samples, 1)\n            Training data.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        y : array-like of shape (n_samples,)\n            Training target.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Weights. If set to None, all weights will be set to 1 (equal\n            weights).\n\n        Returns\n        -------\n        self : object\n            Returns an instance of self.\n\n        Notes\n        -----\n        X is stored for future use, as :meth:`transform` needs X to interpolate\n        new input data.\n        \"\"\"\n        self._validate_params()\n        check_params = dict(accept_sparse=False, ensure_2d=False)\n        X = check_array(\n            X, input_name=\"X\", dtype=[np.float64, np.float32], **check_params\n        )\n        y = check_array(y, input_name=\"y\", dtype=X.dtype, **check_params)\n        check_consistent_length(X, y, sample_weight)\n\n        # Transform y by running the isotonic regression algorithm and\n        # transform X accordingly.\n        X, y = self._build_y(X, y, sample_weight)\n\n        # It is necessary to store the non-redundant part of the training set\n        # on the model to make it possible to support model persistence via\n        # the pickle module as the object built by scipy.interp1d is not\n        # picklable directly.\n        self.X_thresholds_, self.y_thresholds_ = X, y\n\n        # Build the interpolation function\n        self._build_f(X, y)\n        return self\n\n    def transform(self, T):\n        \"\"\"Transform new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            The transformed data.\n        \"\"\"\n\n        if hasattr(self, \"X_thresholds_\"):\n            dtype = self.X_thresholds_.dtype\n        else:\n            dtype = np.float64\n\n        T = check_array(T, dtype=dtype, ensure_2d=False)\n\n        self._check_input_data_shape(T)\n        T = T.reshape(-1)  # use 1d view\n\n        if self.out_of_bounds == \"clip\":\n            T = np.clip(T, self.X_min_, self.X_max_)\n\n        res = self.f_(T)\n\n        # on scipy 0.17, interp1d up-casts to float64, so we cast back\n        res = res.astype(T.dtype)\n\n        return res\n\n    def predict(self, T):\n        \"\"\"Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            Transformed data.\n        \"\"\"\n        return self.transform(T)\n\n    # We implement get_feature_names_out here instead of using\n    # `ClassNamePrefixFeaturesOutMixin`` because `input_features` are ignored.\n    # `input_features` are ignored because `IsotonicRegression` accepts 1d\n    # arrays and the semantics of `feature_names_in_` are not clear for 1d arrays.\n    def get_feature_names_out(self, input_features=None):\n        \"\"\"Get output feature names for transformation.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Ignored.\n\n        Returns\n        -------\n        feature_names_out : ndarray of str objects\n            An ndarray with one string i.e. [\"isotonicregression0\"].\n        \"\"\"\n        check_is_fitted(self, \"f_\")\n        class_name = self.__class__.__name__.lower()\n        return np.asarray([f\"{class_name}0\"], dtype=object)\n\n    def __getstate__(self):\n        \"\"\"Pickle-protocol - return state of the estimator.\"\"\"\n        state = super().__getstate__()\n        # remove interpolation method\n        state.pop(\"f_\", None)\n        return state\n\n    def __setstate__(self, state):\n        \"\"\"Pickle-protocol - set state of the estimator.\n\n        We need to rebuild the interpolation function.\n        \"\"\"\n        super().__setstate__(state)\n        if hasattr(self, \"X_thresholds_\") and hasattr(self, \"y_thresholds_\"):\n            self._build_f(self.X_thresholds_, self.y_thresholds_)\n\n    def _more_tags(self):\n        return {\"X_types\": [\"1darray\"]}"]], "file_names": [["sklearn/isotonic.py", "sklearn/isotonic.py", "sklearn/calibration.py", "sklearn/isotonic.py", "sklearn/isotonic.py", "sklearn/isotonic.py", "sklearn/calibration.py", "sklearn/calibration.py", "sklearn/isotonic.py", "sklearn/isotonic.py"]]}
