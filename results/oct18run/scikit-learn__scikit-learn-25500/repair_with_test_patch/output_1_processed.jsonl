{"model_name_or_path": "agentless", "instance_id": "scikit-learn__scikit-learn-25500", "model_patch": "diff --git a/sklearn/isotonic.py b/sklearn/isotonic.py\nindex 6365edf..8452d9b 100644\n--- a/sklearn/isotonic.py\n+++ b/sklearn/isotonic.py\n@@ -395,6 +395,9 @@ class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n         # on scipy 0.17, interp1d up-casts to float64, so we cast back\n         res = res.astype(T.dtype)\n \n+        import pandas as pd\n+        if isinstance(res, pd.DataFrame):\n+            res = res.to_numpy()\n         return res\n \n     def predict(self, T):\n@@ -450,4 +453,4 @@ class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n             self._build_f(self.X_thresholds_, self.y_thresholds_)\n \n     def _more_tags(self):\n-        return {\"X_types\": [\"1darray\"]}\n+        return {\"X_types\": [\"1darray\"]}\n", "raw_model_patch": "diff --git a/sklearn/isotonic.py b/sklearn/isotonic.py\nindex 6365edf..8452d9b 100644\n--- a/sklearn/isotonic.py\n+++ b/sklearn/isotonic.py\n@@ -395,6 +395,9 @@ class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n         # on scipy 0.17, interp1d up-casts to float64, so we cast back\n         res = res.astype(T.dtype)\n \n+        import pandas as pd\n+        if isinstance(res, pd.DataFrame):\n+            res = res.to_numpy()\n         return res\n \n     def predict(self, T):\n@@ -450,4 +453,4 @@ class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n             self._build_f(self.X_thresholds_, self.y_thresholds_)\n \n     def _more_tags(self):\n-        return {\"X_types\": [\"1darray\"]}\n+        return {\"X_types\": [\"1darray\"]}\n", "original_file_content": "# Authors: Fabian Pedregosa <fabian@fseoane.net>\n#          Alexandre Gramfort <alexandre.gramfort@inria.fr>\n#          Nelle Varoquaux <nelle.varoquaux@gmail.com>\n# License: BSD 3 clause\n\nimport numpy as np\nfrom scipy import interpolate\nfrom scipy.stats import spearmanr\nfrom numbers import Real\nimport warnings\nimport math\n\nfrom .base import BaseEstimator, TransformerMixin, RegressorMixin\nfrom .utils import check_array, check_consistent_length\nfrom .utils.validation import _check_sample_weight, check_is_fitted\nfrom .utils._param_validation import Interval, StrOptions\nfrom ._isotonic import _inplace_contiguous_isotonic_regression, _make_unique\n\n\n__all__ = [\"check_increasing\", \"isotonic_regression\", \"IsotonicRegression\"]\n\n\ndef check_increasing(x, y):\n    \"\"\"Determine whether y is monotonically correlated with x.\n\n    y is found increasing or decreasing with respect to x based on a Spearman\n    correlation test.\n\n    Parameters\n    ----------\n    x : array-like of shape (n_samples,)\n            Training data.\n\n    y : array-like of shape (n_samples,)\n        Training target.\n\n    Returns\n    -------\n    increasing_bool : boolean\n        Whether the relationship is increasing or decreasing.\n\n    Notes\n    -----\n    The Spearman correlation coefficient is estimated from the data, and the\n    sign of the resulting estimate is used as the result.\n\n    In the event that the 95% confidence interval based on Fisher transform\n    spans zero, a warning is raised.\n\n    References\n    ----------\n    Fisher transformation. Wikipedia.\n    https://en.wikipedia.org/wiki/Fisher_transformation\n    \"\"\"\n\n    # Calculate Spearman rho estimate and set return accordingly.\n    rho, _ = spearmanr(x, y)\n    increasing_bool = rho >= 0\n\n    # Run Fisher transform to get the rho CI, but handle rho=+/-1\n    if rho not in [-1.0, 1.0] and len(x) > 3:\n        F = 0.5 * math.log((1.0 + rho) / (1.0 - rho))\n        F_se = 1 / math.sqrt(len(x) - 3)\n\n        # Use a 95% CI, i.e., +/-1.96 S.E.\n        # https://en.wikipedia.org/wiki/Fisher_transformation\n        rho_0 = math.tanh(F - 1.96 * F_se)\n        rho_1 = math.tanh(F + 1.96 * F_se)\n\n        # Warn if the CI spans zero.\n        if np.sign(rho_0) != np.sign(rho_1):\n            warnings.warn(\n                \"Confidence interval of the Spearman \"\n                \"correlation coefficient spans zero. \"\n                \"Determination of ``increasing`` may be \"\n                \"suspect.\"\n            )\n\n    return increasing_bool\n\n\ndef isotonic_regression(\n    y, *, sample_weight=None, y_min=None, y_max=None, increasing=True\n):\n    \"\"\"Solve the isotonic regression model.\n\n    Read more in the :ref:`User Guide <isotonic>`.\n\n    Parameters\n    ----------\n    y : array-like of shape (n_samples,)\n        The data.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Weights on each point of the regression.\n        If None, weight is set to 1 (equal weights).\n\n    y_min : float, default=None\n        Lower bound on the lowest predicted value (the minimum value may\n        still be higher). If not set, defaults to -inf.\n\n    y_max : float, default=None\n        Upper bound on the highest predicted value (the maximum may still be\n        lower). If not set, defaults to +inf.\n\n    increasing : bool, default=True\n        Whether to compute ``y_`` is increasing (if set to True) or decreasing\n        (if set to False).\n\n    Returns\n    -------\n    y_ : list of floats\n        Isotonic fit of y.\n\n    References\n    ----------\n    \"Active set algorithms for isotonic regression; A unifying framework\"\n    by Michael J. Best and Nilotpal Chakravarti, section 3.\n    \"\"\"\n    order = np.s_[:] if increasing else np.s_[::-1]\n    y = check_array(y, ensure_2d=False, input_name=\"y\", dtype=[np.float64, np.float32])\n    y = np.array(y[order], dtype=y.dtype)\n    sample_weight = _check_sample_weight(sample_weight, y, dtype=y.dtype, copy=True)\n    sample_weight = np.ascontiguousarray(sample_weight[order])\n\n    _inplace_contiguous_isotonic_regression(y, sample_weight)\n    if y_min is not None or y_max is not None:\n        # Older versions of np.clip don't accept None as a bound, so use np.inf\n        if y_min is None:\n            y_min = -np.inf\n        if y_max is None:\n            y_max = np.inf\n        np.clip(y, y_min, y_max, y)\n    return y[order]\n\n\nclass IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n    \"\"\"Isotonic regression model.\n\n    Read more in the :ref:`User Guide <isotonic>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    y_min : float, default=None\n        Lower bound on the lowest predicted value (the minimum value may\n        still be higher). If not set, defaults to -inf.\n\n    y_max : float, default=None\n        Upper bound on the highest predicted value (the maximum may still be\n        lower). If not set, defaults to +inf.\n\n    increasing : bool or 'auto', default=True\n        Determines whether the predictions should be constrained to increase\n        or decrease with `X`. 'auto' will decide based on the Spearman\n        correlation estimate's sign.\n\n    out_of_bounds : {'nan', 'clip', 'raise'}, default='nan'\n        Handles how `X` values outside of the training domain are handled\n        during prediction.\n\n        - 'nan', predictions will be NaN.\n        - 'clip', predictions will be set to the value corresponding to\n          the nearest train interval endpoint.\n        - 'raise', a `ValueError` is raised.\n\n    Attributes\n    ----------\n    X_min_ : float\n        Minimum value of input array `X_` for left bound.\n\n    X_max_ : float\n        Maximum value of input array `X_` for right bound.\n\n    X_thresholds_ : ndarray of shape (n_thresholds,)\n        Unique ascending `X` values used to interpolate\n        the y = f(X) monotonic function.\n\n        .. versionadded:: 0.24\n\n    y_thresholds_ : ndarray of shape (n_thresholds,)\n        De-duplicated `y` values suitable to interpolate the y = f(X)\n        monotonic function.\n\n        .. versionadded:: 0.24\n\n    f_ : function\n        The stepwise interpolating function that covers the input domain ``X``.\n\n    increasing_ : bool\n        Inferred value for ``increasing``.\n\n    See Also\n    --------\n    sklearn.linear_model.LinearRegression : Ordinary least squares Linear\n        Regression.\n    sklearn.ensemble.HistGradientBoostingRegressor : Gradient boosting that\n        is a non-parametric model accepting monotonicity constraints.\n    isotonic_regression : Function to solve the isotonic regression model.\n\n    Notes\n    -----\n    Ties are broken using the secondary method from de Leeuw, 1977.\n\n    References\n    ----------\n    Isotonic Median Regression: A Linear Programming Approach\n    Nilotpal Chakravarti\n    Mathematics of Operations Research\n    Vol. 14, No. 2 (May, 1989), pp. 303-308\n\n    Isotone Optimization in R : Pool-Adjacent-Violators\n    Algorithm (PAVA) and Active Set Methods\n    de Leeuw, Hornik, Mair\n    Journal of Statistical Software 2009\n\n    Correctness of Kruskal's algorithms for monotone regression with ties\n    de Leeuw, Psychometrica, 1977\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_regression\n    >>> from sklearn.isotonic import IsotonicRegression\n    >>> X, y = make_regression(n_samples=10, n_features=1, random_state=41)\n    >>> iso_reg = IsotonicRegression().fit(X, y)\n    >>> iso_reg.predict([.1, .2])\n    array([1.8628..., 3.7256...])\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"y_min\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"y_max\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"increasing\": [\"boolean\", StrOptions({\"auto\"})],\n        \"out_of_bounds\": [StrOptions({\"nan\", \"clip\", \"raise\"})],\n    }\n\n    def __init__(self, *, y_min=None, y_max=None, increasing=True, out_of_bounds=\"nan\"):\n        self.y_min = y_min\n        self.y_max = y_max\n        self.increasing = increasing\n        self.out_of_bounds = out_of_bounds\n\n    def _check_input_data_shape(self, X):\n        if not (X.ndim == 1 or (X.ndim == 2 and X.shape[1] == 1)):\n            msg = (\n                \"Isotonic regression input X should be a 1d array or \"\n                \"2d array with 1 feature\"\n            )\n            raise ValueError(msg)\n\n    def _build_f(self, X, y):\n        \"\"\"Build the f_ interp1d function.\"\"\"\n\n        bounds_error = self.out_of_bounds == \"raise\"\n        if len(y) == 1:\n            # single y, constant prediction\n            self.f_ = lambda x: y.repeat(x.shape)\n        else:\n            self.f_ = interpolate.interp1d(\n                X, y, kind=\"linear\", bounds_error=bounds_error\n            )\n\n    def _build_y(self, X, y, sample_weight, trim_duplicates=True):\n        \"\"\"Build the y_ IsotonicRegression.\"\"\"\n        self._check_input_data_shape(X)\n        X = X.reshape(-1)  # use 1d view\n\n        # Determine increasing if auto-determination requested\n        if self.increasing == \"auto\":\n            self.increasing_ = check_increasing(X, y)\n        else:\n            self.increasing_ = self.increasing\n\n        # If sample_weights is passed, removed zero-weight values and clean\n        # order\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n        mask = sample_weight > 0\n        X, y, sample_weight = X[mask], y[mask], sample_weight[mask]\n\n        order = np.lexsort((y, X))\n        X, y, sample_weight = [array[order] for array in [X, y, sample_weight]]\n        unique_X, unique_y, unique_sample_weight = _make_unique(X, y, sample_weight)\n\n        X = unique_X\n        y = isotonic_regression(\n            unique_y,\n            sample_weight=unique_sample_weight,\n            y_min=self.y_min,\n            y_max=self.y_max,\n            increasing=self.increasing_,\n        )\n\n        # Handle the left and right bounds on X\n        self.X_min_, self.X_max_ = np.min(X), np.max(X)\n\n        if trim_duplicates:\n            # Remove unnecessary points for faster prediction\n            keep_data = np.ones((len(y),), dtype=bool)\n            # Aside from the 1st and last point, remove points whose y values\n            # are equal to both the point before and the point after it.\n            keep_data[1:-1] = np.logical_or(\n                np.not_equal(y[1:-1], y[:-2]), np.not_equal(y[1:-1], y[2:])\n            )\n            return X[keep_data], y[keep_data]\n        else:\n            # The ability to turn off trim_duplicates is only used to it make\n            # easier to unit test that removing duplicates in y does not have\n            # any impact the resulting interpolation function (besides\n            # prediction speed).\n            return X, y\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples,) or (n_samples, 1)\n            Training data.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        y : array-like of shape (n_samples,)\n            Training target.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Weights. If set to None, all weights will be set to 1 (equal\n            weights).\n\n        Returns\n        -------\n        self : object\n            Returns an instance of self.\n\n        Notes\n        -----\n        X is stored for future use, as :meth:`transform` needs X to interpolate\n        new input data.\n        \"\"\"\n        self._validate_params()\n        check_params = dict(accept_sparse=False, ensure_2d=False)\n        X = check_array(\n            X, input_name=\"X\", dtype=[np.float64, np.float32], **check_params\n        )\n        y = check_array(y, input_name=\"y\", dtype=X.dtype, **check_params)\n        check_consistent_length(X, y, sample_weight)\n\n        # Transform y by running the isotonic regression algorithm and\n        # transform X accordingly.\n        X, y = self._build_y(X, y, sample_weight)\n\n        # It is necessary to store the non-redundant part of the training set\n        # on the model to make it possible to support model persistence via\n        # the pickle module as the object built by scipy.interp1d is not\n        # picklable directly.\n        self.X_thresholds_, self.y_thresholds_ = X, y\n\n        # Build the interpolation function\n        self._build_f(X, y)\n        return self\n\n    def transform(self, T):\n        \"\"\"Transform new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n            .. versionchanged:: 0.24\n               Also accepts 2d array with 1 feature.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            The transformed data.\n        \"\"\"\n\n        if hasattr(self, \"X_thresholds_\"):\n            dtype = self.X_thresholds_.dtype\n        else:\n            dtype = np.float64\n\n        T = check_array(T, dtype=dtype, ensure_2d=False)\n\n        self._check_input_data_shape(T)\n        T = T.reshape(-1)  # use 1d view\n\n        if self.out_of_bounds == \"clip\":\n            T = np.clip(T, self.X_min_, self.X_max_)\n\n        res = self.f_(T)\n\n        # on scipy 0.17, interp1d up-casts to float64, so we cast back\n        res = res.astype(T.dtype)\n\n        return res\n\n    def predict(self, T):\n        \"\"\"Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like of shape (n_samples,) or (n_samples, 1)\n            Data to transform.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            Transformed data.\n        \"\"\"\n        return self.transform(T)\n\n    # We implement get_feature_names_out here instead of using\n    # `ClassNamePrefixFeaturesOutMixin`` because `input_features` are ignored.\n    # `input_features` are ignored because `IsotonicRegression` accepts 1d\n    # arrays and the semantics of `feature_names_in_` are not clear for 1d arrays.\n    def get_feature_names_out(self, input_features=None):\n        \"\"\"Get output feature names for transformation.\n\n        Parameters\n        ----------\n        input_features : array-like of str or None, default=None\n            Ignored.\n\n        Returns\n        -------\n        feature_names_out : ndarray of str objects\n            An ndarray with one string i.e. [\"isotonicregression0\"].\n        \"\"\"\n        check_is_fitted(self, \"f_\")\n        class_name = self.__class__.__name__.lower()\n        return np.asarray([f\"{class_name}0\"], dtype=object)\n\n    def __getstate__(self):\n        \"\"\"Pickle-protocol - return state of the estimator.\"\"\"\n        state = super().__getstate__()\n        # remove interpolation method\n        state.pop(\"f_\", None)\n        return state\n\n    def __setstate__(self, state):\n        \"\"\"Pickle-protocol - set state of the estimator.\n\n        We need to rebuild the interpolation function.\n        \"\"\"\n        super().__setstate__(state)\n        if hasattr(self, \"X_thresholds_\") and hasattr(self, \"y_thresholds_\"):\n            self._build_f(self.X_thresholds_, self.y_thresholds_)\n\n    def _more_tags(self):\n        return {\"X_types\": [\"1darray\"]}"}
