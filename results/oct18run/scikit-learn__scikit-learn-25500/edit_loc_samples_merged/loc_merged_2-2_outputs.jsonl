{"instance_id": "scikit-learn__scikit-learn-25500", "found_files": ["sklearn/calibration.py", "sklearn/isotonic.py", "sklearn/_config.py", "sklearn/utils/_set_output.py", "sklearn/utils/validation.py"], "additional_artifact_loc_file": {"raw_output_files": "```\nsklearn/calibration.py\nsklearn/isotonic.py\nsklearn/_config.py\nsklearn/utils/_set_output.py\nsklearn/utils/validation.py\n```"}, "file_traj": {"response": "```\nsklearn/calibration.py\nsklearn/isotonic.py\nsklearn/_config.py\nsklearn/utils/_set_output.py\nsklearn/utils/validation.py\n```", "usage": {"completion_tokens": 37, "prompt_tokens": 6286}, "prompt": "Please look through the following GitHub problem description, and Repository structure and provide a list of files that one would need to edit to fix the problem.\n\n### GitHub Problem Description ###\nCalibratedClassifierCV doesn't work with `set_config(transform_output=\"pandas\")`\n### Describe the bug\r\n\r\nCalibratedClassifierCV with isotonic regression doesn't work when we previously set `set_config(transform_output=\"pandas\")`.\r\nThe IsotonicRegression seems to return a dataframe, which is a problem for `_CalibratedClassifier`  in `predict_proba` where it tries to put the dataframe in a numpy array row `proba[:, class_idx] = calibrator.predict(this_pred)`.\r\n\r\n### Steps/Code to Reproduce\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn import set_config\r\nfrom sklearn.calibration import CalibratedClassifierCV\r\nfrom sklearn.linear_model import SGDClassifier\r\n\r\nset_config(transform_output=\"pandas\")\r\nmodel = CalibratedClassifierCV(SGDClassifier(), method='isotonic')\r\nmodel.fit(np.arange(90).reshape(30, -1), np.arange(30) % 2)\r\nmodel.predict(np.arange(90).reshape(30, -1))\r\n```\r\n\r\n### Expected Results\r\n\r\nIt should not crash.\r\n\r\n### Actual Results\r\n\r\n```\r\n../core/model_trainer.py:306: in train_model\r\n    cv_predictions = cross_val_predict(pipeline,\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:968: in cross_val_predict\r\n    predictions = parallel(\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:1085: in __call__\r\n    if self.dispatch_one_batch(iterator):\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:901: in dispatch_one_batch\r\n    self._dispatch(tasks)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:819: in _dispatch\r\n    job = self._backend.apply_async(batch, callback=cb)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/_parallel_backends.py:208: in apply_async\r\n    result = ImmediateResult(func)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/_parallel_backends.py:597: in __init__\r\n    self.results = batch()\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:288: in __call__\r\n    return [func(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:288: in <listcomp>\r\n    return [func(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/utils/fixes.py:117: in __call__\r\n    return self.function(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:1052: in _fit_and_predict\r\n    predictions = func(X_test)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/pipeline.py:548: in predict_proba\r\n    return self.steps[-1][1].predict_proba(Xt, **predict_proba_params)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/calibration.py:477: in predict_proba\r\n    proba = calibrated_classifier.predict_proba(X)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/calibration.py:764: in predict_proba\r\n    proba[:, class_idx] = calibrator.predict(this_pred)\r\nE   ValueError: could not broadcast input array from shape (20,1) into shape (20,)\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.9.15 (main, Nov 24 2022, 14:31:59)  [GCC 11.2.0]\r\nexecutable: /home/philippe/.anaconda3/envs/strategy-training/bin/python\r\n   machine: Linux-5.15.0-57-generic-x86_64-with-glibc2.31\r\n\r\nPython dependencies:\r\n      sklearn: 1.2.0\r\n          pip: 22.2.2\r\n   setuptools: 62.3.2\r\n        numpy: 1.23.5\r\n        scipy: 1.9.3\r\n       Cython: None\r\n       pandas: 1.4.1\r\n   matplotlib: 3.6.3\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libgomp\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0\r\n        version: None\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/numpy.libs/libopenblas64_p-r0-742d56dc.3.20.so\r\n        version: 0.3.20\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/scipy.libs/libopenblasp-r0-41284840.3.18.so\r\n        version: 0.3.18\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n```\r\n\n\n\n###\n\n\n\n### Repository Structure ###\nscikit-learn/\n    conftest.py\n    setup.py\nsklearn/\n    base.py\n    discriminant_analysis.py\n    naive_bayes.py\n    conftest.py\n    kernel_approximation.py\n    kernel_ridge.py\n    exceptions.py\n    pipeline.py\n    random_projection.py\n    calibration.py\n    multioutput.py\n    _min_dependencies.py\n    _distributor_init.py\n    dummy.py\n    isotonic.py\n    __init__.py\n    multiclass.py\n    _config.py\n    feature_selection/\n        _from_model.py\n        _variance_threshold.py\n        _univariate_selection.py\n        _mutual_info.py\n        _base.py\n        _rfe.py\n        _sequential.py\n        __init__.py\n    datasets/\n        _california_housing.py\n        _svmlight_format_io.py\n        _samples_generator.py\n        _rcv1.py\n        _twenty_newsgroups.py\n        _olivetti_faces.py\n        _arff_parser.py\n        _covtype.py\n        _lfw.py\n        _openml.py\n        _base.py\n        __init__.py\n        _species_distributions.py\n        _kddcup99.py\n        descr/\n            __init__.py\n        images/\n            __init__.py\n        data/\n            __init__.py\n    mixture/\n        _bayesian_mixture.py\n        _base.py\n        _gaussian_mixture.py\n        __init__.py\n    neural_network/\n        _multilayer_perceptron.py\n        _rbm.py\n        _base.py\n        _stochastic_optimizers.py\n        __init__.py\n    preprocessing/\n        _encoders.py\n        _discretization.py\n        _label.py\n        _function_transformer.py\n        _data.py\n        _polynomial.py\n        __init__.py\n    tree/\n        _export.py\n        _classes.py\n        _reingold_tilford.py\n        __init__.py\n    _build_utils/\n        openmp_helpers.py\n        __init__.py\n        pre_build_helpers.py\n    __check_build/\n        __init__.py\n    decomposition/\n        _nmf.py\n        _kernel_pca.py\n        _sparse_pca.py\n        _factor_analysis.py\n        _dict_learning.py\n        _truncated_svd.py\n        _base.py\n        _incremental_pca.py\n        _pca.py\n        _lda.py\n        __init__.py\n        _fastica.py\n    model_selection/\n        _split.py\n        _plot.py\n        _search_successive_halving.py\n        _validation.py\n        _search.py\n        __init__.py\n    impute/\n        _iterative.py\n        _knn.py\n        _base.py\n        __init__.py\n    ensemble/\n        _bagging.py\n        _weight_boosting.py\n        _forest.py\n        _stacking.py\n        _voting.py\n        _gb.py\n        _iforest.py\n        _base.py\n        _gb_losses.py\n        __init__.py\n        _hist_gradient_boosting/\n            predictor.py\n            gradient_boosting.py\n            grower.py\n            binning.py\n            __init__.py\n    cluster/\n        _mean_shift.py\n        _dbscan.py\n        _bicluster.py\n        _bisect_k_means.py\n        _kmeans.py\n        _agglomerative.py\n        _feature_agglomeration.py\n        _optics.py\n        _spectral.py\n        _affinity_propagation.py\n        _birch.py\n        __init__.py\n    svm/\n        _classes.py\n        _base.py\n        _bounds.py\n        __init__.py\n    cross_decomposition/\n        _pls.py\n        __init__.py\n    semi_supervised/\n        _self_training.py\n        _label_propagation.py\n        __init__.py\n    neighbors/\n        _graph.py\n        _unsupervised.py\n        _distance_metric.py\n        _lof.py\n        _nca.py\n        _kde.py\n        _base.py\n        _nearest_centroid.py\n        _regression.py\n        _classification.py\n        __init__.py\n    compose/\n        _column_transformer.py\n        _target.py\n        __init__.py\n    externals/\n        _arff.py\n        conftest.py\n        _lobpcg.py\n        __init__.py\n        _packaging/\n            version.py\n            _structures.py\n            __init__.py\n    linear_model/\n        _linear_loss.py\n        _passive_aggressive.py\n        _theil_sen.py\n        _huber.py\n        _ransac.py\n        _sag.py\n        _least_angle.py\n        _ridge.py\n        _coordinate_descent.py\n        _omp.py\n        _perceptron.py\n        _base.py\n        _quantile.py\n        _stochastic_gradient.py\n        _bayes.py\n        __init__.py\n        _logistic.py\n        _glm/\n            glm.py\n            _newton_solver.py\n            __init__.py\n    feature_extraction/\n        image.py\n        _stop_words.py\n        _dict_vectorizer.py\n        _hash.py\n        __init__.py\n        text.py\n    metrics/\n        _ranking.py\n        _base.py\n        _scorer.py\n        pairwise.py\n        _regression.py\n        _classification.py\n        __init__.py\n        _pairwise_distances_reduction/\n            _dispatcher.py\n            __init__.py\n        _plot/\n            confusion_matrix.py\n            base.py\n            roc_curve.py\n            det_curve.py\n            precision_recall_curve.py\n            regression.py\n            __init__.py\n        cluster/\n            _unsupervised.py\n            _bicluster.py\n            _supervised.py\n            __init__.py\n    gaussian_process/\n        _gpr.py\n        _gpc.py\n        __init__.py\n        kernels.py\n    manifold/\n        _t_sne.py\n        _isomap.py\n        _mds.py\n        _locally_linear.py\n        _spectral_embedding.py\n        __init__.py\n    covariance/\n        _graph_lasso.py\n        _elliptic_envelope.py\n        _shrunk_covariance.py\n        _robust_covariance.py\n        _empirical_covariance.py\n        __init__.py\n    utils/\n        validation.py\n        _testing.py\n        estimator_checks.py\n        graph.py\n        _pprint.py\n        random.py\n        _set_output.py\n        _mask.py\n        _arpack.py\n        _param_validation.py\n        _show_versions.py\n        fixes.py\n        _joblib.py\n        _encode.py\n        _estimator_html_repr.py\n        discovery.py\n        deprecation.py\n        metaestimators.py\n        _available_if.py\n        _tags.py\n        extmath.py\n        _bunch.py\n        stats.py\n        sparsefuncs.py\n        _mocking.py\n        _array_api.py\n        __init__.py\n        optimize.py\n        multiclass.py\n        class_weight.py\n        parallel.py\n    inspection/\n        _permutation_importance.py\n        _pd_utils.py\n        _partial_dependence.py\n        __init__.py\n        _plot/\n            partial_dependence.py\n            decision_boundary.py\n            __init__.py\n    experimental/\n        enable_iterative_imputer.py\n        enable_hist_gradient_boosting.py\n        enable_halving_search_cv.py\n        __init__.py\n    _loss/\n        loss.py\n        glm_distribution.py\n        link.py\n        __init__.py\nmaint_tools/\n    sort_whats_new.py\n    check_pxd_in_installation.py\n    update_tracking_issue.py\nasv_benchmarks/\n    benchmarks/\n        metrics.py\n        manifold.py\n        common.py\n        svm.py\n        neighbors.py\n        model_selection.py\n        cluster.py\n        decomposition.py\n        utils.py\n        ensemble.py\n        linear_model.py\n        datasets.py\n        __init__.py\ndoc/\n    conf.py\n    conftest.py\n    tutorial/\n        machine_learning_map/\n            svg2imagemap.py\n            parse_path.py\n            pyparsing.py\n        text_analytics/\n            solutions/\n                exercise_01_language_train_model.py\n                exercise_02_sentiment.py\n                generate_skeletons.py\n            skeletons/\n                exercise_01_language_train_model.py\n                exercise_02_sentiment.py\n            data/\n                languages/\n                    fetch_data.py\n                movie_reviews/\n                    fetch_data.py\n    sphinxext/\n        allow_nan_estimators.py\n        github_link.py\n        add_toctree_functions.py\n        doi_role.py\n        sphinx_issues.py\nbuild_tools/\n    update_environments_and_lock_files.py\n    generate_authors_table.py\n    circle/\n        list_versions.py\n    github/\n        check_wheels.py\n        vendor.py\n    azure/\n        get_commit_message.py\n        get_selected_tests.py\nbenchmarks/\n    bench_lasso.py\n    bench_hist_gradient_boosting_categorical_only.py\n    bench_20newsgroups.py\n    bench_kernel_pca_solvers_time_vs_n_samples.py\n    bench_covertype.py\n    bench_tsne_mnist.py\n    bench_online_ocsvm.py\n    bench_plot_ward.py\n    bench_random_projections.py\n    bench_sgd_regression.py\n    bench_isotonic.py\n    bench_hist_gradient_boosting_higgsboson.py\n    bench_plot_parallel_pairwise.py\n    bench_plot_omp_lars.py\n    bench_rcv1_logreg_convergence.py\n    bench_plot_polynomial_kernel_approximation.py\n    bench_hist_gradient_boosting_adult.py\n    bench_glmnet.py\n    bench_mnist.py\n    bench_sparsify.py\n    bench_text_vectorizers.py\n    bench_plot_hierarchical.py\n    bench_feature_expansions.py\n    plot_tsne_mnist.py\n    bench_plot_incremental_pca.py\n    bench_plot_nmf.py\n    bench_multilabel_metrics.py\n    bench_plot_randomized_svd.py\n    bench_tree.py\n    bench_saga.py\n    bench_kernel_pca_solvers_time_vs_n_components.py\n    bench_plot_lasso_path.py\n    bench_sample_without_replacement.py\n    bench_isolation_forest.py\n    bench_lof.py\n    bench_plot_neighbors.py\n    bench_hist_gradient_boosting_threading.py\n    bench_plot_svd.py\n    bench_plot_fastkmeans.py\n    bench_hist_gradient_boosting.py\n    bench_glm.py\nexamples/\n    feature_selection/\n        plot_f_test_vs_mi.py\n        plot_select_from_model_diabetes.py\n        plot_rfe_with_cross_validation.py\n        plot_rfe_digits.py\n        plot_feature_selection_pipeline.py\n        plot_feature_selection.py\n    datasets/\n        plot_random_dataset.py\n        plot_random_multilabel_dataset.py\n        plot_iris_dataset.py\n        plot_digits_last_image.py\n    mixture/\n        plot_concentration_prior.py\n        plot_gmm_selection.py\n        plot_gmm_init.py\n        plot_gmm_sin.py\n        plot_gmm_pdf.py\n        plot_gmm_covariances.py\n        plot_gmm.py\n    neural_networks/\n        plot_rbm_logistic_classification.py\n        plot_mlp_training_curves.py\n        plot_mlp_alpha.py\n        plot_mnist_filters.py\n    multioutput/\n        plot_classifier_chain_yeast.py\n    preprocessing/\n        plot_all_scaling.py\n        plot_discretization_classification.py\n        plot_map_data_to_normal.py\n        plot_discretization_strategies.py\n        plot_scaling_importance.py\n        plot_discretization.py\n    tree/\n        plot_cost_complexity_pruning.py\n        plot_unveil_tree_structure.py\n        plot_iris_dtc.py\n        plot_tree_regression_multioutput.py\n        plot_tree_regression.py\n    decomposition/\n        plot_varimax_fa.py\n        plot_pca_3d.py\n        plot_ica_vs_pca.py\n        plot_image_denoising.py\n        plot_faces_decomposition.py\n        plot_incremental_pca.py\n        plot_ica_blind_source_separation.py\n        plot_sparse_coding.py\n        plot_pca_iris.py\n        plot_beta_divergence.py\n        plot_kernel_pca.py\n        plot_pca_vs_fa_model_selection.py\n        plot_pca_vs_lda.py\n    model_selection/\n        plot_successive_halving_heatmap.py\n        plot_learning_curve.py\n        plot_det.py\n        plot_confusion_matrix.py\n        plot_roc.py\n        plot_precision_recall.py\n        plot_randomized_search.py\n        plot_grid_search_refit_callable.py\n        plot_permutation_tests_for_classification.py\n        plot_grid_search_digits.py\n        plot_cv_indices.py\n        plot_cv_predict.py\n        plot_grid_search_text_feature_extraction.py\n        plot_underfitting_overfitting.py\n        plot_roc_crossval.py\n        plot_grid_search_stats.py\n        plot_multi_metric_evaluation.py\n        plot_validation_curve.py\n        plot_likelihood_ratios.py\n        plot_successive_halving_iterations.py\n        plot_train_error_vs_test_error.py\n        plot_nested_cross_validation_iris.py\n    impute/\n        plot_iterative_imputer_variants_comparison.py\n        plot_missing_values.py\n    ensemble/\n        plot_gradient_boosting_regularization.py\n        plot_isolation_forest.py\n        plot_feature_transformation.py\n        plot_adaboost_multiclass.py\n        plot_voting_probas.py\n        plot_adaboost_twoclass.py\n        plot_random_forest_embedding.py\n        plot_gradient_boosting_quantile.py\n        plot_forest_importances.py\n        plot_forest_iris.py\n        plot_gradient_boosting_oob.py\n        plot_gradient_boosting_early_stopping.py\n        plot_adaboost_regression.py\n        plot_monotonic_constraints.py\n        plot_gradient_boosting_regression.py\n        plot_ensemble_oob.py\n        plot_random_forest_regression_multioutput.py\n        plot_voting_regressor.py\n        plot_adaboost_hastie_10_2.py\n        plot_forest_importances_faces.py\n        plot_stack_predictors.py\n        plot_gradient_boosting_categorical.py\n        plot_bias_variance.py\n        plot_voting_decision_regions.py\n    cluster/\n        plot_dbscan.py\n        plot_coin_segmentation.py\n        plot_linkage_comparison.py\n        plot_agglomerative_dendrogram.py\n        plot_birch_vs_minibatchkmeans.py\n        plot_digits_linkage.py\n        plot_segmentation_toy.py\n        plot_bisect_kmeans.py\n        plot_cluster_iris.py\n        plot_kmeans_silhouette_analysis.py\n        plot_optics.py\n        plot_affinity_propagation.py\n        plot_kmeans_stability_low_dim_dense.py\n        plot_ward_structured_vs_unstructured.py\n        plot_feature_agglomeration_vs_univariate_selection.py\n        plot_agglomerative_clustering_metrics.py\n        plot_mean_shift.py\n        plot_mini_batch_kmeans.py\n        plot_color_quantization.py\n        plot_dict_face_patches.py\n        plot_kmeans_assumptions.py\n        plot_digits_agglomeration.py\n        plot_inductive_clustering.py\n        plot_kmeans_digits.py\n        plot_kmeans_plusplus.py\n        plot_coin_ward_segmentation.py\n        plot_face_compress.py\n        plot_cluster_comparison.py\n        plot_agglomerative_clustering.py\n        plot_adjusted_for_chance_measures.py\n    svm/\n        plot_separating_hyperplane.py\n        plot_separating_hyperplane_unbalanced.py\n        plot_svm_kernels.py\n        plot_linearsvc_support_vectors.py\n        plot_svm_tie_breaking.py\n        plot_svm_anova.py\n        plot_svm_margin.py\n        plot_svm_scale_c.py\n        plot_rbf_parameters.py\n        plot_oneclass.py\n        plot_weighted_samples.py\n        plot_custom_kernel.py\n        plot_iris_svc.py\n        plot_svm_nonlinear.py\n        plot_svm_regression.py\n    cross_decomposition/\n        plot_compare_cross_decomposition.py\n        plot_pcr_vs_pls.py\n    bicluster/\n        plot_bicluster_newsgroups.py\n        plot_spectral_biclustering.py\n        plot_spectral_coclustering.py\n    miscellaneous/\n        plot_pipeline_display.py\n        plot_isotonic_regression.py\n        plot_kernel_approximation.py\n        plot_outlier_detection_bench.py\n        plot_partial_dependence_visualization_api.py\n        plot_display_object_visualization.py\n        plot_set_output.py\n        plot_johnson_lindenstrauss_bound.py\n        plot_anomaly_comparison.py\n        plot_kernel_ridge_regression.py\n        plot_estimator_representation.py\n        plot_multioutput_face_completion.py\n        plot_roc_curve_visualization_api.py\n        plot_multilabel.py\n    semi_supervised/\n        plot_label_propagation_digits.py\n        plot_self_training_varying_threshold.py\n        plot_semi_supervised_newsgroups.py\n        plot_label_propagation_structure.py\n        plot_label_propagation_digits_active_learning.py\n        plot_semi_supervised_versus_svm_iris.py\n    classification/\n        plot_lda_qda.py\n        plot_digits_classification.py\n        plot_classification_probability.py\n        plot_lda.py\n        plot_classifier_comparison.py\n    neighbors/\n        plot_regression.py\n        plot_nca_illustration.py\n        approximate_nearest_neighbors.py\n        plot_lof_outlier_detection.py\n        plot_nearest_centroid.py\n        plot_kde_1d.py\n        plot_species_kde.py\n        plot_digits_kde_sampling.py\n        plot_classification.py\n        plot_caching_nearest_neighbors.py\n        plot_lof_novelty_detection.py\n        plot_nca_dim_reduction.py\n        plot_nca_classification.py\n    kernel_approximation/\n        plot_scalable_poly_kernels.py\n    compose/\n        plot_digits_pipe.py\n        plot_column_transformer_mixed_types.py\n        plot_feature_union.py\n        plot_compare_reduction.py\n        plot_transformed_target.py\n        plot_column_transformer.py\n    linear_model/\n        plot_robust_fit.py\n        plot_theilsen.py\n        plot_lasso_model_selection.py\n        plot_lasso_lars.py\n        plot_lasso_dense_vs_sparse_data.py\n        plot_nnls.py\n        plot_ransac.py\n        plot_sgd_early_stopping.py\n        plot_lasso_and_elasticnet.py\n        plot_elastic_net_precomputed_gram_matrix_with_weighted_samples.py\n        plot_ridge_path.py\n        plot_sgd_loss_functions.py\n        plot_bayesian_ridge_curvefit.py\n        plot_sgd_weighted_samples.py\n        plot_huber_vs_ridge.py\n        plot_sgd_iris.py\n        plot_sgd_penalties.py\n        plot_logistic_path.py\n        plot_poisson_regression_non_normal_loss.py\n        plot_ard.py\n        plot_ols_3d.py\n        plot_tweedie_regression_insurance_claims.py\n        plot_sgd_comparison.py\n        plot_logistic.py\n        plot_logistic_multinomial.py\n        plot_omp.py\n        plot_sparse_logistic_regression_20newsgroups.py\n        plot_sgd_separating_hyperplane.py\n        plot_ols.py\n        plot_polynomial_interpolation.py\n        plot_ols_ridge_variance.py\n        plot_sparse_logistic_regression_mnist.py\n        plot_logistic_l1_l2_sparsity.py\n        plot_multi_task_lasso_support.py\n        plot_lasso_coordinate_descent_path.py\n        plot_sgdocsvm_vs_ocsvm.py\n        plot_quantile_regression.py\n        plot_lasso_lars_ic.py\n        plot_ridge_coeffs.py\n        plot_iris_logistic.py\n    calibration/\n        plot_calibration_curve.py\n        plot_calibration.py\n        plot_compare_calibration.py\n        plot_calibration_multiclass.py\n    gaussian_process/\n        plot_gpc_isoprobability.py\n        plot_gpc_iris.py\n        plot_gpr_on_structured_data.py\n        plot_compare_gpr_krr.py\n        plot_gpc_xor.py\n        plot_gpc.py\n        plot_gpr_noisy_targets.py\n        plot_gpr_prior_posterior.py\n        plot_gpr_noisy.py\n        plot_gpr_co2.py\n    manifold/\n        plot_compare_methods.py\n        plot_manifold_sphere.py\n        plot_lle_digits.py\n        plot_swissroll.py\n        plot_t_sne_perplexity.py\n        plot_mds.py\n    release_highlights/\n        plot_release_highlights_1_1_0.py\n        plot_release_highlights_0_22_0.py\n        plot_release_highlights_1_0_0.py\n        plot_release_highlights_1_2_0.py\n        plot_release_highlights_0_23_0.py\n        plot_release_highlights_0_24_0.py\n    covariance/\n        plot_mahalanobis_distances.py\n        plot_robust_vs_empirical_covariance.py\n        plot_covariance_estimation.py\n        plot_lw_vs_oas.py\n        plot_sparse_cov.py\n    text/\n        plot_document_classification_20newsgroups.py\n        plot_document_clustering.py\n        plot_hashing_vs_dict_vectorizer.py\n    applications/\n        plot_species_distribution_modeling.py\n        plot_out_of_core_classification.py\n        plot_digits_denoising.py\n        plot_model_complexity_influence.py\n        plot_prediction_latency.py\n        plot_stock_market.py\n        plot_tomography_l1_reconstruction.py\n        svm_gui.py\n        plot_cyclical_feature_engineering.py\n        wikipedia_principal_eigenvector.py\n        plot_face_recognition.py\n        plot_outlier_detection_wine.py\n        plot_topics_extraction_with_nmf_lda.py\n    inspection/\n        plot_linear_model_coefficient_interpretation.py\n        plot_causal_interpretation.py\n        plot_partial_dependence.py\n        plot_permutation_importance_multicollinear.py\n        plot_permutation_importance.py\n    exercises/\n        plot_cv_diabetes.py\n        plot_iris_exercise.py\n        plot_cv_digits.py\n        plot_digits_classification_exercise.py\n\n###\n\nPlease only provide the full path and return at most 5 files.\nThe returned files should be separated by new lines ordered by most to least important and wrapped with ```\nFor example:\n```\nfile1.py\nfile2.py\n```"}, "found_related_locs": [["class: CalibratedClassifierCV\nfunction: _fit_classifier_calibrator_pair\nfunction: _get_prediction_method\nfunction: _compute_predictions\nfunction: _fit_calibrator\nclass: _CalibratedClassifier"], ["class: IsotonicRegression"], ["variable: _global_config\nfunction: set_config"]], "additional_artifact_loc_related": [{"raw_output_loc": "```\nsklearn/calibration.py\nclass: CalibratedClassifierCV\nfunction: _fit_classifier_calibrator_pair\nfunction: _get_prediction_method\nfunction: _compute_predictions\nfunction: _fit_calibrator\nclass: _CalibratedClassifier\n\nsklearn/isotonic.py\nclass: IsotonicRegression\n\nsklearn/_config.py\nvariable: _global_config\nfunction: set_config\n```"}], "related_loc_traj": {"response": "```\nsklearn/calibration.py\nclass: CalibratedClassifierCV\nfunction: _fit_classifier_calibrator_pair\nfunction: _get_prediction_method\nfunction: _compute_predictions\nfunction: _fit_calibrator\nclass: _CalibratedClassifier\n\nsklearn/isotonic.py\nclass: IsotonicRegression\n\nsklearn/_config.py\nvariable: _global_config\nfunction: set_config\n```", "usage": {"completion_tokens": 85, "prompt_tokens": 2920}, "prompt": "\nPlease look through the following GitHub Problem Description, and the Skeleton of Relevant Files.\nIdentify all locations that need inspection or editing to fix the problem, including directly related areas as well as any potentially related global variables, functions, and classes.\nFor each location you provide, either give the name of the class, the name of a method in a class, the name of a function, or the name of a global variable.\n\n### GitHub Problem Description ###\nCalibratedClassifierCV doesn't work with `set_config(transform_output=\"pandas\")`\n### Describe the bug\r\n\r\nCalibratedClassifierCV with isotonic regression doesn't work when we previously set `set_config(transform_output=\"pandas\")`.\r\nThe IsotonicRegression seems to return a dataframe, which is a problem for `_CalibratedClassifier`  in `predict_proba` where it tries to put the dataframe in a numpy array row `proba[:, class_idx] = calibrator.predict(this_pred)`.\r\n\r\n### Steps/Code to Reproduce\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn import set_config\r\nfrom sklearn.calibration import CalibratedClassifierCV\r\nfrom sklearn.linear_model import SGDClassifier\r\n\r\nset_config(transform_output=\"pandas\")\r\nmodel = CalibratedClassifierCV(SGDClassifier(), method='isotonic')\r\nmodel.fit(np.arange(90).reshape(30, -1), np.arange(30) % 2)\r\nmodel.predict(np.arange(90).reshape(30, -1))\r\n```\r\n\r\n### Expected Results\r\n\r\nIt should not crash.\r\n\r\n### Actual Results\r\n\r\n```\r\n../core/model_trainer.py:306: in train_model\r\n    cv_predictions = cross_val_predict(pipeline,\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:968: in cross_val_predict\r\n    predictions = parallel(\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:1085: in __call__\r\n    if self.dispatch_one_batch(iterator):\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:901: in dispatch_one_batch\r\n    self._dispatch(tasks)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:819: in _dispatch\r\n    job = self._backend.apply_async(batch, callback=cb)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/_parallel_backends.py:208: in apply_async\r\n    result = ImmediateResult(func)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/_parallel_backends.py:597: in __init__\r\n    self.results = batch()\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:288: in __call__\r\n    return [func(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:288: in <listcomp>\r\n    return [func(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/utils/fixes.py:117: in __call__\r\n    return self.function(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:1052: in _fit_and_predict\r\n    predictions = func(X_test)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/pipeline.py:548: in predict_proba\r\n    return self.steps[-1][1].predict_proba(Xt, **predict_proba_params)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/calibration.py:477: in predict_proba\r\n    proba = calibrated_classifier.predict_proba(X)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/calibration.py:764: in predict_proba\r\n    proba[:, class_idx] = calibrator.predict(this_pred)\r\nE   ValueError: could not broadcast input array from shape (20,1) into shape (20,)\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.9.15 (main, Nov 24 2022, 14:31:59)  [GCC 11.2.0]\r\nexecutable: /home/philippe/.anaconda3/envs/strategy-training/bin/python\r\n   machine: Linux-5.15.0-57-generic-x86_64-with-glibc2.31\r\n\r\nPython dependencies:\r\n      sklearn: 1.2.0\r\n          pip: 22.2.2\r\n   setuptools: 62.3.2\r\n        numpy: 1.23.5\r\n        scipy: 1.9.3\r\n       Cython: None\r\n       pandas: 1.4.1\r\n   matplotlib: 3.6.3\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libgomp\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0\r\n        version: None\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/numpy.libs/libopenblas64_p-r0-742d56dc.3.20.so\r\n        version: 0.3.20\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/scipy.libs/libopenblasp-r0-41284840.3.18.so\r\n        version: 0.3.18\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n```\r\n\n\n\n###\n\n\n\n### Skeleton of Relevant Files ###\n\n### File: sklearn/calibration.py ###\n```python\n\n\nclass CalibratedClassifierCV(ClassifierMixin, MetaEstimatorMixin, BaseEstimator):\n\n    _parameter_constraints: dict = {\n        \"estimator\": [\n            HasMethods([\"fit\", \"predict_proba\"]),\n            HasMethods([\"fit\", \"decision_function\"]),\n            None,\n        ],\n        \"method\": [StrOptions({\"isotonic\", \"sigmoid\"})],\n        \"cv\": [\"cv_object\", StrOptions({\"prefit\"})],\n        \"n_jobs\": [Integral, None],\n        \"ensemble\": [\"boolean\"],\n        \"base_estimator\": [\n            HasMethods([\"fit\", \"predict_proba\"]),\n            HasMethods([\"fit\", \"decision_function\"]),\n            None,\n            Hidden(StrOptions({\"deprecated\"})),\n        ],\n    }\n\n    def __init__(\n        self,\n        estimator=None,\n        *,\n        method=\"sigmoid\",\n        cv=None,\n        n_jobs=None,\n        ensemble=True,\n        base_estimator=\"deprecated\",\n    ):\n...\n    def fit(self, X, y, sample_weight=None, **fit_params):\n...\n    def predict_proba(self, X):\n...\n    def predict(self, X):\n...\n    def _more_tags(self):\n...\n\ndef _fit_classifier_calibrator_pair(\n    estimator,\n    X,\n    y,\n    train,\n    test,\n    supports_sw,\n    method,\n    classes,\n    sample_weight=None,\n    **fit_params,\n):\n...\n\ndef _get_prediction_method(clf):\n...\n\ndef _compute_predictions(pred_method, method_name, X, n_classes):\n...\n\ndef _fit_calibrator(clf, predictions, y, classes, method, sample_weight=None):\n...\n\nclass _CalibratedClassifier:\n\n    def __init__(self, estimator, calibrators, *, classes, method=\"sigmoid\"):\n...\n    def predict_proba(self, X):\n...\n\ndef _sigmoid_calibration(predictions, y, sample_weight=None):\n...\n\nclass _SigmoidCalibration(RegressorMixin, BaseEstimator):\n\n    def fit(self, X, y, sample_weight=None):\n...\n    def predict(self, T):\n...\n\ndef calibration_curve(\n    y_true,\n    y_prob,\n    *,\n    pos_label=None,\n    normalize=\"deprecated\",\n    n_bins=5,\n    strategy=\"uniform\",\n):\n...\n\nclass CalibrationDisplay:\n\n    def __init__(\n        self, prob_true, prob_pred, y_prob, *, estimator_name=None, pos_label=None\n    ):\n...\n    def plot(self, *, ax=None, name=None, ref_line=True, **kwargs):\n...\n    @classmethod\n    def from_estimator(\n        cls,\n        estimator,\n        X,\n        y,\n        *,\n        n_bins=5,\n        strategy=\"uniform\",\n        pos_label=None,\n        name=None,\n        ref_line=True,\n        ax=None,\n        **kwargs,\n    ):\n...\n    @classmethod\n    def from_predictions(\n        cls,\n        y_true,\n        y_prob,\n        *,\n        n_bins=5,\n        strategy=\"uniform\",\n        pos_label=None,\n        name=None,\n        ref_line=True,\n        ax=None,\n        **kwargs,\n    ):\n\n```\n\n### File: sklearn/isotonic.py ###\n```python\n# Authors: Fabian Pedregosa <fabian@fseoane.net>\n#          Alexandre Gramfort <alexandre.gramfort@inria.fr>\n#          Nelle Varoquaux <nelle.varoquaux@gmail.com>\n# License: BSD 3 clause\n\n\n\n__all__ = [\"check_increasing\", \"isotonic_regression\", \"IsotonicRegression\"]\n\n\ndef check_increasing(x, y):\n...\n\ndef isotonic_regression(\n    y, *, sample_weight=None, y_min=None, y_max=None, increasing=True\n):\n...\n\nclass IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n\n    _parameter_constraints: dict = {\n        \"y_min\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"y_max\": [Interval(Real, None, None, closed=\"both\"), None],\n        \"increasing\": [\"boolean\", StrOptions({\"auto\"})],\n        \"out_of_bounds\": [StrOptions({\"nan\", \"clip\", \"raise\"})],\n    }\n\n    def __init__(self, *, y_min=None, y_max=None, increasing=True, out_of_bounds=\"nan\"):\n...\n    def _check_input_data_shape(self, X):\n...\n    def _build_f(self, X, y):\n...\n    def _build_y(self, X, y, sample_weight, trim_duplicates=True):\n...\n    def fit(self, X, y, sample_weight=None):\n...\n    def transform(self, T):\n...\n    def predict(self, T):\n...\n    # We implement get_feature_names_out here instead of using\n    # `ClassNamePrefixFeaturesOutMixin`` because `input_features` are ignored.\n    # `input_features` are ignored because `IsotonicRegression` accepts 1d\n    # arrays and the semantics of `feature_names_in_` are not clear for 1d arrays.\n    def get_feature_names_out(self, input_features=None):\n...\n    def __getstate__(self):\n...\n    def __setstate__(self, state):\n...\n    def _more_tags(self):\n\n```\n\n### File: sklearn/_config.py ###\n```python\n\n_global_config = {\n    \"assume_finite\": bool(os.environ.get(\"SKLEARN_ASSUME_FINITE\", False)),\n    \"working_memory\": int(os.environ.get(\"SKLEARN_WORKING_MEMORY\", 1024)),\n    \"print_changed_only\": True,\n    \"display\": \"diagram\",\n    \"pairwise_dist_chunk_size\": int(\n        os.environ.get(\"SKLEARN_PAIRWISE_DIST_CHUNK_SIZE\", 256)\n    ),\n    \"enable_cython_pairwise_dist\": True,\n    \"array_api_dispatch\": False,\n    \"transform_output\": \"default\",\n}\n_threadlocal = threading.local()\n\n\ndef _get_threadlocal_config():\n...\n\ndef get_config():\n...\n\ndef set_config(\n    assume_finite=None,\n    working_memory=None,\n    print_changed_only=None,\n    display=None,\n    pairwise_dist_chunk_size=None,\n    enable_cython_pairwise_dist=None,\n    array_api_dispatch=None,\n    transform_output=None,\n):\n...\n\n@contextmanager\ndef config_context(\n    *,\n    assume_finite=None,\n    working_memory=None,\n    print_changed_only=None,\n    display=None,\n    pairwise_dist_chunk_size=None,\n    enable_cython_pairwise_dist=None,\n    array_api_dispatch=None,\n    transform_output=None,\n):\n\n```\n\n\n###\n\nPlease provide the complete set of locations as either a class name, a function name, or a variable name.\nNote that if you include a class, you do not need to list its specific methods.\nYou can include either the entire class or don't include the class name and instead include specific methods in the class.\n### Examples:\n```\nfull_path1/file1.py\nfunction: my_function_1\nclass: MyClass1\nfunction: MyClass2.my_method\n\nfull_path2/file2.py\nvariable: my_var\nfunction: MyClass3.my_method\n\nfull_path3/file3.py\nfunction: my_function_2\nfunction: my_function_3\nfunction: MyClass4.my_method_1\nclass: MyClass5\n```\n\nReturn just the locations.\n"}, "found_edit_locs": ["class: _CalibratedClassifier\nfunction: _CalibratedClassifier.predict_proba\nline: 763", "class: IsotonicRegression\nfunction: IsotonicRegression.predict\nline: 413", ""], "additional_artifact_loc_edit_location": [{"raw_output_loc": ["```\nsklearn/calibration.py\nfunction: _fit_calibrator\nline: 655\n\nsklearn/calibration.py\nclass: _CalibratedClassifier\nfunction: predict_proba\nline: 763\n```", "```\nsklearn/calibration.py\nclass: _CalibratedClassifier\nfunction: _CalibratedClassifier.predict_proba\nline: 763\n```", "```\nsklearn/calibration.py\nclass: _CalibratedClassifier\nfunction: _CalibratedClassifier.predict_proba\nline: 763\n\nsklearn/isotonic.py\nclass: IsotonicRegression\nfunction: IsotonicRegression.predict\nline: 413\n```", "```\nsklearn/calibration.py\nfunction: _CalibratedClassifier.predict_proba\nline: 763\n\nsklearn/calibration.py\nfunction: _compute_predictions\nline: 641\n```"]}], "edit_loc_traj": {"prompt": "\nPlease review the following GitHub problem description, and relevant files, and provide a set of locations that need to be edited to fix the issue.\nThe locations can be specified as class names, function or method names, or exact line numbers that require modification.\n\n### GitHub Problem Description ###\nCalibratedClassifierCV doesn't work with `set_config(transform_output=\"pandas\")`\n### Describe the bug\r\n\r\nCalibratedClassifierCV with isotonic regression doesn't work when we previously set `set_config(transform_output=\"pandas\")`.\r\nThe IsotonicRegression seems to return a dataframe, which is a problem for `_CalibratedClassifier`  in `predict_proba` where it tries to put the dataframe in a numpy array row `proba[:, class_idx] = calibrator.predict(this_pred)`.\r\n\r\n### Steps/Code to Reproduce\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn import set_config\r\nfrom sklearn.calibration import CalibratedClassifierCV\r\nfrom sklearn.linear_model import SGDClassifier\r\n\r\nset_config(transform_output=\"pandas\")\r\nmodel = CalibratedClassifierCV(SGDClassifier(), method='isotonic')\r\nmodel.fit(np.arange(90).reshape(30, -1), np.arange(30) % 2)\r\nmodel.predict(np.arange(90).reshape(30, -1))\r\n```\r\n\r\n### Expected Results\r\n\r\nIt should not crash.\r\n\r\n### Actual Results\r\n\r\n```\r\n../core/model_trainer.py:306: in train_model\r\n    cv_predictions = cross_val_predict(pipeline,\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:968: in cross_val_predict\r\n    predictions = parallel(\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:1085: in __call__\r\n    if self.dispatch_one_batch(iterator):\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:901: in dispatch_one_batch\r\n    self._dispatch(tasks)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:819: in _dispatch\r\n    job = self._backend.apply_async(batch, callback=cb)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/_parallel_backends.py:208: in apply_async\r\n    result = ImmediateResult(func)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/_parallel_backends.py:597: in __init__\r\n    self.results = batch()\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:288: in __call__\r\n    return [func(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/joblib/parallel.py:288: in <listcomp>\r\n    return [func(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/utils/fixes.py:117: in __call__\r\n    return self.function(*args, **kwargs)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:1052: in _fit_and_predict\r\n    predictions = func(X_test)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/pipeline.py:548: in predict_proba\r\n    return self.steps[-1][1].predict_proba(Xt, **predict_proba_params)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/calibration.py:477: in predict_proba\r\n    proba = calibrated_classifier.predict_proba(X)\r\n../../../../.anaconda3/envs/strategy-training/lib/python3.9/site-packages/sklearn/calibration.py:764: in predict_proba\r\n    proba[:, class_idx] = calibrator.predict(this_pred)\r\nE   ValueError: could not broadcast input array from shape (20,1) into shape (20,)\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.9.15 (main, Nov 24 2022, 14:31:59)  [GCC 11.2.0]\r\nexecutable: /home/philippe/.anaconda3/envs/strategy-training/bin/python\r\n   machine: Linux-5.15.0-57-generic-x86_64-with-glibc2.31\r\n\r\nPython dependencies:\r\n      sklearn: 1.2.0\r\n          pip: 22.2.2\r\n   setuptools: 62.3.2\r\n        numpy: 1.23.5\r\n        scipy: 1.9.3\r\n       Cython: None\r\n       pandas: 1.4.1\r\n   matplotlib: 3.6.3\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libgomp\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0\r\n        version: None\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/numpy.libs/libopenblas64_p-r0-742d56dc.3.20.so\r\n        version: 0.3.20\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /home/philippe/.anaconda3/envs/strategy-training/lib/python3.9/site-packages/scipy.libs/libopenblasp-r0-41284840.3.18.so\r\n        version: 0.3.18\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n```\r\n\n\n\n###\n\n\n\n### sklearn/calibration.py\n...\n45|    check_is_fitted,\n46|)\n47|from .utils import _safe_indexing\n48|from .isotonic import IsotonicRegression\n49|from .svm import LinearSVC\n50|from .model_selection import check_cv, cross_val_predict\n51|from .metrics._base import _check_pos_label_consistency\n52|from .metrics._plot.base import _get_response\n53|\n54|\n55|class CalibratedClassifierCV(ClassifierMixin, MetaEstimatorMixin, BaseEstimator):\n56|    \"\"\"Probability calibration with isotonic regression or logistic regression.\n57|\n58|    This class uses cross-validation to both estimate the parameters of a\n59|    classifier and subsequently calibrate a classifier. With default\n60|    `ensemble=True`, for each cv split it\n61|    fits a copy of the base estimator to the training subset, and calibrates it\n62|    using the testing subset. For prediction, predicted probabilities are\n63|    averaged across these individual calibrated classifiers. When\n64|    `ensemble=False`, cross-validation is used to obtain unbiased predictions,\n65|    via :func:`~sklearn.model_selection.cross_val_predict`, which are then\n66|    used for calibration. For prediction, the base estimator, trained using all\n67|    the data, is used. This is the method implemented when `probabilities=True`\n68|    for :mod:`sklearn.svm` estimators.\n69|\n70|    Already fitted classifiers can be calibrated via the parameter\n71|    `cv=\"prefit\"`. In this case, no cross-validation is used and all provided\n72|    data is used for calibration. The user has to take care manually that data\n73|    for model fitting and calibration are disjoint.\n74|\n75|    The calibration is based on the :term:`decision_function` method of the\n76|    `estimator` if it exists, else on :term:`predict_proba`.\n77|\n78|    Read more in the :ref:`User Guide <calibration>`.\n79|\n80|    Parameters\n81|    ----------\n82|    estimator : estimator instance, default=None\n83|        The classifier whose output need to be calibrated to provide more\n84|        accurate `predict_proba` outputs. The default classifier is\n85|        a :class:`~sklearn.svm.LinearSVC`.\n86|\n87|        .. versionadded:: 1.2\n88|\n89|    method : {'sigmoid', 'isotonic'}, default='sigmoid'\n90|        The method to use for calibration. Can be 'sigmoid' which\n91|        corresponds to Platt's method (i.e. a logistic regression model) or\n92|        'isotonic' which is a non-parametric approach. It is not advised to\n93|        use isotonic calibration with too few calibration samples\n94|        ``(<<1000)`` since it tends to overfit.\n95|\n96|    cv : int, cross-validation generator, iterable or \"prefit\", \\\n97|            default=None\n98|        Determines the cross-validation splitting strategy.\n99|        Possible inputs for cv are:\n100|\n101|        - None, to use the default 5-fold cross-validation,\n102|        - integer, to specify the number of folds.\n103|        - :term:`CV splitter`,\n104|        - An iterable yielding (train, test) splits as arrays of indices.\n105|\n106|        For integer/None inputs, if ``y`` is binary or multiclass,\n107|        :class:`~sklearn.model_selection.StratifiedKFold` is used. If ``y`` is\n108|        neither binary nor multiclass, :class:`~sklearn.model_selection.KFold`\n109|        is used.\n110|\n111|        Refer to the :ref:`User Guide <cross_validation>` for the various\n112|        cross-validation strategies that can be used here.\n113|\n114|        If \"prefit\" is passed, it is assumed that `estimator` has been\n115|        fitted already and all data is used for calibration.\n116|\n117|        .. versionchanged:: 0.22\n118|            ``cv`` default value if None changed from 3-fold to 5-fold.\n119|\n120|    n_jobs : int, default=None\n121|        Number of jobs to run in parallel.\n122|        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n123|        ``-1`` means using all processors.\n124|\n125|        Base estimator clones are fitted in parallel across cross-validation\n126|        iterations. Therefore parallelism happens only when `cv != \"prefit\"`.\n127|\n128|        See :term:`Glossary <n_jobs>` for more details.\n129|\n130|        .. versionadded:: 0.24\n131|\n132|    ensemble : bool, default=True\n133|        Determines how the calibrator is fitted when `cv` is not `'prefit'`.\n134|        Ignored if `cv='prefit'`.\n135|\n136|        If `True`, the `estimator` is fitted using training data, and\n137|        calibrated using testing data, for each `cv` fold. The final estimator\n138|        is an ensemble of `n_cv` fitted classifier and calibrator pairs, where\n139|        `n_cv` is the number of cross-validation folds. The output is the\n140|        average predicted probabilities of all pairs.\n141|\n142|        If `False`, `cv` is used to compute unbiased predictions, via\n143|        :func:`~sklearn.model_selection.cross_val_predict`, which are then\n144|        used for calibration. At prediction time, the classifier used is the\n145|        `estimator` trained on all the data.\n146|        Note that this method is also internally implemented  in\n147|        :mod:`sklearn.svm` estimators with the `probabilities=True` parameter.\n148|\n149|        .. versionadded:: 0.24\n150|\n151|    base_estimator : estimator instance\n152|        This parameter is deprecated. Use `estimator` instead.\n153|\n154|        .. deprecated:: 1.2\n155|           The parameter `base_estimator` is deprecated in 1.2 and will be\n156|           removed in 1.4. Use `estimator` instead.\n157|\n158|    Attributes\n159|    ----------\n160|    classes_ : ndarray of shape (n_classes,)\n161|        The class labels.\n162|\n163|    n_features_in_ : int\n164|        Number of features seen during :term:`fit`. Only defined if the\n165|        underlying estimator exposes such an attribute when fit.\n166|\n167|        .. versionadded:: 0.24\n168|\n169|    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n170|        Names of features seen during :term:`fit`. Only defined if the\n171|        underlying estimator exposes such an attribute when fit.\n172|\n173|        .. versionadded:: 1.0\n174|\n175|    calibrated_classifiers_ : list (len() equal to cv or 1 if `cv=\"prefit\"` \\\n176|            or `ensemble=False`)\n177|        The list of classifier and calibrator pairs.\n178|\n179|        - When `cv=\"prefit\"`, the fitted `estimator` and fitted\n180|          calibrator.\n181|        - When `cv` is not \"prefit\" and `ensemble=True`, `n_cv` fitted\n182|          `estimator` and calibrator pairs. `n_cv` is the number of\n183|          cross-validation folds.\n184|        - When `cv` is not \"prefit\" and `ensemble=False`, the `estimator`,\n185|          fitted on all the data, and fitted calibrator.\n186|\n187|        .. versionchanged:: 0.24\n188|            Single calibrated classifier case when `ensemble=False`.\n189|\n190|    See Also\n191|    --------\n192|    calibration_curve : Compute true and predicted probabilities\n193|        for a calibration curve.\n194|\n195|    References\n196|    ----------\n197|    .. [1] Obtaining calibrated probability estimates from decision trees\n198|           and naive Bayesian classifiers, B. Zadrozny & C. Elkan, ICML 2001\n199|\n200|    .. [2] Transforming Classifier Scores into Accurate Multiclass\n201|           Probability Estimates, B. Zadrozny & C. Elkan, (KDD 2002)\n202|\n203|    .. [3] Probabilistic Outputs for Support Vector Machines and Comparisons to\n204|           Regularized Likelihood Methods, J. Platt, (1999)\n205|\n206|    .. [4] Predicting Good Probabilities with Supervised Learning,\n207|           A. Niculescu-Mizil & R. Caruana, ICML 2005\n208|\n209|    Examples\n210|    --------\n211|    >>> from sklearn.datasets import make_classification\n212|    >>> from sklearn.naive_bayes import GaussianNB\n213|    >>> from sklearn.calibration import CalibratedClassifierCV\n214|    >>> X, y = make_classification(n_samples=100, n_features=2,\n215|    ...                            n_redundant=0, random_state=42)\n216|    >>> base_clf = GaussianNB()\n217|    >>> calibrated_clf = CalibratedClassifierCV(base_clf, cv=3)\n218|    >>> calibrated_clf.fit(X, y)\n219|    CalibratedClassifierCV(...)\n220|    >>> len(calibrated_clf.calibrated_classifiers_)\n221|    3\n222|    >>> calibrated_clf.predict_proba(X)[:5, :]\n223|    array([[0.110..., 0.889...],\n224|           [0.072..., 0.927...],\n225|           [0.928..., 0.071...],\n226|           [0.928..., 0.071...],\n227|           [0.071..., 0.928...]])\n228|    >>> from sklearn.model_selection import train_test_split\n229|    >>> X, y = make_classification(n_samples=100, n_features=2,\n230|    ...                            n_redundant=0, random_state=42)\n231|    >>> X_train, X_calib, y_train, y_calib = train_test_split(\n232|    ...        X, y, random_state=42\n233|    ... )\n234|    >>> base_clf = GaussianNB()\n235|    >>> base_clf.fit(X_train, y_train)\n236|    GaussianNB()\n237|    >>> calibrated_clf = CalibratedClassifierCV(base_clf, cv=\"prefit\")\n238|    >>> calibrated_clf.fit(X_calib, y_calib)\n239|    CalibratedClassifierCV(...)\n240|    >>> len(calibrated_clf.calibrated_classifiers_)\n241|    1\n242|    >>> calibrated_clf.predict_proba([[-0.5, 0.5]])\n243|    array([[0.936..., 0.063...]])\n244|    \"\"\"\n245|\n246|    _parameter_constraints: dict = {\n247|        \"estimator\": [\n248|            HasMethods([\"fit\", \"predict_proba\"]),\n249|            HasMethods([\"fit\", \"decision_function\"]),\n250|            None,\n251|        ],\n252|        \"method\": [StrOptions({\"isotonic\", \"sigmoid\"})],\n253|        \"cv\": [\"cv_object\", StrOptions({\"prefit\"})],\n254|        \"n_jobs\": [Integral, None],\n255|        \"ensemble\": [\"boolean\"],\n256|        \"base_estimator\": [\n257|            HasMethods([\"fit\", \"predict_proba\"]),\n258|            HasMethods([\"fit\", \"decision_function\"]),\n259|            None,\n260|            Hidden(StrOptions({\"deprecated\"})),\n261|        ],\n262|    }\n263|\n264|    def __init__(\n265|        self,\n266|        estimator=None,\n267|        *,\n268|        method=\"sigmoid\",\n269|        cv=None,\n270|        n_jobs=None,\n271|        ensemble=True,\n272|        base_estimator=\"deprecated\",\n273|    ):\n274|        self.estimator = estimator\n275|        self.method = method\n276|        self.cv = cv\n277|        self.n_jobs = n_jobs\n278|        self.ensemble = ensemble\n279|        self.base_estimator = base_estimator\n280|\n281|    def fit(self, X, y, sample_weight=None, **fit_params):\n282|        \"\"\"Fit the calibrated model.\n283|\n284|        Parameters\n285|        ----------\n286|        X : array-like of shape (n_samples, n_features)\n287|            Training data.\n288|\n289|        y : array-like of shape (n_samples,)\n290|            Target values.\n291|\n292|        sample_weight : array-like of shape (n_samples,), default=None\n293|            Sample weights. If None, then samples are equally weighted.\n294|\n295|        **fit_params : dict\n296|            Parameters to pass to the `fit` method of the underlying\n297|            classifier.\n298|\n299|        Returns\n300|        -------\n301|        self : object\n302|            Returns an instance of self.\n303|        \"\"\"\n304|        self._validate_params()\n305|\n306|        check_classification_targets(y)\n307|        X, y = indexable(X, y)\n308|        if sample_weight is not None:\n309|            sample_weight = _check_sample_weight(sample_weight, X)\n310|\n311|        for sample_aligned_params in fit_params.values():\n312|            check_consistent_length(y, sample_aligned_params)\n313|\n314|        # TODO(1.4): Remove when base_estimator is removed\n315|        if self.base_estimator != \"deprecated\":\n316|            if self.estimator is not None:\n317|                raise ValueError(\n318|                    \"Both `base_estimator` and `estimator` are set. Only set \"\n319|                    \"`estimator` since `base_estimator` is deprecated.\"\n320|                )\n321|            warnings.warn(\n322|                \"`base_estimator` was renamed to `estimator` in version 1.2 and \"\n323|                \"will be removed in 1.4.\",\n324|                FutureWarning,\n325|            )\n326|            estimator = self.base_estimator\n327|        else:\n328|            estimator = self.estimator\n329|\n330|        if estimator is None:\n331|            # we want all classifiers that don't expose a random_state\n332|            # to be deterministic (and we don't want to expose this one).\n333|            estimator = LinearSVC(random_state=0)\n334|\n335|        self.calibrated_classifiers_ = []\n336|        if self.cv == \"prefit\":\n337|            # `classes_` should be consistent with that of estimator\n338|            check_is_fitted(self.estimator, attributes=[\"classes_\"])\n339|            self.classes_ = self.estimator.classes_\n340|\n341|            pred_method, method_name = _get_prediction_method(estimator)\n342|            n_classes = len(self.classes_)\n343|            predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n344|\n345|            calibrated_classifier = _fit_calibrator(\n346|                estimator,\n347|                predictions,\n348|                y,\n349|                self.classes_,\n350|                self.method,\n351|                sample_weight,\n352|            )\n353|            self.calibrated_classifiers_.append(calibrated_classifier)\n354|        else:\n355|            # Set `classes_` using all `y`\n356|            label_encoder_ = LabelEncoder().fit(y)\n357|            self.classes_ = label_encoder_.classes_\n358|            n_classes = len(self.classes_)\n359|\n360|            # sample_weight checks\n361|            fit_parameters = signature(estimator.fit).parameters\n362|            supports_sw = \"sample_weight\" in fit_parameters\n363|            if sample_weight is not None and not supports_sw:\n364|                estimator_name = type(estimator).__name__\n365|                warnings.warn(\n366|                    f\"Since {estimator_name} does not appear to accept sample_weight, \"\n367|                    \"sample weights will only be used for the calibration itself. This \"\n368|                    \"can be caused by a limitation of the current scikit-learn API. \"\n369|                    \"See the following issue for more details: \"\n370|                    \"https://github.com/scikit-learn/scikit-learn/issues/21134. Be \"\n371|                    \"warned that the result of the calibration is likely to be \"\n372|                    \"incorrect.\"\n373|                )\n374|\n375|            # Check that each cross-validation fold can have at least one\n376|            # example per class\n377|            if isinstance(self.cv, int):\n378|                n_folds = self.cv\n379|            elif hasattr(self.cv, \"n_splits\"):\n380|                n_folds = self.cv.n_splits\n381|            else:\n382|                n_folds = None\n383|            if n_folds and np.any(\n384|                [np.sum(y == class_) < n_folds for class_ in self.classes_]\n385|            ):\n386|                raise ValueError(\n387|                    f\"Requesting {n_folds}-fold \"\n388|                    \"cross-validation but provided less than \"\n389|                    f\"{n_folds} examples for at least one class.\"\n390|                )\n391|            cv = check_cv(self.cv, y, classifier=True)\n392|\n393|            if self.ensemble:\n394|                parallel = Parallel(n_jobs=self.n_jobs)\n395|                self.calibrated_classifiers_ = parallel(\n396|                    delayed(_fit_classifier_calibrator_pair)(\n397|                        clone(estimator),\n398|                        X,\n399|                        y,\n400|                        train=train,\n401|                        test=test,\n402|                        method=self.method,\n403|                        classes=self.classes_,\n404|                        supports_sw=supports_sw,\n405|                        sample_weight=sample_weight,\n406|                        **fit_params,\n407|                    )\n408|                    for train, test in cv.split(X, y)\n409|                )\n410|            else:\n411|                this_estimator = clone(estimator)\n412|                _, method_name = _get_prediction_method(this_estimator)\n413|                fit_params = (\n414|                    {\"sample_weight\": sample_weight}\n415|                    if sample_weight is not None and supports_sw\n416|                    else None\n417|                )\n418|                pred_method = partial(\n419|                    cross_val_predict,\n420|                    estimator=this_estimator,\n421|                    X=X,\n422|                    y=y,\n423|                    cv=cv,\n424|                    method=method_name,\n425|                    n_jobs=self.n_jobs,\n426|                    fit_params=fit_params,\n427|                )\n428|                predictions = _compute_predictions(\n429|                    pred_method, method_name, X, n_classes\n430|                )\n431|\n432|                if sample_weight is not None and supports_sw:\n433|                    this_estimator.fit(X, y, sample_weight=sample_weight)\n434|                else:\n435|                    this_estimator.fit(X, y)\n436|                # Note: Here we don't pass on fit_params because the supported\n437|                # calibrators don't support fit_params anyway\n438|                calibrated_classifier = _fit_calibrator(\n439|                    this_estimator,\n440|                    predictions,\n441|                    y,\n442|                    self.classes_,\n443|                    self.method,\n444|                    sample_weight,\n445|                )\n446|                self.calibrated_classifiers_.append(calibrated_classifier)\n447|\n448|        first_clf = self.calibrated_classifiers_[0].estimator\n449|        if hasattr(first_clf, \"n_features_in_\"):\n450|            self.n_features_in_ = first_clf.n_features_in_\n451|        if hasattr(first_clf, \"feature_names_in_\"):\n452|            self.feature_names_in_ = first_clf.feature_names_in_\n453|        return self\n454|\n455|    def predict_proba(self, X):\n456|        \"\"\"Calibrated probabilities of classification.\n457|\n458|        This function returns calibrated probabilities of classification\n459|        according to each class on an array of test vectors X.\n460|\n461|        Parameters\n462|        ----------\n463|        X : array-like of shape (n_samples, n_features)\n464|            The samples, as accepted by `estimator.predict_proba`.\n465|\n466|        Returns\n467|        -------\n468|        C : ndarray of shape (n_samples, n_classes)\n469|            The predicted probas.\n470|        \"\"\"\n471|        check_is_fitted(self)\n472|        # Compute the arithmetic mean of the predictions of the calibrated\n473|        # classifiers\n474|        mean_proba = np.zeros((_num_samples(X), len(self.classes_)))\n475|        for calibrated_classifier in self.calibrated_classifiers_:\n476|            proba = calibrated_classifier.predict_proba(X)\n477|            mean_proba += proba\n478|\n479|        mean_proba /= len(self.calibrated_classifiers_)\n480|\n481|        return mean_proba\n482|\n483|    def predict(self, X):\n484|        \"\"\"Predict the target of new samples.\n485|\n486|        The predicted class is the class that has the highest probability,\n487|        and can thus be different from the prediction of the uncalibrated classifier.\n488|\n489|        Parameters\n490|        ----------\n491|        X : array-like of shape (n_samples, n_features)\n492|            The samples, as accepted by `estimator.predict`.\n493|\n494|        Returns\n495|        -------\n496|        C : ndarray of shape (n_samples,)\n497|            The predicted class.\n498|        \"\"\"\n499|        check_is_fitted(self)\n500|        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n501|\n502|    def _more_tags(self):\n503|        return {\n504|            \"_xfail_checks\": {\n505|                \"check_sample_weights_invariance\": (\n506|                    \"Due to the cross-validation and sample ordering, removing a sample\"\n507|                    \" is not strictly equal to putting is weight to zero. Specific unit\"\n508|                    \" tests are added for CalibratedClassifierCV specifically.\"\n509|                ),\n510|            }\n511|        }\n512|\n513|\n514|def _fit_classifier_calibrator_pair(\n515|    estimator,\n516|    X,\n517|    y,\n518|    train,\n519|    test,\n520|    supports_sw,\n521|    method,\n522|    classes,\n523|    sample_weight=None,\n524|    **fit_params,\n525|):\n526|    \"\"\"Fit a classifier/calibration pair on a given train/test split.\n527|\n528|    Fit the classifier on the train set, compute its predictions on the test\n529|    set and use the predictions as input to fit the calibrator along with the\n530|    test labels.\n531|\n532|    Parameters\n533|    ----------\n534|    estimator : estimator instance\n535|        Cloned base estimator.\n536|\n537|    X : array-like, shape (n_samples, n_features)\n538|        Sample data.\n539|\n540|    y : array-like, shape (n_samples,)\n541|        Targets.\n542|\n543|    train : ndarray, shape (n_train_indices,)\n544|        Indices of the training subset.\n545|\n546|    test : ndarray, shape (n_test_indices,)\n547|        Indices of the testing subset.\n548|\n549|    supports_sw : bool\n550|        Whether or not the `estimator` supports sample weights.\n551|\n552|    method : {'sigmoid', 'isotonic'}\n553|        Method to use for calibration.\n554|\n555|    classes : ndarray, shape (n_classes,)\n556|        The target classes.\n557|\n558|    sample_weight : array-like, default=None\n559|        Sample weights for `X`.\n560|\n561|    **fit_params : dict\n562|        Parameters to pass to the `fit` method of the underlying\n563|        classifier.\n564|\n565|    Returns\n566|    -------\n567|    calibrated_classifier : _CalibratedClassifier instance\n568|    \"\"\"\n569|    fit_params_train = _check_fit_params(X, fit_params, train)\n570|    X_train, y_train = _safe_indexing(X, train), _safe_indexing(y, train)\n571|    X_test, y_test = _safe_indexing(X, test), _safe_indexing(y, test)\n572|\n573|    if sample_weight is not None and supports_sw:\n574|        sw_train = _safe_indexing(sample_weight, train)\n575|        estimator.fit(X_train, y_train, sample_weight=sw_train, **fit_params_train)\n576|    else:\n577|        estimator.fit(X_train, y_train, **fit_params_train)\n578|\n579|    n_classes = len(classes)\n580|    pred_method, method_name = _get_prediction_method(estimator)\n581|    predictions = _compute_predictions(pred_method, method_name, X_test, n_classes)\n582|\n583|    sw_test = None if sample_weight is None else _safe_indexing(sample_weight, test)\n584|    calibrated_classifier = _fit_calibrator(\n585|        estimator, predictions, y_test, classes, method, sample_weight=sw_test\n586|    )\n587|    return calibrated_classifier\n588|\n589|\n590|def _get_prediction_method(clf):\n591|    \"\"\"Return prediction method.\n592|\n593|    `decision_function` method of `clf` returned, if it\n594|    exists, otherwise `predict_proba` method returned.\n595|\n596|    Parameters\n597|    ----------\n598|    clf : Estimator instance\n599|        Fitted classifier to obtain the prediction method from.\n600|\n601|    Returns\n602|    -------\n603|    prediction_method : callable\n604|        The prediction method.\n605|    method_name : str\n606|        The name of the prediction method.\n607|    \"\"\"\n608|    if hasattr(clf, \"decision_function\"):\n609|        method = getattr(clf, \"decision_function\")\n610|        return method, \"decision_function\"\n611|\n612|    if hasattr(clf, \"predict_proba\"):\n613|        method = getattr(clf, \"predict_proba\")\n614|        return method, \"predict_proba\"\n615|\n616|\n617|def _compute_predictions(pred_method, method_name, X, n_classes):\n618|    \"\"\"Return predictions for `X` and reshape binary outputs to shape\n619|    (n_samples, 1).\n620|\n621|    Parameters\n622|    ----------\n623|    pred_method : callable\n624|        Prediction method.\n625|\n626|    method_name: str\n627|        Name of the prediction method\n628|\n629|    X : array-like or None\n630|        Data used to obtain predictions.\n631|\n632|    n_classes : int\n633|        Number of classes present.\n634|\n635|    Returns\n636|    -------\n637|    predictions : array-like, shape (X.shape[0], len(clf.classes_))\n638|        The predictions. Note if there are 2 classes, array is of shape\n639|        (X.shape[0], 1).\n640|    \"\"\"\n641|    predictions = pred_method(X=X)\n642|\n643|    if method_name == \"decision_function\":\n644|        if predictions.ndim == 1:\n645|            predictions = predictions[:, np.newaxis]\n646|    elif method_name == \"predict_proba\":\n647|        if n_classes == 2:\n648|            predictions = predictions[:, 1:]\n649|    else:  # pragma: no cover\n650|        # this branch should be unreachable.\n651|        raise ValueError(f\"Invalid prediction method: {method_name}\")\n652|    return predictions\n653|\n654|\n655|def _fit_calibrator(clf, predictions, y, classes, method, sample_weight=None):\n656|    \"\"\"Fit calibrator(s) and return a `_CalibratedClassifier`\n657|    instance.\n658|\n659|    `n_classes` (i.e. `len(clf.classes_)`) calibrators are fitted.\n660|    However, if `n_classes` equals 2, one calibrator is fitted.\n661|\n662|    Parameters\n663|    ----------\n664|    clf : estimator instance\n665|        Fitted classifier.\n666|\n667|    predictions : array-like, shape (n_samples, n_classes) or (n_samples, 1) \\\n668|                    when binary.\n669|        Raw predictions returned by the un-calibrated base classifier.\n670|\n671|    y : array-like, shape (n_samples,)\n672|        The targets.\n673|\n674|    classes : ndarray, shape (n_classes,)\n675|        All the prediction classes.\n676|\n677|    method : {'sigmoid', 'isotonic'}\n678|        The method to use for calibration.\n679|\n680|    sample_weight : ndarray, shape (n_samples,), default=None\n681|        Sample weights. If None, then samples are equally weighted.\n682|\n683|    Returns\n684|    -------\n685|    pipeline : _CalibratedClassifier instance\n686|    \"\"\"\n687|    Y = label_binarize(y, classes=classes)\n688|    label_encoder = LabelEncoder().fit(classes)\n689|    pos_class_indices = label_encoder.transform(clf.classes_)\n690|    calibrators = []\n691|    for class_idx, this_pred in zip(pos_class_indices, predictions.T):\n692|        if method == \"isotonic\":\n693|            calibrator = IsotonicRegression(out_of_bounds=\"clip\")\n694|        else:  # \"sigmoid\"\n695|            calibrator = _SigmoidCalibration()\n696|        calibrator.fit(this_pred, Y[:, class_idx], sample_weight)\n697|        calibrators.append(calibrator)\n698|\n699|    pipeline = _CalibratedClassifier(clf, calibrators, method=method, classes=classes)\n700|    return pipeline\n701|\n702|\n703|class _CalibratedClassifier:\n704|    \"\"\"Pipeline-like chaining a fitted classifier and its fitted calibrators.\n705|\n706|    Parameters\n707|    ----------\n708|    estimator : estimator instance\n709|        Fitted classifier.\n710|\n711|    calibrators : list of fitted estimator instances\n712|        List of fitted calibrators (either 'IsotonicRegression' or\n713|        '_SigmoidCalibration'). The number of calibrators equals the number of\n714|        classes. However, if there are 2 classes, the list contains only one\n715|        fitted calibrator.\n716|\n717|    classes : array-like of shape (n_classes,)\n718|        All the prediction classes.\n719|\n720|    method : {'sigmoid', 'isotonic'}, default='sigmoid'\n721|        The method to use for calibration. Can be 'sigmoid' which\n722|        corresponds to Platt's method or 'isotonic' which is a\n723|        non-parametric approach based on isotonic regression.\n724|    \"\"\"\n725|\n726|    def __init__(self, estimator, calibrators, *, classes, method=\"sigmoid\"):\n727|        self.estimator = estimator\n728|        self.calibrators = calibrators\n729|        self.classes = classes\n730|        self.method = method\n731|\n732|    def predict_proba(self, X):\n733|        \"\"\"Calculate calibrated probabilities.\n734|\n735|        Calculates classification calibrated probabilities\n736|        for each class, in a one-vs-all manner, for `X`.\n737|\n738|        Parameters\n739|        ----------\n740|        X : ndarray of shape (n_samples, n_features)\n741|            The sample data.\n742|\n743|        Returns\n744|        -------\n745|        proba : array, shape (n_samples, n_classes)\n746|            The predicted probabilities. Can be exact zeros.\n747|        \"\"\"\n748|        n_classes = len(self.classes)\n749|        pred_method, method_name = _get_prediction_method(self.estimator)\n750|        predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n751|\n752|        label_encoder = LabelEncoder().fit(self.classes)\n753|        pos_class_indices = label_encoder.transform(self.estimator.classes_)\n754|\n755|        proba = np.zeros((_num_samples(X), n_classes))\n756|        for class_idx, this_pred, calibrator in zip(\n757|            pos_class_indices, predictions.T, self.calibrators\n758|        ):\n759|            if n_classes == 2:\n760|                # When binary, `predictions` consists only of predictions for\n761|                # clf.classes_[1] but `pos_class_indices` = 0\n762|                class_idx += 1\n763|            proba[:, class_idx] = calibrator.predict(this_pred)\n764|\n765|        # Normalize the probabilities\n766|        if n_classes == 2:\n767|            proba[:, 0] = 1.0 - proba[:, 1]\n768|        else:\n769|            denominator = np.sum(proba, axis=1)[:, np.newaxis]\n770|            # In the edge case where for each class calibrator returns a null\n771|            # probability for a given sample, use the uniform distribution\n772|            # instead.\n773|            uniform_proba = np.full_like(proba, 1 / n_classes)\n774|            proba = np.divide(\n775|                proba, denominator, out=uniform_proba, where=denominator != 0\n776|            )\n777|\n778|        # Deal with cases where the predicted probability minimally exceeds 1.0\n779|        proba[(1.0 < proba) & (proba <= 1.0 + 1e-5)] = 1.0\n780|\n781|        return proba\n782|\n783|\n784|def _sigmoid_calibration(predictions, y, sample_weight=None):\n785|    \"\"\"Probability Calibration with sigmoid method (Platt 2000)\n786|\n787|    Parameters\n788|    ----------\n789|    predictions : ndarray of shape (n_samples,)\n790|        The decision function or predict proba for the samples.\n791|\n...\n\n\n### sklearn/isotonic.py\n...\n127|    if y_min is not None or y_max is not None:\n128|        # Older versions of np.clip don't accept None as a bound, so use np.inf\n129|        if y_min is None:\n130|            y_min = -np.inf\n131|        if y_max is None:\n132|            y_max = np.inf\n133|        np.clip(y, y_min, y_max, y)\n134|    return y[order]\n135|\n136|\n137|class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):\n138|    \"\"\"Isotonic regression model.\n139|\n140|    Read more in the :ref:`User Guide <isotonic>`.\n141|\n142|    .. versionadded:: 0.13\n143|\n144|    Parameters\n145|    ----------\n146|    y_min : float, default=None\n147|        Lower bound on the lowest predicted value (the minimum value may\n148|        still be higher). If not set, defaults to -inf.\n149|\n150|    y_max : float, default=None\n151|        Upper bound on the highest predicted value (the maximum may still be\n152|        lower). If not set, defaults to +inf.\n153|\n154|    increasing : bool or 'auto', default=True\n155|        Determines whether the predictions should be constrained to increase\n156|        or decrease with `X`. 'auto' will decide based on the Spearman\n157|        correlation estimate's sign.\n158|\n159|    out_of_bounds : {'nan', 'clip', 'raise'}, default='nan'\n160|        Handles how `X` values outside of the training domain are handled\n161|        during prediction.\n162|\n163|        - 'nan', predictions will be NaN.\n164|        - 'clip', predictions will be set to the value corresponding to\n165|          the nearest train interval endpoint.\n166|        - 'raise', a `ValueError` is raised.\n167|\n168|    Attributes\n169|    ----------\n170|    X_min_ : float\n171|        Minimum value of input array `X_` for left bound.\n172|\n173|    X_max_ : float\n174|        Maximum value of input array `X_` for right bound.\n175|\n176|    X_thresholds_ : ndarray of shape (n_thresholds,)\n177|        Unique ascending `X` values used to interpolate\n178|        the y = f(X) monotonic function.\n179|\n180|        .. versionadded:: 0.24\n181|\n182|    y_thresholds_ : ndarray of shape (n_thresholds,)\n183|        De-duplicated `y` values suitable to interpolate the y = f(X)\n184|        monotonic function.\n185|\n186|        .. versionadded:: 0.24\n187|\n188|    f_ : function\n189|        The stepwise interpolating function that covers the input domain ``X``.\n190|\n191|    increasing_ : bool\n192|        Inferred value for ``increasing``.\n193|\n194|    See Also\n195|    --------\n196|    sklearn.linear_model.LinearRegression : Ordinary least squares Linear\n197|        Regression.\n198|    sklearn.ensemble.HistGradientBoostingRegressor : Gradient boosting that\n199|        is a non-parametric model accepting monotonicity constraints.\n200|    isotonic_regression : Function to solve the isotonic regression model.\n201|\n202|    Notes\n203|    -----\n204|    Ties are broken using the secondary method from de Leeuw, 1977.\n205|\n206|    References\n207|    ----------\n208|    Isotonic Median Regression: A Linear Programming Approach\n209|    Nilotpal Chakravarti\n210|    Mathematics of Operations Research\n211|    Vol. 14, No. 2 (May, 1989), pp. 303-308\n212|\n213|    Isotone Optimization in R : Pool-Adjacent-Violators\n214|    Algorithm (PAVA) and Active Set Methods\n215|    de Leeuw, Hornik, Mair\n216|    Journal of Statistical Software 2009\n217|\n218|    Correctness of Kruskal's algorithms for monotone regression with ties\n219|    de Leeuw, Psychometrica, 1977\n220|\n221|    Examples\n222|    --------\n223|    >>> from sklearn.datasets import make_regression\n224|    >>> from sklearn.isotonic import IsotonicRegression\n225|    >>> X, y = make_regression(n_samples=10, n_features=1, random_state=41)\n226|    >>> iso_reg = IsotonicRegression().fit(X, y)\n227|    >>> iso_reg.predict([.1, .2])\n228|    array([1.8628..., 3.7256...])\n229|    \"\"\"\n230|\n231|    _parameter_constraints: dict = {\n232|        \"y_min\": [Interval(Real, None, None, closed=\"both\"), None],\n233|        \"y_max\": [Interval(Real, None, None, closed=\"both\"), None],\n234|        \"increasing\": [\"boolean\", StrOptions({\"auto\"})],\n235|        \"out_of_bounds\": [StrOptions({\"nan\", \"clip\", \"raise\"})],\n236|    }\n237|\n238|    def __init__(self, *, y_min=None, y_max=None, increasing=True, out_of_bounds=\"nan\"):\n239|        self.y_min = y_min\n240|        self.y_max = y_max\n241|        self.increasing = increasing\n242|        self.out_of_bounds = out_of_bounds\n243|\n244|    def _check_input_data_shape(self, X):\n245|        if not (X.ndim == 1 or (X.ndim == 2 and X.shape[1] == 1)):\n246|            msg = (\n247|                \"Isotonic regression input X should be a 1d array or \"\n248|                \"2d array with 1 feature\"\n249|            )\n250|            raise ValueError(msg)\n251|\n252|    def _build_f(self, X, y):\n253|        \"\"\"Build the f_ interp1d function.\"\"\"\n254|\n255|        bounds_error = self.out_of_bounds == \"raise\"\n256|        if len(y) == 1:\n257|            # single y, constant prediction\n258|            self.f_ = lambda x: y.repeat(x.shape)\n259|        else:\n260|            self.f_ = interpolate.interp1d(\n261|                X, y, kind=\"linear\", bounds_error=bounds_error\n262|            )\n263|\n264|    def _build_y(self, X, y, sample_weight, trim_duplicates=True):\n265|        \"\"\"Build the y_ IsotonicRegression.\"\"\"\n266|        self._check_input_data_shape(X)\n267|        X = X.reshape(-1)  # use 1d view\n268|\n269|        # Determine increasing if auto-determination requested\n270|        if self.increasing == \"auto\":\n271|            self.increasing_ = check_increasing(X, y)\n272|        else:\n273|            self.increasing_ = self.increasing\n274|\n275|        # If sample_weights is passed, removed zero-weight values and clean\n276|        # order\n277|        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n278|        mask = sample_weight > 0\n279|        X, y, sample_weight = X[mask], y[mask], sample_weight[mask]\n280|\n281|        order = np.lexsort((y, X))\n282|        X, y, sample_weight = [array[order] for array in [X, y, sample_weight]]\n283|        unique_X, unique_y, unique_sample_weight = _make_unique(X, y, sample_weight)\n284|\n285|        X = unique_X\n286|        y = isotonic_regression(\n287|            unique_y,\n288|            sample_weight=unique_sample_weight,\n289|            y_min=self.y_min,\n290|            y_max=self.y_max,\n291|            increasing=self.increasing_,\n292|        )\n293|\n294|        # Handle the left and right bounds on X\n295|        self.X_min_, self.X_max_ = np.min(X), np.max(X)\n296|\n297|        if trim_duplicates:\n298|            # Remove unnecessary points for faster prediction\n299|            keep_data = np.ones((len(y),), dtype=bool)\n300|            # Aside from the 1st and last point, remove points whose y values\n301|            # are equal to both the point before and the point after it.\n302|            keep_data[1:-1] = np.logical_or(\n303|                np.not_equal(y[1:-1], y[:-2]), np.not_equal(y[1:-1], y[2:])\n304|            )\n305|            return X[keep_data], y[keep_data]\n306|        else:\n307|            # The ability to turn off trim_duplicates is only used to it make\n308|            # easier to unit test that removing duplicates in y does not have\n309|            # any impact the resulting interpolation function (besides\n310|            # prediction speed).\n311|            return X, y\n312|\n313|    def fit(self, X, y, sample_weight=None):\n314|        \"\"\"Fit the model using X, y as training data.\n315|\n316|        Parameters\n317|        ----------\n318|        X : array-like of shape (n_samples,) or (n_samples, 1)\n319|            Training data.\n320|\n321|            .. versionchanged:: 0.24\n322|               Also accepts 2d array with 1 feature.\n323|\n324|        y : array-like of shape (n_samples,)\n325|            Training target.\n326|\n327|        sample_weight : array-like of shape (n_samples,), default=None\n328|            Weights. If set to None, all weights will be set to 1 (equal\n329|            weights).\n330|\n331|        Returns\n332|        -------\n333|        self : object\n334|            Returns an instance of self.\n335|\n336|        Notes\n337|        -----\n338|        X is stored for future use, as :meth:`transform` needs X to interpolate\n339|        new input data.\n340|        \"\"\"\n341|        self._validate_params()\n342|        check_params = dict(accept_sparse=False, ensure_2d=False)\n343|        X = check_array(\n344|            X, input_name=\"X\", dtype=[np.float64, np.float32], **check_params\n345|        )\n346|        y = check_array(y, input_name=\"y\", dtype=X.dtype, **check_params)\n347|        check_consistent_length(X, y, sample_weight)\n348|\n349|        # Transform y by running the isotonic regression algorithm and\n350|        # transform X accordingly.\n351|        X, y = self._build_y(X, y, sample_weight)\n352|\n353|        # It is necessary to store the non-redundant part of the training set\n354|        # on the model to make it possible to support model persistence via\n355|        # the pickle module as the object built by scipy.interp1d is not\n356|        # picklable directly.\n357|        self.X_thresholds_, self.y_thresholds_ = X, y\n358|\n359|        # Build the interpolation function\n360|        self._build_f(X, y)\n361|        return self\n362|\n363|    def transform(self, T):\n364|        \"\"\"Transform new data by linear interpolation.\n365|\n366|        Parameters\n367|        ----------\n368|        T : array-like of shape (n_samples,) or (n_samples, 1)\n369|            Data to transform.\n370|\n371|            .. versionchanged:: 0.24\n372|               Also accepts 2d array with 1 feature.\n373|\n374|        Returns\n375|        -------\n376|        y_pred : ndarray of shape (n_samples,)\n377|            The transformed data.\n378|        \"\"\"\n379|\n380|        if hasattr(self, \"X_thresholds_\"):\n381|            dtype = self.X_thresholds_.dtype\n382|        else:\n383|            dtype = np.float64\n384|\n385|        T = check_array(T, dtype=dtype, ensure_2d=False)\n386|\n387|        self._check_input_data_shape(T)\n388|        T = T.reshape(-1)  # use 1d view\n389|\n390|        if self.out_of_bounds == \"clip\":\n391|            T = np.clip(T, self.X_min_, self.X_max_)\n392|\n393|        res = self.f_(T)\n394|\n395|        # on scipy 0.17, interp1d up-casts to float64, so we cast back\n396|        res = res.astype(T.dtype)\n397|\n398|        return res\n399|\n400|    def predict(self, T):\n401|        \"\"\"Predict new data by linear interpolation.\n402|\n403|        Parameters\n404|        ----------\n405|        T : array-like of shape (n_samples,) or (n_samples, 1)\n406|            Data to transform.\n407|\n408|        Returns\n409|        -------\n410|        y_pred : ndarray of shape (n_samples,)\n411|            Transformed data.\n412|        \"\"\"\n413|        return self.transform(T)\n414|\n415|    # We implement get_feature_names_out here instead of using\n416|    # `ClassNamePrefixFeaturesOutMixin`` because `input_features` are ignored.\n417|    # `input_features` are ignored because `IsotonicRegression` accepts 1d\n418|    # arrays and the semantics of `feature_names_in_` are not clear for 1d arrays.\n419|    def get_feature_names_out(self, input_features=None):\n420|        \"\"\"Get output feature names for transformation.\n421|\n422|        Parameters\n423|        ----------\n424|        input_features : array-like of str or None, default=None\n425|            Ignored.\n426|\n427|        Returns\n428|        -------\n429|        feature_names_out : ndarray of str objects\n430|            An ndarray with one string i.e. [\"isotonicregression0\"].\n431|        \"\"\"\n432|        check_is_fitted(self, \"f_\")\n433|        class_name = self.__class__.__name__.lower()\n434|        return np.asarray([f\"{class_name}0\"], dtype=object)\n435|\n436|    def __getstate__(self):\n437|        \"\"\"Pickle-protocol - return state of the estimator.\"\"\"\n438|        state = super().__getstate__()\n439|        # remove interpolation method\n440|        state.pop(\"f_\", None)\n441|        return state\n442|\n443|    def __setstate__(self, state):\n444|        \"\"\"Pickle-protocol - set state of the estimator.\n445|\n446|        We need to rebuild the interpolation function.\n447|        \"\"\"\n448|        super().__setstate__(state)\n449|        if hasattr(self, \"X_thresholds_\") and hasattr(self, \"y_thresholds_\"):\n450|            self._build_f(self.X_thresholds_, self.y_thresholds_)\n451|\n452|    def _more_tags(self):\n453|        return {\"X_types\": [\"1darray\"]}\n\n\n### sklearn/_config.py\n1|\"\"\"Global configuration state and functions for management\n2|\"\"\"\n3|import os\n4|from contextlib import contextmanager as contextmanager\n5|import threading\n6|\n7|_global_config = {\n8|    \"assume_finite\": bool(os.environ.get(\"SKLEARN_ASSUME_FINITE\", False)),\n9|    \"working_memory\": int(os.environ.get(\"SKLEARN_WORKING_MEMORY\", 1024)),\n10|    \"print_changed_only\": True,\n11|    \"display\": \"diagram\",\n12|    \"pairwise_dist_chunk_size\": int(\n13|        os.environ.get(\"SKLEARN_PAIRWISE_DIST_CHUNK_SIZE\", 256)\n14|    ),\n15|    \"enable_cython_pairwise_dist\": True,\n16|    \"array_api_dispatch\": False,\n17|    \"transform_output\": \"default\",\n18|}\n19|_threadlocal = threading.local()\n20|\n21|\n22|def _get_threadlocal_config():\n23|    \"\"\"Get a threadlocal **mutable** configuration. If the configuration\n24|    does not exist, copy the default global configuration.\"\"\"\n25|    if not hasattr(_threadlocal, \"global_config\"):\n26|        _threadlocal.global_config = _global_config.copy()\n27|    return _threadlocal.global_config\n28|\n...\n38|    See Also\n39|    --------\n40|    config_context : Context manager for global scikit-learn configuration.\n41|    set_config : Set global scikit-learn configuration.\n42|    \"\"\"\n43|    # Return a copy of the threadlocal configuration so that users will\n44|    # not be able to modify the configuration with the returned dict.\n45|    return _get_threadlocal_config().copy()\n46|\n47|\n48|def set_config(\n49|    assume_finite=None,\n50|    working_memory=None,\n51|    print_changed_only=None,\n52|    display=None,\n53|    pairwise_dist_chunk_size=None,\n54|    enable_cython_pairwise_dist=None,\n55|    array_api_dispatch=None,\n56|    transform_output=None,\n57|):\n58|    \"\"\"Set global scikit-learn configuration\n59|\n60|    .. versionadded:: 0.19\n61|\n62|    Parameters\n63|    ----------\n64|    assume_finite : bool, default=None\n65|        If True, validation for finiteness will be skipped,\n66|        saving time, but leading to potential crashes. If\n67|        False, validation for finiteness will be performed,\n68|        avoiding error.  Global default: False.\n69|\n70|        .. versionadded:: 0.19\n71|\n72|    working_memory : int, default=None\n73|        If set, scikit-learn will attempt to limit the size of temporary arrays\n74|        to this number of MiB (per job when parallelised), often saving both\n75|        computation time and memory on expensive operations that can be\n76|        performed in chunks. Global default: 1024.\n77|\n78|        .. versionadded:: 0.20\n79|\n80|    print_changed_only : bool, default=None\n81|        If True, only the parameters that were set to non-default\n82|        values will be printed when printing an estimator. For example,\n83|        ``print(SVC())`` while True will only print 'SVC()' while the default\n84|        behaviour would be to print 'SVC(C=1.0, cache_size=200, ...)' with\n85|        all the non-changed parameters.\n86|\n87|        .. versionadded:: 0.21\n88|\n89|    display : {'text', 'diagram'}, default=None\n90|        If 'diagram', estimators will be displayed as a diagram in a Jupyter\n91|        lab or notebook context. If 'text', estimators will be displayed as\n92|        text. Default is 'diagram'.\n93|\n94|        .. versionadded:: 0.23\n95|\n96|    pairwise_dist_chunk_size : int, default=None\n97|        The number of row vectors per chunk for the accelerated pairwise-\n98|        distances reduction backend. Default is 256 (suitable for most of\n99|        modern laptops' caches and architectures).\n100|\n101|        Intended for easier benchmarking and testing of scikit-learn internals.\n102|        End users are not expected to benefit from customizing this configuration\n103|        setting.\n104|\n105|        .. versionadded:: 1.1\n106|\n107|    enable_cython_pairwise_dist : bool, default=None\n108|        Use the accelerated pairwise-distances reduction backend when\n109|        possible. Global default: True.\n110|\n111|        Intended for easier benchmarking and testing of scikit-learn internals.\n112|        End users are not expected to benefit from customizing this configuration\n113|        setting.\n114|\n115|        .. versionadded:: 1.1\n116|\n117|    array_api_dispatch : bool, default=None\n118|        Use Array API dispatching when inputs follow the Array API standard.\n119|        Default is False.\n120|\n121|        See the :ref:`User Guide <array_api>` for more details.\n122|\n123|        .. versionadded:: 1.2\n124|\n125|    transform_output : str, default=None\n126|        Configure output of `transform` and `fit_transform`.\n127|\n128|        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n129|        for an example on how to use the API.\n130|\n131|        - `\"default\"`: Default output format of a transformer\n132|        - `\"pandas\"`: DataFrame output\n133|        - `None`: Transform configuration is unchanged\n134|\n135|        .. versionadded:: 1.2\n136|\n137|    See Also\n138|    --------\n139|    config_context : Context manager for global scikit-learn configuration.\n140|    get_config : Retrieve current values of the global configuration.\n141|    \"\"\"\n142|    local_config = _get_threadlocal_config()\n143|\n144|    if assume_finite is not None:\n145|        local_config[\"assume_finite\"] = assume_finite\n146|    if working_memory is not None:\n147|        local_config[\"working_memory\"] = working_memory\n148|    if print_changed_only is not None:\n149|        local_config[\"print_changed_only\"] = print_changed_only\n150|    if display is not None:\n151|        local_config[\"display\"] = display\n152|    if pairwise_dist_chunk_size is not None:\n153|        local_config[\"pairwise_dist_chunk_size\"] = pairwise_dist_chunk_size\n154|    if enable_cython_pairwise_dist is not None:\n155|        local_config[\"enable_cython_pairwise_dist\"] = enable_cython_pairwise_dist\n156|    if array_api_dispatch is not None:\n157|        local_config[\"array_api_dispatch\"] = array_api_dispatch\n158|    if transform_output is not None:\n159|        local_config[\"transform_output\"] = transform_output\n160|\n161|\n162|@contextmanager\n163|def config_context(\n164|    *,\n165|    assume_finite=None,\n166|    working_memory=None,\n167|    print_changed_only=None,\n168|    display=None,\n169|    pairwise_dist_chunk_size=None,\n...\n\n\n\n\n###\n\nPlease provide the class name, function or method name, or the exact line numbers that need to be edited.\n### Examples:\n```\nfull_path1/file1.py\nline: 10\nclass: MyClass1\nline: 51\n\nfull_path2/file2.py\nfunction: MyClass2.my_method\nline: 12\n\nfull_path3/file3.py\nfunction: my_function\nline: 24\nline: 156\n```\n\nReturn just the location(s)\n", "response": ["```\nsklearn/calibration.py\nfunction: _fit_calibrator\nline: 655\n\nsklearn/calibration.py\nclass: _CalibratedClassifier\nfunction: predict_proba\nline: 763\n```", "```\nsklearn/calibration.py\nclass: _CalibratedClassifier\nfunction: _CalibratedClassifier.predict_proba\nline: 763\n```", "```\nsklearn/calibration.py\nclass: _CalibratedClassifier\nfunction: _CalibratedClassifier.predict_proba\nline: 763\n\nsklearn/isotonic.py\nclass: IsotonicRegression\nfunction: IsotonicRegression.predict\nline: 413\n```", "```\nsklearn/calibration.py\nfunction: _CalibratedClassifier.predict_proba\nline: 763\n\nsklearn/calibration.py\nfunction: _compute_predictions\nline: 641\n```"], "usage": {"completion_tokens": 181, "prompt_tokens": 13814}}}
